{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kipoi: model zoo for genomics Links kipoi.org - Main website kipoi.org/docs - Documentation github.com/kipoi/models - Model zoo for genomics maintained by the Kipoi team bioarxiv preprint - Kipoi: accelerating the community exchange and reuse of predictive models for genomics Installation 1. Install miniconda/anaconda Kipoi requires conda to manage model dependencies. Make sure you have either anaconda ( download page ) or miniconda ( download page ) installed. If you are using OSX, see Installing python on OSX . 2. Install Git LFS For downloading models, Kipoi uses git and Git Large File Storage (LFS). See how to install git here . To install git-lfs on Ubuntu, run: curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash sudo apt-get install -y git git-lfs git-lfs install Alternatively, you can install git-lfs through conda: conda install -c conda-forge git-lfs && git lfs install 3. Install Kipoi Next, install Kipoi using pip : pip install kipoi Quick start The following diagram gives a short overview over Kipoi's workflow: If you want to check which models are available in Kipoi you can use the website , where you can also see example commands for how to use the models on the CLI, python and R. Alternatively you can run kipoi ls in the command line or in python: import kipoi kipoi.list_models() Once a model has been selected (here: rbp_eclip/UPF1 ), the model environments have to be installed. To do so use one of the kipoi env commands. For example to install an environment for model rbp_eclip/UPF1 : kipoi env create rbp_eclip/UPF1 Aside: models versus model groups : A Kipoi model is a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide a path to a model group (e.g \"rbp_eclip\" or \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"rbp_eclip/UPF1\" or \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, a ValueError will be thrown. If you are working on a machine that has GPUs, you will want to add the --gpu flag to the command. And if you want to make use of the kipoi-veff plug-in then add --vep . For more options please run kipoi env create --help . The command will tell you how the execution environment for the model is called, e.g.: INFO [kipoi.cli.env] Environment name: kipoi-rbp_eclip__UPF1 Before using a model in any way, make sure that you have activated its environment, e.g.: prior to executing kipoi or python or R in the attempt to use Kipoi with the model. To activate the model environment run for example: source activate kipoi-rbp_eclip__UPF1 Command-line Once the model environment is activated Kipoi's API can be accessed from the commandline using: $ kipoi usage: kipoi <command> [-h] ... # Kipoi model-zoo command line tool. Available sub-commands: # - using models: ls List all the available models list_plugins List all the available plugins info Print dataloader keyword argument info predict Run the model prediction pull Download the directory associated with the model preproc Run the dataloader and save the results to an hdf5 array env Tools for managing Kipoi conda environments # - contributing models: init Initialize a new Kipoi model test Runs a set of unit-tests for the model test-source Runs a set of unit-tests for many/all models in a source # - plugin commands: veff Variant effect prediction interpret Model interpretation using feature importance scores like ISM, grad*input or DeepLIFT. Explore the CLI usage by running kipoi <command> -h . Also, see docs/using/getting started cli for more information. Python Once the model environment is activated ( source activate kipoi-rbp_eclip__UPF1 ) Kipoi's python API can be used to: The following commands give a short overview. For details please take a look at the python API documentation. Load the model from model the source: import kipoi model = kipoi.get_model(\"rbp_eclip/UPF1\") # load the model To get model predictions using the dataloader we can run: model.pipeline.predict(dict(fasta_file=\"hg19.fa\", intervals_file=\"intervals.bed\", gtf_file=\"gencode.v24.annotation_chr22.gtf\")) # runs: raw files -[dataloader]-> numpy arrays -[model]-> predictions To predict the values of a model input x , which for example was generated by dataloader, we can use: model.predict_on_batch(x) # implemented by all the models regardless of the framework Here x has to be a numpy.ndarray or a list or a dict of a numpy.ndarray , depending on the model requirements, for details please see the documentation of the API or of datalaoder.yaml and model.yaml . For more information see: notebooks/python-api.ipynb and docs/using getting started Configure Kipoi in .kipoi/config.yaml You can add your own (private) model sources. See docs/using/03_Model_sources/ . Contributing models See docs/contributing getting started and docs/tutorials/contributing/models for more information. Plugins Kipoi supports plug-ins which are published as additional python packages. Two plug-ins that are available are: kipoi_veff Variant effect prediction plugin compatible with (DNA) sequence based models. It allows to annotate a vcf file using model predictions for the reference and alternative alleles. The output is written to a new VCF file. For more information see tutorials/variant_effect_prediction_simple/ or tutorials/variant_effect_prediction/ . pip install kipoi_veff kipoi_interpret Model interpretation plugin for Kipoi. Allows to use feature importance scores like in-silico mutagenesis (ISM), saliency maps or DeepLift with a wide range of Kipoi models. example notebook pip install kipoi_interpret Documentation Documentation can be found here: kipoi.org/docs Citing Kipoi If you use Kipoi for your research, please cite the publication of the model you are using (see model's cite_as entry) and our Bioarxiv preprint: https://doi.org/10.1101/375345. @article {kipoi, author = {Avsec, Ziga and Kreuzhuber, Roman and Israeli, Johnny and Xu, Nancy and Cheng, Jun and Shrikumar, Avanti and Banerjee, Abhimanyu and Kim, Daniel S and Urban, Lara and Kundaje, Anshul and Stegle, Oliver and Gagneur, Julien}, title = {Kipoi: accelerating the community exchange and reuse of predictive models for genomics}, year = {2018}, doi = {10.1101/375345}, publisher = {Cold Spring Harbor Laboratory}, URL = {https://www.biorxiv.org/content/early/2018/07/24/375345}, eprint = {https://www.biorxiv.org/content/early/2018/07/24/375345.full.pdf}, journal = {bioRxiv} } Development If you want to help with the development of Kipoi, you are more than welcome to join in! For the local setup for development, you should install kipoi using: conda install pytorch-cpu pip install -e '.[develop]' This will install some additional packages like pytest . You can test the package by running py.test . If you wish to run tests in parallel, run py.test -n 6 .","title":"Home"},{"location":"#kipoi-model-zoo-for-genomics","text":"","title":"Kipoi: model zoo for genomics"},{"location":"#links","text":"kipoi.org - Main website kipoi.org/docs - Documentation github.com/kipoi/models - Model zoo for genomics maintained by the Kipoi team bioarxiv preprint - Kipoi: accelerating the community exchange and reuse of predictive models for genomics","title":"Links"},{"location":"#installation","text":"","title":"Installation"},{"location":"#1-install-minicondaanaconda","text":"Kipoi requires conda to manage model dependencies. Make sure you have either anaconda ( download page ) or miniconda ( download page ) installed. If you are using OSX, see Installing python on OSX .","title":"1. Install miniconda/anaconda"},{"location":"#2-install-git-lfs","text":"For downloading models, Kipoi uses git and Git Large File Storage (LFS). See how to install git here . To install git-lfs on Ubuntu, run: curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash sudo apt-get install -y git git-lfs git-lfs install Alternatively, you can install git-lfs through conda: conda install -c conda-forge git-lfs && git lfs install","title":"2. Install Git LFS"},{"location":"#3-install-kipoi","text":"Next, install Kipoi using pip : pip install kipoi","title":"3. Install Kipoi"},{"location":"#quick-start","text":"The following diagram gives a short overview over Kipoi's workflow: If you want to check which models are available in Kipoi you can use the website , where you can also see example commands for how to use the models on the CLI, python and R. Alternatively you can run kipoi ls in the command line or in python: import kipoi kipoi.list_models() Once a model has been selected (here: rbp_eclip/UPF1 ), the model environments have to be installed. To do so use one of the kipoi env commands. For example to install an environment for model rbp_eclip/UPF1 : kipoi env create rbp_eclip/UPF1 Aside: models versus model groups : A Kipoi model is a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide a path to a model group (e.g \"rbp_eclip\" or \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"rbp_eclip/UPF1\" or \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, a ValueError will be thrown. If you are working on a machine that has GPUs, you will want to add the --gpu flag to the command. And if you want to make use of the kipoi-veff plug-in then add --vep . For more options please run kipoi env create --help . The command will tell you how the execution environment for the model is called, e.g.: INFO [kipoi.cli.env] Environment name: kipoi-rbp_eclip__UPF1 Before using a model in any way, make sure that you have activated its environment, e.g.: prior to executing kipoi or python or R in the attempt to use Kipoi with the model. To activate the model environment run for example: source activate kipoi-rbp_eclip__UPF1","title":"Quick start"},{"location":"#command-line","text":"Once the model environment is activated Kipoi's API can be accessed from the commandline using: $ kipoi usage: kipoi <command> [-h] ... # Kipoi model-zoo command line tool. Available sub-commands: # - using models: ls List all the available models list_plugins List all the available plugins info Print dataloader keyword argument info predict Run the model prediction pull Download the directory associated with the model preproc Run the dataloader and save the results to an hdf5 array env Tools for managing Kipoi conda environments # - contributing models: init Initialize a new Kipoi model test Runs a set of unit-tests for the model test-source Runs a set of unit-tests for many/all models in a source # - plugin commands: veff Variant effect prediction interpret Model interpretation using feature importance scores like ISM, grad*input or DeepLIFT. Explore the CLI usage by running kipoi <command> -h . Also, see docs/using/getting started cli for more information.","title":"Command-line"},{"location":"#python","text":"Once the model environment is activated ( source activate kipoi-rbp_eclip__UPF1 ) Kipoi's python API can be used to: The following commands give a short overview. For details please take a look at the python API documentation. Load the model from model the source: import kipoi model = kipoi.get_model(\"rbp_eclip/UPF1\") # load the model To get model predictions using the dataloader we can run: model.pipeline.predict(dict(fasta_file=\"hg19.fa\", intervals_file=\"intervals.bed\", gtf_file=\"gencode.v24.annotation_chr22.gtf\")) # runs: raw files -[dataloader]-> numpy arrays -[model]-> predictions To predict the values of a model input x , which for example was generated by dataloader, we can use: model.predict_on_batch(x) # implemented by all the models regardless of the framework Here x has to be a numpy.ndarray or a list or a dict of a numpy.ndarray , depending on the model requirements, for details please see the documentation of the API or of datalaoder.yaml and model.yaml . For more information see: notebooks/python-api.ipynb and docs/using getting started","title":"Python"},{"location":"#configure-kipoi-in-kipoiconfigyaml","text":"You can add your own (private) model sources. See docs/using/03_Model_sources/ .","title":"Configure Kipoi in .kipoi/config.yaml"},{"location":"#contributing-models","text":"See docs/contributing getting started and docs/tutorials/contributing/models for more information.","title":"Contributing models"},{"location":"#plugins","text":"Kipoi supports plug-ins which are published as additional python packages. Two plug-ins that are available are:","title":"Plugins"},{"location":"#kipoi_veff","text":"Variant effect prediction plugin compatible with (DNA) sequence based models. It allows to annotate a vcf file using model predictions for the reference and alternative alleles. The output is written to a new VCF file. For more information see tutorials/variant_effect_prediction_simple/ or tutorials/variant_effect_prediction/ . pip install kipoi_veff","title":"kipoi_veff"},{"location":"#kipoi_interpret","text":"Model interpretation plugin for Kipoi. Allows to use feature importance scores like in-silico mutagenesis (ISM), saliency maps or DeepLift with a wide range of Kipoi models. example notebook pip install kipoi_interpret","title":"kipoi_interpret"},{"location":"#documentation","text":"Documentation can be found here: kipoi.org/docs","title":"Documentation"},{"location":"#citing-kipoi","text":"If you use Kipoi for your research, please cite the publication of the model you are using (see model's cite_as entry) and our Bioarxiv preprint: https://doi.org/10.1101/375345. @article {kipoi, author = {Avsec, Ziga and Kreuzhuber, Roman and Israeli, Johnny and Xu, Nancy and Cheng, Jun and Shrikumar, Avanti and Banerjee, Abhimanyu and Kim, Daniel S and Urban, Lara and Kundaje, Anshul and Stegle, Oliver and Gagneur, Julien}, title = {Kipoi: accelerating the community exchange and reuse of predictive models for genomics}, year = {2018}, doi = {10.1101/375345}, publisher = {Cold Spring Harbor Laboratory}, URL = {https://www.biorxiv.org/content/early/2018/07/24/375345}, eprint = {https://www.biorxiv.org/content/early/2018/07/24/375345.full.pdf}, journal = {bioRxiv} }","title":"Citing Kipoi"},{"location":"#development","text":"If you want to help with the development of Kipoi, you are more than welcome to join in! For the local setup for development, you should install kipoi using: conda install pytorch-cpu pip install -e '.[develop]' This will install some additional packages like pytest . You can test the package by running py.test . If you wish to run tests in parallel, run py.test -n 6 .","title":"Development"},{"location":"contributing/","text":"Contributing Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions Report Bugs Report bugs at https://github.com/kipoi/kipoi/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs Look through the GitHub issues for bugs. Anything tagged with \u201cbug\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it. Implement Features Look through the GitHub issues for features. Anything tagged with \u201cenhancement\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it. Write Documentation Kipoi could always use more documentation, whether as part of the official Kipoi docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback The best way to send feedback is to file an issue at https://github.com/kipoi/kipoi/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Workflow make an issue for the thing you want to implement create the corresponding branch develop write units tests in tests/ write documentation in markdown (see other functions for example) push the changes make a pull request once the pull request is merged, the issue will be closed Get Started! Ready to contribute? Here\u2019s how to set up kipoi for local development. Fork the kipoi repo on GitHub. Clone your fork locally: $ git clone git@github.com:your_name_here/kipoi.git Install your local copy into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development: $ conda create -n kipoi-py35 python=3.5 $ cd kipoi/ $ source activate kipoi-py35 $ pip install -e '.[develop]' Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you\u2019re done making changes, check that your changes pass the tests: $ py.test tests/ -n 4 Where -n 4 will use 4 cores in parallel to run tests. Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring. The pull request should work for Python 2.7, 3.5 and 3.6.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/kipoi/kipoi/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \u201cbug\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \u201cenhancement\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"Kipoi could always use more documentation, whether as part of the official Kipoi docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/kipoi/kipoi/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#workflow","text":"make an issue for the thing you want to implement create the corresponding branch develop write units tests in tests/ write documentation in markdown (see other functions for example) push the changes make a pull request once the pull request is merged, the issue will be closed","title":"Workflow"},{"location":"contributing/#get-started","text":"Ready to contribute? Here\u2019s how to set up kipoi for local development. Fork the kipoi repo on GitHub. Clone your fork locally: $ git clone git@github.com:your_name_here/kipoi.git Install your local copy into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development: $ conda create -n kipoi-py35 python=3.5 $ cd kipoi/ $ source activate kipoi-py35 $ pip install -e '.[develop]' Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you\u2019re done making changes, check that your changes pass the tests: $ py.test tests/ -n 4 Where -n 4 will use 4 cores in parallel to run tests. Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring. The pull request should work for Python 2.7, 3.5 and 3.6.","title":"Pull Request Guidelines"},{"location":"api/dataloader/","text":"Dataloader Available dataloaders [source] Dataset kipoi.data.Dataset() An abstract class representing a Dataset. All other datasets should subclass it. All subclasses should override __len__ , that provides the size of the dataset, and __getitem__ , supporting integer indexing in range from 0 to len(self) exclusive. [source] BatchDataset kipoi.data.BatchDataset() An abstract class representing a BatchDataset. [source] SampleIterator kipoi.data.SampleIterator() [source] BatchIterator kipoi.data.BatchIterator() get_dataloader_factory get_dataloader_factory(dataloader, source='kipoi') PreloadedDataset PreloadedDataset(self) Generated by supplying a function returning the full dataset. The full dataset is a nested (list/dict) python structure of numpy arrays with the same first axis dimension. SampleGenerator SampleGenerator(self) Transform a generator of samples into SampleIterator BatchGenerator BatchGenerator(self) Transform a generator of batches into BatchIterator","title":"Dataloader"},{"location":"api/dataloader/#dataloader","text":"","title":"Dataloader"},{"location":"api/dataloader/#available-dataloaders","text":"[source]","title":"Available dataloaders"},{"location":"api/dataloader/#dataset","text":"kipoi.data.Dataset() An abstract class representing a Dataset. All other datasets should subclass it. All subclasses should override __len__ , that provides the size of the dataset, and __getitem__ , supporting integer indexing in range from 0 to len(self) exclusive. [source]","title":"Dataset"},{"location":"api/dataloader/#batchdataset","text":"kipoi.data.BatchDataset() An abstract class representing a BatchDataset. [source]","title":"BatchDataset"},{"location":"api/dataloader/#sampleiterator","text":"kipoi.data.SampleIterator() [source]","title":"SampleIterator"},{"location":"api/dataloader/#batchiterator","text":"kipoi.data.BatchIterator()","title":"BatchIterator"},{"location":"api/dataloader/#get_dataloader_factory","text":"get_dataloader_factory(dataloader, source='kipoi')","title":"get_dataloader_factory"},{"location":"api/dataloader/#preloadeddataset","text":"PreloadedDataset(self) Generated by supplying a function returning the full dataset. The full dataset is a nested (list/dict) python structure of numpy arrays with the same first axis dimension.","title":"PreloadedDataset"},{"location":"api/dataloader/#samplegenerator","text":"SampleGenerator(self) Transform a generator of samples into SampleIterator","title":"SampleGenerator"},{"location":"api/dataloader/#batchgenerator","text":"BatchGenerator(self) Transform a generator of batches into BatchIterator","title":"BatchGenerator"},{"location":"api/metadata/","text":"Metadata Here are some of the custom classes that people might find useful when writing the metadata fields of the dataloaders. Available classes [source] GenomicRanges kipoi.metadata.GenomicRanges(chr, start, end, id, strand='*')","title":"Metadata"},{"location":"api/metadata/#metadata","text":"Here are some of the custom classes that people might find useful when writing the metadata fields of the dataloaders.","title":"Metadata"},{"location":"api/metadata/#available-classes","text":"[source]","title":"Available classes"},{"location":"api/metadata/#genomicranges","text":"kipoi.metadata.GenomicRanges(chr, start, end, id, strand='*')","title":"GenomicRanges"},{"location":"api/model/","text":"Model Available models [source] KerasModel kipoi.model.KerasModel(weights, arch=None, custom_objects=None, backend=None, image_dim_ordering=None) Loads the serialized Keras model Arguments weights : File path to the hdf5 weights or the hdf5 Keras model arch : Architecture json model. If None, weights is assumed to speficy the whole model custom_objects : Python file defining the custom Keras objects in a OBJECTS dictionary backend : Keras backend to use ('tensorflow', 'theano', ...) image_dim_ordering : 'tf' or 'th': Whether to use 'tf' ('channels_last') or 'th' ('cannels_first') dimension ordering. model.yml entry - __Model__: - __type__: Keras - __args__: - __weights__: model.h5 - __arch__: model.json - __custom_objects__: custom_keras_objects.py [source] PyTorchModel kipoi.model.PyTorchModel(file=None, build_fn=None, weights=None, auto_use_cuda=True) Loads a pytorch model. [source] SklearnModel kipoi.model.SklearnModel(pkl_file, predict_method='predict') Loads the serialized scikit learn model Arguments pkl_file : File path to the dumped sklearn file in the pickle format. model.yml entry - __Model__: - __type__: sklearn - __args__: - __pkl_file__: asd.pkl - __predict_method__: Which prediction method to use. Available options: 'predict', 'predict_proba' or 'predict_log_proba'. [source] TensorFlowModel kipoi.model.TensorFlowModel(input_nodes, target_nodes, checkpoint_path, const_feed_dict_pkl=None) get_model get_model(model, source='kipoi', with_dataloader=True) Load the model from source , as well as the default dataloder to model.default_dataloder. Args : model, str: model name source, str: source name with_dataloader, bool: if True, the default dataloader is loaded to model.default_dataloadera and the pipeline at model.pipeline enabled.","title":"Model"},{"location":"api/model/#model","text":"","title":"Model"},{"location":"api/model/#available-models","text":"[source]","title":"Available models"},{"location":"api/model/#kerasmodel","text":"kipoi.model.KerasModel(weights, arch=None, custom_objects=None, backend=None, image_dim_ordering=None) Loads the serialized Keras model Arguments weights : File path to the hdf5 weights or the hdf5 Keras model arch : Architecture json model. If None, weights is assumed to speficy the whole model custom_objects : Python file defining the custom Keras objects in a OBJECTS dictionary backend : Keras backend to use ('tensorflow', 'theano', ...) image_dim_ordering : 'tf' or 'th': Whether to use 'tf' ('channels_last') or 'th' ('cannels_first') dimension ordering. model.yml entry - __Model__: - __type__: Keras - __args__: - __weights__: model.h5 - __arch__: model.json - __custom_objects__: custom_keras_objects.py [source]","title":"KerasModel"},{"location":"api/model/#pytorchmodel","text":"kipoi.model.PyTorchModel(file=None, build_fn=None, weights=None, auto_use_cuda=True) Loads a pytorch model. [source]","title":"PyTorchModel"},{"location":"api/model/#sklearnmodel","text":"kipoi.model.SklearnModel(pkl_file, predict_method='predict') Loads the serialized scikit learn model Arguments pkl_file : File path to the dumped sklearn file in the pickle format. model.yml entry - __Model__: - __type__: sklearn - __args__: - __pkl_file__: asd.pkl - __predict_method__: Which prediction method to use. Available options: 'predict', 'predict_proba' or 'predict_log_proba'. [source]","title":"SklearnModel"},{"location":"api/model/#tensorflowmodel","text":"kipoi.model.TensorFlowModel(input_nodes, target_nodes, checkpoint_path, const_feed_dict_pkl=None)","title":"TensorFlowModel"},{"location":"api/model/#get_model","text":"get_model(model, source='kipoi', with_dataloader=True) Load the model from source , as well as the default dataloder to model.default_dataloder. Args : model, str: model name source, str: source name with_dataloader, bool: if True, the default dataloader is loaded to model.default_dataloadera and the pipeline at model.pipeline enabled.","title":"get_model"},{"location":"api/pipeline/","text":"Pipeline [source] Pipeline kipoi.pipeline.Pipeline(model, dataloader_cls) Provides the predict_example, predict and predict_generator to the kipoi.Model","title":"Pipeline"},{"location":"api/pipeline/#pipeline","text":"[source]","title":"Pipeline"},{"location":"api/pipeline/#pipeline_1","text":"kipoi.pipeline.Pipeline(model, dataloader_cls) Provides the predict_example, predict and predict_generator to the kipoi.Model","title":"Pipeline"},{"location":"api/sources/","text":"Remote sources Available sources [source] GitLFSSource kipoi.sources.GitLFSSource(remote_url, local_path) [source] GitSource kipoi.sources.GitSource(remote_url, local_path) [source] LocalSource kipoi.sources.LocalSource(local_path) [source] GithubPermalinkSource kipoi.sources.GithubPermalinkSource(local_path)","title":"Sources"},{"location":"api/sources/#remote-sources","text":"","title":"Remote sources"},{"location":"api/sources/#available-sources","text":"[source]","title":"Available sources"},{"location":"api/sources/#gitlfssource","text":"kipoi.sources.GitLFSSource(remote_url, local_path) [source]","title":"GitLFSSource"},{"location":"api/sources/#gitsource","text":"kipoi.sources.GitSource(remote_url, local_path) [source]","title":"GitSource"},{"location":"api/sources/#localsource","text":"kipoi.sources.LocalSource(local_path) [source]","title":"LocalSource"},{"location":"api/sources/#githubpermalinksource","text":"kipoi.sources.GithubPermalinkSource(local_path)","title":"GithubPermalinkSource"},{"location":"api/postprocessing/variant_effect_scores/","text":"Variant effect scores Available scores {{autogenerated}}","title":"Variant effect scores"},{"location":"api/postprocessing/variant_effect_scores/#variant-effect-scores","text":"","title":"Variant effect scores"},{"location":"api/postprocessing/variant_effect_scores/#available-scores","text":"{{autogenerated}}","title":"Available scores"},{"location":"api/postprocessing/variant_effects/","text":"Variant effects Main function: {{autogenerated}}","title":"Variant effects"},{"location":"api/postprocessing/variant_effects/#variant-effects","text":"Main function: {{autogenerated}}","title":"Variant effects"},{"location":"contributing/01_Getting_started/","text":"Contributing models - Getting started Kipoi stores models (descriptions, parameter files, dataloader code, ...) as folders in the kipoi/models github repository. Files residing in folders with a suffix of _files are tracked via Git Large File Storage (LFS). New models are added by simply submitting a pull-request to https://github.com/kipoi/models . As a reminder - Kipoi model have to have the following folder structure in which all the relevant files have their assigned places: \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u251c\u2500\u2500 dataloader_files/ #/ files required by the dataloader \u2502 \u251c\u2500\u2500 x_transfomer.pkl \u2502 \u2514\u2500\u2500 y_transfomer.pkl \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 model_files/ #/ files required by the model \u2502 \u251c\u2500\u2500 model.json \u2502 \u2514\u2500\u2500 weights.h5 \u2514\u2500\u2500 example_files/ #/ small example files \u251c\u2500\u2500 features.csv \u2514\u2500\u2500 targets.csv Required steps Here is a list of steps required to contribute a model to kipoi/models : 1. Install Kipoi Install git-lfs conda install -c conda-forge git-lfs && git lfs install For alternative installation options see https://git-lfs.github.com/ . Install kipoi pip install kipoi Run kipoi ls (this will checkout the kipoi/models repo to ~/.kipoi/models ) 2. Add the model cd ~/.kipoi/models Write the model : Create a new folder <my new model> containing all the required files. The required files can be created by doing one of the following three options: Option 1: Copy the existing model: cp -R <existing model> <my new model> , edit/replace/add the copied files until they fit your new model. Option 2: Run kipoi init , answer the questions, edit/replace the created files until they fit your new model. Option 3: mkdir <my new model> & write all the files from scratch Test the model Step 1: kipoi test ~/.kipoi/models/my_new_model Step 2: kipoi test-source kipoi --all -k my_new_model 3. Submit the pull-request Option 1: Fork the repository Make sure you have all the recent changes locally cd ~/.kipoi/models export GIT_LFS_SKIP_SMUDGE=1 && git pull - pulls all the changes but doesn't download the files tracked by git-lfs. Commit your changes git add my_new_model/ git commit -m \"Added <my new model>\" Fork the https://github.com/kipoi/models repo on github (click on the Fork button) Add your fork as a git remote to ~/.kipoi/models git remote add fork https://github.com/<username>/models.git Push to your fork git push fork master Submit a pull-request click the New pull request button on your github fork - https://github.com/<username>/models> Option 2: Create a new branch on kipoi/models If you wish to contribute models more frequently, please join the team . You will be added to the Kipoi organization. This will allow you to push to branches of the kipoi/models github repo directly. Make sure you have all the recent changes locally cd ~/.kipoi/models export GIT_LFS_SKIP_SMUDGE=1 && git pull - pulls all the changes but doesn't download the files tracked by git-lfs. Create a new branch in ~/.kipoi/models git stash - this will store/stash all local changes in git stash git checkout -b my_new_model - create a new branch git stash pop - get the stashed files back Commit changes git add my_new_model/ git commit -m \"Added <my new model>\" Push changes to my_new_model branch git push -u origin my_new_model Submit a pull-request click the New pull request button on my_new_model branch of repo https://github.com/kipoi/models . Rest of this document will go more into the details about steps writing the model and testing the model. How to write the model Best place to start figuring out which files you need to contribute is to look at some of the existing models. Explore the https://github.com/kipoi/models repository and see if there are any models similar to yours (in terms of the dependencies, framework, input-output data modalities). See tutorials/contributing_models for a step-by-step procedure for contributing models. In terms of what to include in your model: The information in these pages here are the minimum requirement. The more information you can share with other users the better! If you have converted the model from using a script, please add that. If you have additional test and validation scripts that you wrote while verifying the Kipoi model, etc. , please add them. You will make future users happy. Hint: If you want to take a look at a specific model that is already in the zoo, but instead of the content of the model files there is just a hash entry, then use kipoi pull <model_name> to download the model data. Option #1: Copy existing model Once you have found the closest match, simply copy the directory and start editing/replacing the files. Edit the files in this order: model.yaml dataloader.yaml dataloader.py overwrite files in model_files/ example_files/ LICENSE Option #2: Use kipoi init Alternatively, you can use kipoi init instead of copying the existing model: cd ~/.kipoi/models && kipoi init This will ask you a few questions and create a new model folder. $ kipoi init INFO [kipoi.cli.main] Initializing a new Kipoi model Please answer the questions below. Defaults are shown in square brackets. You might find the following links useful: - (model_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_models.md - (dataloader_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_dataloaders.md -------------------------------------------- model_name [my_model]: my_new_model author_name [Your name]: Ziga Avsec author_github [Your github username]: avsecz author_email [Your email(optional)]: model_doc [Model description]: Model predicting iris species Select model_license: 1 - MIT 2 - BSD 3 - ISCL 4 - Apache Software License 2.0 5 - Not open source Choose from 1, 2, 3, 4, 5 [1]: Select model_type: 1 - keras 2 - custom 3 - sklearn Choose from 1, 2, 3 [1]: 1 Select model_input_type: 1 - np.array 2 - list of np.arrays 3 - dict of np.arrays Choose from 1, 2, 3 [1]: 2 Select model_output_type: 1 - np.array 2 - list of np.arrays 3 - dict of np.arrays Choose from 1, 2, 3 [1]: 3 Select dataloader_type: 1 - Dataset 2 - PreloadedDataset 3 - BatchDataset 4 - SampleIterator 5 - SampleGenerator 6 - BatchIterator 7 - BatchGenerator Choose from 1, 2, 3, 4, 5, 6, 7 [1]: 1 -------------------------------------------- INFO [kipoi.cli.main] Done! Created the following folder into the current working directory: my_new_model The created folder contains a model and a dataloader for predicting the Iris species. You will now have to edit the model.yaml and to edit the dataloader.yaml files according to your model. Also you will have to copy you rmodel files into the model_files directory. You can check whether you have succeeded and your model is setup correctly with the commands below. How to test the model Be aware that the test functions will only check whether the definition side of things (model.yaml, dataloader.yaml, syntax errors, etc.) is setup correctly, you will have to validate yourself whether the outputs created by using the predict function produce the desired model output! Step 1: Run kipoi test ~/.kipoi/models/my_new_model This checks the yaml files and runs kipoi predict for the example files (specified in dataloader.yaml > args > my_arg > example ). Once this command returns no errors or warnings proceed to the next step. Step 2: Run kipoi test-source kipoi --all -k my_new_model This will run kipoi test in a new conda environment with dependencies specified in model.yaml and dataloader.yaml . Removing or updating models To remove, rename or update an existing model, send a pull-request (as when contributing models, see 3. Submit the pull-request ).","title":"Getting started"},{"location":"contributing/01_Getting_started/#contributing-models-getting-started","text":"Kipoi stores models (descriptions, parameter files, dataloader code, ...) as folders in the kipoi/models github repository. Files residing in folders with a suffix of _files are tracked via Git Large File Storage (LFS). New models are added by simply submitting a pull-request to https://github.com/kipoi/models . As a reminder - Kipoi model have to have the following folder structure in which all the relevant files have their assigned places: \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u251c\u2500\u2500 dataloader_files/ #/ files required by the dataloader \u2502 \u251c\u2500\u2500 x_transfomer.pkl \u2502 \u2514\u2500\u2500 y_transfomer.pkl \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 model_files/ #/ files required by the model \u2502 \u251c\u2500\u2500 model.json \u2502 \u2514\u2500\u2500 weights.h5 \u2514\u2500\u2500 example_files/ #/ small example files \u251c\u2500\u2500 features.csv \u2514\u2500\u2500 targets.csv","title":"Contributing models - Getting started"},{"location":"contributing/01_Getting_started/#required-steps","text":"Here is a list of steps required to contribute a model to kipoi/models :","title":"Required steps"},{"location":"contributing/01_Getting_started/#1-install-kipoi","text":"Install git-lfs conda install -c conda-forge git-lfs && git lfs install For alternative installation options see https://git-lfs.github.com/ . Install kipoi pip install kipoi Run kipoi ls (this will checkout the kipoi/models repo to ~/.kipoi/models )","title":"1. Install Kipoi"},{"location":"contributing/01_Getting_started/#2-add-the-model","text":"cd ~/.kipoi/models Write the model : Create a new folder <my new model> containing all the required files. The required files can be created by doing one of the following three options: Option 1: Copy the existing model: cp -R <existing model> <my new model> , edit/replace/add the copied files until they fit your new model. Option 2: Run kipoi init , answer the questions, edit/replace the created files until they fit your new model. Option 3: mkdir <my new model> & write all the files from scratch Test the model Step 1: kipoi test ~/.kipoi/models/my_new_model Step 2: kipoi test-source kipoi --all -k my_new_model","title":"2. Add the model"},{"location":"contributing/01_Getting_started/#3-submit-the-pull-request","text":"","title":"3. Submit the pull-request"},{"location":"contributing/01_Getting_started/#option-1-fork-the-repository","text":"Make sure you have all the recent changes locally cd ~/.kipoi/models export GIT_LFS_SKIP_SMUDGE=1 && git pull - pulls all the changes but doesn't download the files tracked by git-lfs. Commit your changes git add my_new_model/ git commit -m \"Added <my new model>\" Fork the https://github.com/kipoi/models repo on github (click on the Fork button) Add your fork as a git remote to ~/.kipoi/models git remote add fork https://github.com/<username>/models.git Push to your fork git push fork master Submit a pull-request click the New pull request button on your github fork - https://github.com/<username>/models>","title":"Option 1: Fork the repository"},{"location":"contributing/01_Getting_started/#option-2-create-a-new-branch-on-kipoimodels","text":"If you wish to contribute models more frequently, please join the team . You will be added to the Kipoi organization. This will allow you to push to branches of the kipoi/models github repo directly. Make sure you have all the recent changes locally cd ~/.kipoi/models export GIT_LFS_SKIP_SMUDGE=1 && git pull - pulls all the changes but doesn't download the files tracked by git-lfs. Create a new branch in ~/.kipoi/models git stash - this will store/stash all local changes in git stash git checkout -b my_new_model - create a new branch git stash pop - get the stashed files back Commit changes git add my_new_model/ git commit -m \"Added <my new model>\" Push changes to my_new_model branch git push -u origin my_new_model Submit a pull-request click the New pull request button on my_new_model branch of repo https://github.com/kipoi/models . Rest of this document will go more into the details about steps writing the model and testing the model.","title":"Option 2: Create a new branch on kipoi/models"},{"location":"contributing/01_Getting_started/#how-to-write-the-model","text":"Best place to start figuring out which files you need to contribute is to look at some of the existing models. Explore the https://github.com/kipoi/models repository and see if there are any models similar to yours (in terms of the dependencies, framework, input-output data modalities). See tutorials/contributing_models for a step-by-step procedure for contributing models. In terms of what to include in your model: The information in these pages here are the minimum requirement. The more information you can share with other users the better! If you have converted the model from using a script, please add that. If you have additional test and validation scripts that you wrote while verifying the Kipoi model, etc. , please add them. You will make future users happy. Hint: If you want to take a look at a specific model that is already in the zoo, but instead of the content of the model files there is just a hash entry, then use kipoi pull <model_name> to download the model data.","title":"How to write the model"},{"location":"contributing/01_Getting_started/#option-1-copy-existing-model","text":"Once you have found the closest match, simply copy the directory and start editing/replacing the files. Edit the files in this order: model.yaml dataloader.yaml dataloader.py overwrite files in model_files/ example_files/ LICENSE","title":"Option #1: Copy existing model"},{"location":"contributing/01_Getting_started/#option-2-use-kipoi-init","text":"Alternatively, you can use kipoi init instead of copying the existing model: cd ~/.kipoi/models && kipoi init This will ask you a few questions and create a new model folder. $ kipoi init INFO [kipoi.cli.main] Initializing a new Kipoi model Please answer the questions below. Defaults are shown in square brackets. You might find the following links useful: - (model_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_models.md - (dataloader_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_dataloaders.md -------------------------------------------- model_name [my_model]: my_new_model author_name [Your name]: Ziga Avsec author_github [Your github username]: avsecz author_email [Your email(optional)]: model_doc [Model description]: Model predicting iris species Select model_license: 1 - MIT 2 - BSD 3 - ISCL 4 - Apache Software License 2.0 5 - Not open source Choose from 1, 2, 3, 4, 5 [1]: Select model_type: 1 - keras 2 - custom 3 - sklearn Choose from 1, 2, 3 [1]: 1 Select model_input_type: 1 - np.array 2 - list of np.arrays 3 - dict of np.arrays Choose from 1, 2, 3 [1]: 2 Select model_output_type: 1 - np.array 2 - list of np.arrays 3 - dict of np.arrays Choose from 1, 2, 3 [1]: 3 Select dataloader_type: 1 - Dataset 2 - PreloadedDataset 3 - BatchDataset 4 - SampleIterator 5 - SampleGenerator 6 - BatchIterator 7 - BatchGenerator Choose from 1, 2, 3, 4, 5, 6, 7 [1]: 1 -------------------------------------------- INFO [kipoi.cli.main] Done! Created the following folder into the current working directory: my_new_model The created folder contains a model and a dataloader for predicting the Iris species. You will now have to edit the model.yaml and to edit the dataloader.yaml files according to your model. Also you will have to copy you rmodel files into the model_files directory. You can check whether you have succeeded and your model is setup correctly with the commands below.","title":"Option #2: Use kipoi init"},{"location":"contributing/01_Getting_started/#how-to-test-the-model","text":"Be aware that the test functions will only check whether the definition side of things (model.yaml, dataloader.yaml, syntax errors, etc.) is setup correctly, you will have to validate yourself whether the outputs created by using the predict function produce the desired model output!","title":"How to test the model"},{"location":"contributing/01_Getting_started/#step-1-run-kipoi-test-kipoimodelsmy_new_model","text":"This checks the yaml files and runs kipoi predict for the example files (specified in dataloader.yaml > args > my_arg > example ). Once this command returns no errors or warnings proceed to the next step.","title":"Step 1: Run kipoi test ~/.kipoi/models/my_new_model"},{"location":"contributing/01_Getting_started/#step-2-run-kipoi-test-source-kipoi-all-k-my_new_model","text":"This will run kipoi test in a new conda environment with dependencies specified in model.yaml and dataloader.yaml .","title":"Step 2: Run kipoi test-source kipoi --all -k my_new_model"},{"location":"contributing/01_Getting_started/#removing-or-updating-models","text":"To remove, rename or update an existing model, send a pull-request (as when contributing models, see 3. Submit the pull-request ).","title":"Removing or updating models"},{"location":"contributing/02_Writing_model.yaml/","text":"model.yaml The model.yaml file describes the individual model in the model zoo. It defines its dependencies, framework, architecture, input / output schema, general information and more. Correct defintions in the model.yaml enable to make full use of Kipoi features and make sure that a model can be executed at any point in future. To help understand the synthax of YAML please take a look at: YAML Synthax Basics Here is an example model.yaml : type: keras # use `kipoi.model.KerasModel` args: # arguments of `kipoi.model.KerasModel` arch: model_files/model.json weights: model_files/weights.h5 default_dataloader: . # path to the dataloader directory. Here it's defined in the same directory info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species version: 0.1 # optional cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python=3.5 - h5py # - soumith::pytorch # specify packages from other channels via <channel>::<package> pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" The model.yaml file has the following mandatory fields: type The model type refers to base framework which the model was defined in. Kipoi comes with a support for Keras, PyTorch, SciKit-learn and tensorflow models. To indicate which kind of model will be used the following values for type are allowed: keras , pytorch , sklearn , tensorflow , and custom . The model type is required to find the right internal prepresentation of a model within Kipoi, which enables loading weights and architecture correctly and offers to have a unified API across frameworks. In the model.yaml file the definition of a Keras model would like this: type: keras args Model arguments define where the files are files and functions are located to instantiate the model. Most entries of args will contain paths to files, those paths are relative to the location of the model.yaml file. The correct definition of args depends on the type that was selected: keras models For Keras models the following args are available: weights : File path to the hdf5 weights or the hdf5 Keras model. arch : Architecture json model. If None, weights is assumed to speficy the whole model custom_objects : Python file defining the custom Keras objects in a OBJECTS dictionary backend : Keras backend to use ('tensorflow', 'theano', 'cntk') image_dim_ordering : 'tf' or 'th' : Whether the model was trained with using 'tf' ('channels_last') or 'th' ('cannels_first') dimension ordering. The Keras framework offers different ways to store model architecture and weights: Architecture and weights can be stored separately: type: keras args: arch: model_files/model.json weights: model_files/weights.h5 The architecture can be stored together with the weights: type: keras args: weights: model_files/model.h5 In Keras models can have custom layers, which then have to be available at the instantiation of the Keras model, those should be stored in one python file that comes with the model architecture and weights. This file defines a dictionary containing custom Keras components called OBJECTS . These objects will be added to custom_objects when loading the model with keras.models.load_model . Example of a custom_keras_objects.py : from concise.layers import SplineT OBJECTS = {\"SplineT\": SplineT} Example of the corresponding model.yaml entry: type: keras args: ... custom_objects: model_files/custom_keras_objects.py Here all the objects present in model_files/custom_keras_objects.py will be made available to Keras when loading the model. pytorch models Pytorch offers much freedom as to how the model is stored. In Kipoi a pytorch model has the following args : file , build_fn , weights . If cuda is available the model will automatically be switched to cuda mode, so the user does not have to take care of that and the build_fn should not attempt to do this conversion. The following ways of instantiating a model are supported: Build function: In the example below Kipoi expects that when calling get_full_model() (which is defined in model_files/model_def.py ) A pytorch model is returned that for which the weights have already been loaded. type: pytorch args: file: model_files/model_def.py build_fn: get_full_model Build function + weights: In the example below the model is instantiated by calling get_model() which can be found in model_files/model_def.py . After that the weights will be loaded by executing model.load_state_dict(torch.load(weights)) . type: pytorch args: file: model_files/model_def.py build_fn: get_model weights: model_files/weights.pth Architecture and weights in one file: In this case Kipoi assumes that model = torch.load(weights) will be a valid pytorch model. Care has to be taken when storing the architecture a model this way as only standard pytorch layers will be loaded correctly, please see the pytorch documentation for details. type: pytorch args: weights: model_files/model.pth sklearn models SciKit-learn models can be loaded from a pickle file as defined below. The command used for loading is: joblib.load(pkl_file) type: sklearn args: pkl_file: model_files/model.pkl predict_method: predict_proba # Optional. predict by default. Available: predict, predict_proba, predict_log_proba tensorflow models Tensorflow models are expected to be stored by calling saver = tf.train.Saver(); saver.save(checkpoint_path) . The input_nodes argument is then a string, list of strings or dictionary of strings that define the input node names. The target_nodes argument is a string, list of strings or dictionary of strings that define the model target node names. type: tensorflow args: input_nodes: \"inputs\" target_nodes: \"preds\" checkpoint_path: \"model_files/model.tf\" If a model requires a constant feed of data which is not provided by the dataloader the const_feed_dict_pkl argument can be defined additionally to the above. Values given in the pickle file will be added to the batch samples created by the dataloader. If values with identical keys have been created by the dataloader they will be overwritten with what is given in const_feed_dict_pkl . type: tensorflow args: ... const_feed_dict_pkl: \"model_files/const_feed_dict.pkl\" custom models It is possible to defined a model class independent of the ones which are made available in Kipoi. In that case the contributor-defined Model class must be a subclass of BaseModel defined in kipoi.model . Custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! If for example a custom model class definition ( MyModel ) lies in a file my_model.py , then the model.yaml will contain: type: custom args: file: my_model.py object: MyModel Kipoi will then use an instance of MyModel as a model. Keep in mind that MyModel has to be subclass of BaseModel , which in other words means that def predict_on_batch(self, x) has to be implemented. So if batch is for example what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to work. info The info field of a model.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the model. A short description of what it does and what it is designed for. version : Model version license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the model and its use cases cite_as : Link to the journal, arXiv, ... trained_on : Description of the training dataset training_procedure : Description of the training procedure A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com - name: Second Author doc: My fancy model description version: 1.0 license: GNU tags: - TFBS - tag2 cite_as: http://www.the_journal.com/mypublication trained_on: The XXXX dataset from YYYY training_procedure: 10-fold cross validation default_dataloader The default_dataloader points to the location of the dataloader.yaml file. By default this will be in the same folder as the model.yaml file, in which case default_dataloader doesn't have to be defined. If dataloader.yaml lies in different subfolder then default_dataloader: path/to/folder would be used where dataloader.yaml would lie in folder . schema Schema defines what the model inputs and outputs are, what they consist in and what the dimensions are. schema contains two categories inputs and targets which each specify the shapes of the model input and model output. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A dictionary of numpy arrays as inputs or targets: schema: inputs: seq: shape: (1000,4) inp2: shape: (10) inputs The inputs fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input , not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ), which is important for variant effect prediction. targets The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model input name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line). How model types handle schemas The different model types handle those three different encapsulations of numpy arrays differently: keras type models Input In case a Keras model is used the batch produced by the dataloader is passed on as it is to the model.predict_on_batch() function. So if for example a dictionary is defined in the model.yaml and that is produced by the dataloader then this dicationary is passed on to model.predict_on_batch() . Output The model is expected to return the schema that is defined in model.yaml. If for example a model returns a list of numpy arrays then that has to be defined correctly in the model.yaml schema. pytorch type models Pytorch needs torch.autograd.Variable instances to work. Hence all inputs are automatically converted into Variable objects and results are converted back into numpy arrays transparently. If cuda is available the model will automatically be used in cuda mode and also the input variables will be switched to cuda . Input For prediction the following will happen to the tree different encapsulations of input arrays: A single array: Will be passed directly as the only argument to model call: model(Variable(from_numpy(x))) A list of arrays: The model will be called with the list of converted array as args (e.g.: model(*list_of_variables) ) A dictionary of arrays: The model will be called with the dictionary of converted array as kwargs (e.g.: model(**dict_of_variables) ) Output The model return values will be converted back into encapsulations of numpy arrays, where: a single Variable object will be converted into a numpy arrays lists of Variable objects will be converted into a list of numpy arrays in the same order and sklearn type models The batch generated by the dataloader will be passed on directly to the SciKit-learn model using model.predict(x) , model.predict_proba(x) or model.predict_log_proba (depending on the predict_method argument). tensorflow type models Input The feed_dict for running a tensorflow session is generated by converting the batch samples into the feed_dict using input_nodes defined in the args section of the model.yaml. For prediction the following will happen to the tree different encapsulations of input arrays: If input_nodes is a single string the model will be fed with a dictionary {input_ops: x} If input_nodes is a list then the batch is also exptected to be a list in the corresponding order and the feed dict will be created from that. If input_nodes is a dictionary then the batch is also exptected to be a dictionary with the same keys and the feed dict will be created from that. Output The return value of the tensorflow model is returned without further transformations and the model outpu schema defined in the schema field of model.yaml has to match that. dependencies One of the core elements of ensuring functionality of a model is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future. If your model is a python-based model and you have not tested whether your model works in python 2 and python 3, then make sure that you also add the correct python version as a dependency e.g.: python=2.7 . conda Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used. pip Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). postprocessing The postprocessing section of a model.yaml is necessary to indicate that a model is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your model up for variant effect prediction, please take a look at the documentation of variant effect prediction.","title":"model.yaml"},{"location":"contributing/02_Writing_model.yaml/#modelyaml","text":"The model.yaml file describes the individual model in the model zoo. It defines its dependencies, framework, architecture, input / output schema, general information and more. Correct defintions in the model.yaml enable to make full use of Kipoi features and make sure that a model can be executed at any point in future. To help understand the synthax of YAML please take a look at: YAML Synthax Basics Here is an example model.yaml : type: keras # use `kipoi.model.KerasModel` args: # arguments of `kipoi.model.KerasModel` arch: model_files/model.json weights: model_files/weights.h5 default_dataloader: . # path to the dataloader directory. Here it's defined in the same directory info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species version: 0.1 # optional cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python=3.5 - h5py # - soumith::pytorch # specify packages from other channels via <channel>::<package> pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" The model.yaml file has the following mandatory fields:","title":"model.yaml"},{"location":"contributing/02_Writing_model.yaml/#type","text":"The model type refers to base framework which the model was defined in. Kipoi comes with a support for Keras, PyTorch, SciKit-learn and tensorflow models. To indicate which kind of model will be used the following values for type are allowed: keras , pytorch , sklearn , tensorflow , and custom . The model type is required to find the right internal prepresentation of a model within Kipoi, which enables loading weights and architecture correctly and offers to have a unified API across frameworks. In the model.yaml file the definition of a Keras model would like this: type: keras","title":"type"},{"location":"contributing/02_Writing_model.yaml/#args","text":"Model arguments define where the files are files and functions are located to instantiate the model. Most entries of args will contain paths to files, those paths are relative to the location of the model.yaml file. The correct definition of args depends on the type that was selected:","title":"args"},{"location":"contributing/02_Writing_model.yaml/#keras-models","text":"For Keras models the following args are available: weights : File path to the hdf5 weights or the hdf5 Keras model. arch : Architecture json model. If None, weights is assumed to speficy the whole model custom_objects : Python file defining the custom Keras objects in a OBJECTS dictionary backend : Keras backend to use ('tensorflow', 'theano', 'cntk') image_dim_ordering : 'tf' or 'th' : Whether the model was trained with using 'tf' ('channels_last') or 'th' ('cannels_first') dimension ordering. The Keras framework offers different ways to store model architecture and weights: Architecture and weights can be stored separately: type: keras args: arch: model_files/model.json weights: model_files/weights.h5 The architecture can be stored together with the weights: type: keras args: weights: model_files/model.h5 In Keras models can have custom layers, which then have to be available at the instantiation of the Keras model, those should be stored in one python file that comes with the model architecture and weights. This file defines a dictionary containing custom Keras components called OBJECTS . These objects will be added to custom_objects when loading the model with keras.models.load_model . Example of a custom_keras_objects.py : from concise.layers import SplineT OBJECTS = {\"SplineT\": SplineT} Example of the corresponding model.yaml entry: type: keras args: ... custom_objects: model_files/custom_keras_objects.py Here all the objects present in model_files/custom_keras_objects.py will be made available to Keras when loading the model.","title":"keras models"},{"location":"contributing/02_Writing_model.yaml/#pytorch-models","text":"Pytorch offers much freedom as to how the model is stored. In Kipoi a pytorch model has the following args : file , build_fn , weights . If cuda is available the model will automatically be switched to cuda mode, so the user does not have to take care of that and the build_fn should not attempt to do this conversion. The following ways of instantiating a model are supported: Build function: In the example below Kipoi expects that when calling get_full_model() (which is defined in model_files/model_def.py ) A pytorch model is returned that for which the weights have already been loaded. type: pytorch args: file: model_files/model_def.py build_fn: get_full_model Build function + weights: In the example below the model is instantiated by calling get_model() which can be found in model_files/model_def.py . After that the weights will be loaded by executing model.load_state_dict(torch.load(weights)) . type: pytorch args: file: model_files/model_def.py build_fn: get_model weights: model_files/weights.pth Architecture and weights in one file: In this case Kipoi assumes that model = torch.load(weights) will be a valid pytorch model. Care has to be taken when storing the architecture a model this way as only standard pytorch layers will be loaded correctly, please see the pytorch documentation for details. type: pytorch args: weights: model_files/model.pth","title":"pytorch models"},{"location":"contributing/02_Writing_model.yaml/#sklearn-models","text":"SciKit-learn models can be loaded from a pickle file as defined below. The command used for loading is: joblib.load(pkl_file) type: sklearn args: pkl_file: model_files/model.pkl predict_method: predict_proba # Optional. predict by default. Available: predict, predict_proba, predict_log_proba","title":"sklearn models"},{"location":"contributing/02_Writing_model.yaml/#tensorflow-models","text":"Tensorflow models are expected to be stored by calling saver = tf.train.Saver(); saver.save(checkpoint_path) . The input_nodes argument is then a string, list of strings or dictionary of strings that define the input node names. The target_nodes argument is a string, list of strings or dictionary of strings that define the model target node names. type: tensorflow args: input_nodes: \"inputs\" target_nodes: \"preds\" checkpoint_path: \"model_files/model.tf\" If a model requires a constant feed of data which is not provided by the dataloader the const_feed_dict_pkl argument can be defined additionally to the above. Values given in the pickle file will be added to the batch samples created by the dataloader. If values with identical keys have been created by the dataloader they will be overwritten with what is given in const_feed_dict_pkl . type: tensorflow args: ... const_feed_dict_pkl: \"model_files/const_feed_dict.pkl\"","title":"tensorflow models"},{"location":"contributing/02_Writing_model.yaml/#custom-models","text":"It is possible to defined a model class independent of the ones which are made available in Kipoi. In that case the contributor-defined Model class must be a subclass of BaseModel defined in kipoi.model . Custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! If for example a custom model class definition ( MyModel ) lies in a file my_model.py , then the model.yaml will contain: type: custom args: file: my_model.py object: MyModel Kipoi will then use an instance of MyModel as a model. Keep in mind that MyModel has to be subclass of BaseModel , which in other words means that def predict_on_batch(self, x) has to be implemented. So if batch is for example what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to work.","title":"custom models"},{"location":"contributing/02_Writing_model.yaml/#info","text":"The info field of a model.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the model. A short description of what it does and what it is designed for. version : Model version license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the model and its use cases cite_as : Link to the journal, arXiv, ... trained_on : Description of the training dataset training_procedure : Description of the training procedure A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com - name: Second Author doc: My fancy model description version: 1.0 license: GNU tags: - TFBS - tag2 cite_as: http://www.the_journal.com/mypublication trained_on: The XXXX dataset from YYYY training_procedure: 10-fold cross validation","title":"info"},{"location":"contributing/02_Writing_model.yaml/#default_dataloader","text":"The default_dataloader points to the location of the dataloader.yaml file. By default this will be in the same folder as the model.yaml file, in which case default_dataloader doesn't have to be defined. If dataloader.yaml lies in different subfolder then default_dataloader: path/to/folder would be used where dataloader.yaml would lie in folder .","title":"default_dataloader"},{"location":"contributing/02_Writing_model.yaml/#schema","text":"Schema defines what the model inputs and outputs are, what they consist in and what the dimensions are. schema contains two categories inputs and targets which each specify the shapes of the model input and model output. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A dictionary of numpy arrays as inputs or targets: schema: inputs: seq: shape: (1000,4) inp2: shape: (10)","title":"schema"},{"location":"contributing/02_Writing_model.yaml/#inputs","text":"The inputs fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input , not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ), which is important for variant effect prediction.","title":"inputs"},{"location":"contributing/02_Writing_model.yaml/#targets","text":"The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model input name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line).","title":"targets"},{"location":"contributing/02_Writing_model.yaml/#how-model-types-handle-schemas","text":"The different model types handle those three different encapsulations of numpy arrays differently:","title":"How model types handle schemas"},{"location":"contributing/02_Writing_model.yaml/#keras-type-models","text":"","title":"keras type models"},{"location":"contributing/02_Writing_model.yaml/#input","text":"In case a Keras model is used the batch produced by the dataloader is passed on as it is to the model.predict_on_batch() function. So if for example a dictionary is defined in the model.yaml and that is produced by the dataloader then this dicationary is passed on to model.predict_on_batch() .","title":"Input"},{"location":"contributing/02_Writing_model.yaml/#output","text":"The model is expected to return the schema that is defined in model.yaml. If for example a model returns a list of numpy arrays then that has to be defined correctly in the model.yaml schema.","title":"Output"},{"location":"contributing/02_Writing_model.yaml/#pytorch-type-models","text":"Pytorch needs torch.autograd.Variable instances to work. Hence all inputs are automatically converted into Variable objects and results are converted back into numpy arrays transparently. If cuda is available the model will automatically be used in cuda mode and also the input variables will be switched to cuda .","title":"pytorch type models"},{"location":"contributing/02_Writing_model.yaml/#input_1","text":"For prediction the following will happen to the tree different encapsulations of input arrays: A single array: Will be passed directly as the only argument to model call: model(Variable(from_numpy(x))) A list of arrays: The model will be called with the list of converted array as args (e.g.: model(*list_of_variables) ) A dictionary of arrays: The model will be called with the dictionary of converted array as kwargs (e.g.: model(**dict_of_variables) )","title":"Input"},{"location":"contributing/02_Writing_model.yaml/#output_1","text":"The model return values will be converted back into encapsulations of numpy arrays, where: a single Variable object will be converted into a numpy arrays lists of Variable objects will be converted into a list of numpy arrays in the same order and","title":"Output"},{"location":"contributing/02_Writing_model.yaml/#sklearn-type-models","text":"The batch generated by the dataloader will be passed on directly to the SciKit-learn model using model.predict(x) , model.predict_proba(x) or model.predict_log_proba (depending on the predict_method argument).","title":"sklearn type models"},{"location":"contributing/02_Writing_model.yaml/#tensorflow-type-models","text":"","title":"tensorflow type models"},{"location":"contributing/02_Writing_model.yaml/#input_2","text":"The feed_dict for running a tensorflow session is generated by converting the batch samples into the feed_dict using input_nodes defined in the args section of the model.yaml. For prediction the following will happen to the tree different encapsulations of input arrays: If input_nodes is a single string the model will be fed with a dictionary {input_ops: x} If input_nodes is a list then the batch is also exptected to be a list in the corresponding order and the feed dict will be created from that. If input_nodes is a dictionary then the batch is also exptected to be a dictionary with the same keys and the feed dict will be created from that.","title":"Input"},{"location":"contributing/02_Writing_model.yaml/#output_2","text":"The return value of the tensorflow model is returned without further transformations and the model outpu schema defined in the schema field of model.yaml has to match that.","title":"Output"},{"location":"contributing/02_Writing_model.yaml/#dependencies","text":"One of the core elements of ensuring functionality of a model is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future. If your model is a python-based model and you have not tested whether your model works in python 2 and python 3, then make sure that you also add the correct python version as a dependency e.g.: python=2.7 .","title":"dependencies"},{"location":"contributing/02_Writing_model.yaml/#conda","text":"Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used.","title":"conda"},{"location":"contributing/02_Writing_model.yaml/#pip","text":"Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ).","title":"pip"},{"location":"contributing/02_Writing_model.yaml/#postprocessing","text":"The postprocessing section of a model.yaml is necessary to indicate that a model is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your model up for variant effect prediction, please take a look at the documentation of variant effect prediction.","title":"postprocessing"},{"location":"contributing/03_Writing_dataloader.yaml/","text":"dataloader.yaml The dataloader.yaml file describes how a dataloader for a certain model can be created and how it has to be set up. A model without functional dataloader is as bad as a model that doesn't work, so the correct setup of the dataloader.yaml is essential for the use of a model in the zoo. Make sure you have read Writing dataloader.py . To help understand the synthax of YAML please take a look at: YAML Synthax Basics Here is an example dataloader.yaml : type: Dataset defined_as: dataloader.py::MyDataset # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: example_files/features.csv # example files targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: example_files/targets.csv optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org version: 0.1 doc: Model predicting the Iris species dependencies: conda: - python=3.5 - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: Features in cm: sepal length, sepal width, petal length, petal width. targets: shape: (3, ) doc: One-hot encoded array of classes: setosa, versicolor, virginica. metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: type: int doc: Just an example metadata column type The type of the dataloader indicates from which class the dataloader is inherits. It has to be one of the following values: PreloadedDataset Dataset BatchDataset SampleIterator SampleGenerator BatchIterator BatchGenerator defined_as defined_as indicates where the dataloader class can be found. It is a string value of path/to/file.py::class_name with a the relative path from where the dataloader.yaml lies. E.g.: model_files/dataloader.py::MyDataLoader . This class will then be instantiated by Kipoi with keyword arguments that have to be mentioned explicitely in args (see below). args A dataloader will always require arguments, they might for example be a path to the reference genome fasta file, a bed file that defines which regions should be investigated, etc. Dataloader arguments are given defined as a yaml dictionary with argument names as keys, e.g.: args: reference_fasta: example: example_files/chr22.fa argument_2: example: example_files/example_input.txt An argument has the following fields: doc : A free text field describing the argument example : A value that can be used to demonstrate the functionality of the dataloader and of the entire model. Those example files are very useful for users and for automatic testing procedures. For example the command line call kipoi test uses the exmaple values given for dataloader arguments to assess that a model can be used and is functional. It is therefore important to submit all necessary example files with the model. type : Optional: datatype of the argument ( str , bool , int , float ) optional : Optional: Boolean flag ( true / false ) for an argument if it is optional. info The info field of a dataloader.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the dataloader. A short description of what it does. version : Version of the dataloader license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the dataloader and its use cases A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com doc: Datalaoder for my fancy model description version: 1.0 license: GNU tags: - TFBS - tag2 output_schema output_schema defines what the dataloader outputs are, what they consist in, what the dimensions are and some additional meta data. output_schema contains three categories inputs , targets and metadata . inputs and targets each specify the shapes of data generated for the model input and model. Offering the targets option enables the opportunity to possibly train models with the same dataloader. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: output_schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: output_schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A list of numpy arrays as inputs or targets: output_schema: inputs: seq: shape: (1000,4) inp2: shape: (10) inputs The inputs fields of output_schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input, not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ), which is important for variant effect prediction. associated_metadata : Link the respective model input to metadata, such as a genomic region. E.g: If model input is a DNA sequence, then metadata may contain the genomic region from where it was extracted. If the associated metadata field is called ranges then associated_metadata: ranges has to be set. targets The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model target name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line). metadata Metadata fields capture additional information on the data generated by the dataloader. So for example a model input can be linked to a metadata field using its associated_metadata flag (see above). The metadata fields themselves are yaml dictionaries where the name of the metadata field is the key of dictionary and possible attributes are: doc : A free text description of the metadata element type : The datatype of the metadata field: str , int , float , array , GenomicRanges . Where the convenience class GenomicRanges is defined in kipoi.metadata , which is essentially an in-memory representation of a bed file. Definition of metadata is essential for postprocessing algorihms as variant effect prediction. Please refer to their detailed description for their requirements. An example of the defintion of dataloader.yaml with metadata can be seen here: output_schema: inputs: - name: seq shape: (1000,4) associated_metadata: my_ranges - name: inp2 shape: (10) ... metadata: my_ranges: type: GenomicRanges doc: Region from where inputs.seq was extracted dependencies One of the core elements of ensuring functionality of a dataloader is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future. conda Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used. pip Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). postprocessing The postprocessing section of a dataloader.yaml is necessary to indicate that a dataloader is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your dataloader up for variant effect prediction, please take a look at the documentation of variant effect prediction.","title":"dataloader.yaml"},{"location":"contributing/03_Writing_dataloader.yaml/#dataloaderyaml","text":"The dataloader.yaml file describes how a dataloader for a certain model can be created and how it has to be set up. A model without functional dataloader is as bad as a model that doesn't work, so the correct setup of the dataloader.yaml is essential for the use of a model in the zoo. Make sure you have read Writing dataloader.py . To help understand the synthax of YAML please take a look at: YAML Synthax Basics Here is an example dataloader.yaml : type: Dataset defined_as: dataloader.py::MyDataset # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: example_files/features.csv # example files targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: example_files/targets.csv optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org version: 0.1 doc: Model predicting the Iris species dependencies: conda: - python=3.5 - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: Features in cm: sepal length, sepal width, petal length, petal width. targets: shape: (3, ) doc: One-hot encoded array of classes: setosa, versicolor, virginica. metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: type: int doc: Just an example metadata column","title":"dataloader.yaml"},{"location":"contributing/03_Writing_dataloader.yaml/#type","text":"The type of the dataloader indicates from which class the dataloader is inherits. It has to be one of the following values: PreloadedDataset Dataset BatchDataset SampleIterator SampleGenerator BatchIterator BatchGenerator","title":"type"},{"location":"contributing/03_Writing_dataloader.yaml/#defined_as","text":"defined_as indicates where the dataloader class can be found. It is a string value of path/to/file.py::class_name with a the relative path from where the dataloader.yaml lies. E.g.: model_files/dataloader.py::MyDataLoader . This class will then be instantiated by Kipoi with keyword arguments that have to be mentioned explicitely in args (see below).","title":"defined_as"},{"location":"contributing/03_Writing_dataloader.yaml/#args","text":"A dataloader will always require arguments, they might for example be a path to the reference genome fasta file, a bed file that defines which regions should be investigated, etc. Dataloader arguments are given defined as a yaml dictionary with argument names as keys, e.g.: args: reference_fasta: example: example_files/chr22.fa argument_2: example: example_files/example_input.txt An argument has the following fields: doc : A free text field describing the argument example : A value that can be used to demonstrate the functionality of the dataloader and of the entire model. Those example files are very useful for users and for automatic testing procedures. For example the command line call kipoi test uses the exmaple values given for dataloader arguments to assess that a model can be used and is functional. It is therefore important to submit all necessary example files with the model. type : Optional: datatype of the argument ( str , bool , int , float ) optional : Optional: Boolean flag ( true / false ) for an argument if it is optional.","title":"args"},{"location":"contributing/03_Writing_dataloader.yaml/#info","text":"The info field of a dataloader.yaml file contains general information about the model. authors : a list of authors with the field: name , and the optional fields: github and email . Where the github name is the github user id of the respective author doc : Free text documentation of the dataloader. A short description of what it does. version : Version of the dataloader license : String indicating the license, if not defined it defaults to MIT tags : A list of key words describing the dataloader and its use cases A dummy example could look like this: info: authors: - name: My Name github: myGithubName email: my@email.com doc: Datalaoder for my fancy model description version: 1.0 license: GNU tags: - TFBS - tag2","title":"info"},{"location":"contributing/03_Writing_dataloader.yaml/#output_schema","text":"output_schema defines what the dataloader outputs are, what they consist in, what the dimensions are and some additional meta data. output_schema contains three categories inputs , targets and metadata . inputs and targets each specify the shapes of data generated for the model input and model. Offering the targets option enables the opportunity to possibly train models with the same dataloader. In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition: A single numpy array as input or target: output_schema: inputs: name: seq shape: (1000,4) A list of numpy arrays as inputs or targets: output_schema: targets: - name: seq shape: (1000,4) - name: inp2 shape: (10) A list of numpy arrays as inputs or targets: output_schema: inputs: seq: shape: (1000,4) inp2: shape: (10)","title":"output_schema"},{"location":"contributing/03_Writing_dataloader.yaml/#inputs","text":"The inputs fields of output_schema may be lists, dictionaries or single occurences of the following entries: shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of (1000, 4) inputs shape: (1000, 4) should be set. If a dimension is of variable size then the numerical should be replaced by None . doc : A free text description of the model input name : Name of model input, not required if input is a dictionary. special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ), which is important for variant effect prediction. associated_metadata : Link the respective model input to metadata, such as a genomic region. E.g: If model input is a DNA sequence, then metadata may contain the genomic region from where it was extracted. If the associated metadata field is called ranges then associated_metadata: ranges has to be set.","title":"inputs"},{"location":"contributing/03_Writing_dataloader.yaml/#targets","text":"The targets fields of schema may be lists, dictionaries or single occurences of the following entries: shape : Required: Details see in input doc : A free text description of the model target name : Name of model target, not required if target is a dictionary. column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line).","title":"targets"},{"location":"contributing/03_Writing_dataloader.yaml/#metadata","text":"Metadata fields capture additional information on the data generated by the dataloader. So for example a model input can be linked to a metadata field using its associated_metadata flag (see above). The metadata fields themselves are yaml dictionaries where the name of the metadata field is the key of dictionary and possible attributes are: doc : A free text description of the metadata element type : The datatype of the metadata field: str , int , float , array , GenomicRanges . Where the convenience class GenomicRanges is defined in kipoi.metadata , which is essentially an in-memory representation of a bed file. Definition of metadata is essential for postprocessing algorihms as variant effect prediction. Please refer to their detailed description for their requirements. An example of the defintion of dataloader.yaml with metadata can be seen here: output_schema: inputs: - name: seq shape: (1000,4) associated_metadata: my_ranges - name: inp2 shape: (10) ... metadata: my_ranges: type: GenomicRanges doc: Region from where inputs.seq was extracted","title":"metadata"},{"location":"contributing/03_Writing_dataloader.yaml/#dependencies","text":"One of the core elements of ensuring functionality of a dataloader is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the conda and pip sections respectively. Both can either be defined as a list of packages or as a text file (ending in .txt ) which lists the dependencies. Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: package>=1.0 is very likely to break at some point in future.","title":"dependencies"},{"location":"contributing/03_Writing_dataloader.yaml/#conda","text":"Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ). If conda packages need to be loaded from a channel then the nomenclature channel_name::package_name can be used.","title":"conda"},{"location":"contributing/03_Writing_dataloader.yaml/#pip","text":"Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in .txt ).","title":"pip"},{"location":"contributing/03_Writing_dataloader.yaml/#postprocessing","text":"The postprocessing section of a dataloader.yaml is necessary to indicate that a dataloader is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your dataloader up for variant effect prediction, please take a look at the documentation of variant effect prediction.","title":"postprocessing"},{"location":"contributing/04_Writing_dataloader.py/","text":"Dataloader The main aim of a dataloader is to generate batches of data with which a model can be run. It therefore has to return a dictionary with three keys: inputs targets (optional) metadata (optional). As the names suggest, the inputs will get feeded to the model to make the predictions and targets could be used to train the model. The metadata field is used to give additional information about the samples (like sample ID, or genomic ranges for DNA-sequence based models). In a batch of data returned by the dataloader, all three fields can be further nested - i.e. inputs can be a list of numpy arrays or a dictionary of numpy arrays. The only restriction is that the leaf objects are numpy arrays and that the first axis (batch dimension) is the same for all arrays. Note that the inputs and targets have to be compatible with the model you are using. Keras, for instance, can accept as inputs and targets all three options: single numpy array, list of numpy arrays, dictionary of numpy arrays (note: to use as input a dictionary of numpy arrays you have to use the functional API and specify the name fields in the keras.layers.Input layer). On the other hand, the Scikit-learn models only allow the inputs and targets to be a single 2-dimensional numpy array. Conceptionally, there are three ways how you can write a dataloader. The dataloader can either yield: individual samples batches of data whole dataset Note that when a dataloader returns individual samples, the returned numpy arrays shouldn't contain the batch axis. The batch axis will get generated by Kipoi when batching the samples. Also, the samples may contain non-numpy array scalar types like bool , float , int , str . These will later get stacked into a one-dimensional numpy array. Dataloader types Specifically, a dataloader has to inherit from one of the following classes defined in kipoi.data : PreloadedDataset Function that returns the whole dataset as a nested dictionary/list of numpy arrays useful when: the dataset is expected to load quickly and fit into the memory Dataset Class that inherits from kipoi.data.Dataset and implements __len__ and __getitem__ methods. __getitem__ returns a single sample from the dataset. useful when: dataset length is easy to infer, there are no significant performance gain when reading data of the disk in batches BatchDataset Class that inherits from kipoi.data.BatchDataset and implements __len__ and __getitem__ methods. __getitem__ returns a single batch of samples from the dataset. useful when: dataset length is easy to infer, and there is a significant performance gain when reading data of the disk in batches SampleIterator Class that inherits from kipoi.data.SampleIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there are no significant performance gain when reading data of the disk in batches BatchIterator Class that inherits from kipoi.data.BatchIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single batch of samples sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there is a significant performance gain when reading data of the disk in batches SampleGenerator A generator function that yields a single sample from the dataset and returns when all the samples were yielded. useful when: same as for SampleIterator , but can be typically implemented in fewer lines of code BatchGenerator A generator function that yields a single batch of samples from the dataset and returns when all the samples were yielded. useful when: same as for BatchIterator , but can be typically implemented in fewer lines of code Here is a table showing the (recommended) requirements for each dataloader type: Dataloader type Length known? Significant benefit from loading data in batches? Fits into memory and loads quickly? PreloadedDataset yes yes yes Dataset yes no no BatchDataset yes yes no SampleIterator no no no BatchIterator no yes no SampleGenerator no no no BatchGenerator no yes no Dataset example Here is an example dataloader that gets as input a fasta file and a bed file and returns a one-hot encoded sequence (under 'inputs') along with the used genomic interval (under 'metadata/ranges'). Note that we additionally defined the build method. This is useful when writing dataloaders that will support dataloading in parallel: the build method gets executed on each worker individually. Hence, it's recommended to initialize file handles in the build method instead of the __init__ method. from __future__ import absolute_import, division, print_function import numpy as np from pybedtools import BedTool from genomelake.extractors import FastaExtractor from kipoi.data import Dataset from kipoi.metadata import GenomicRanges class SeqDataset(Dataset): \"\"\" Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file): self.bt = BedTool(intervals_file) self.fasta_file = fasta_file self.fasta_extractor = None def __len__(self): return len(self.bt) def __getitem__(self, idx): if self.fasta_extractor is None: self.fasta_extractor = FastaExtractor(self.fasta_file) interval = self.bt[idx] seq = np.squeeze(self.fasta_extractor([interval]), axis=0) return { \"inputs\": seq, # lacks targets \"metadata\": { \"ranges\": GenomicRanges.from_interval(interval) } } Note that we have initialized the fasta_extractor on the first call of __getitem__ . The reason for this is that when we use parallel dataloading, each process will get a copy of the SeqDataset(...) object. Upon the first call of __getitem__ the extractor and hence the underlying file-handle will be setup for each worker independently. Further examples To see examples of other dataloaders, run kipoi init from the command-line and choose each time a different dataloader_type. $ kipoi init INFO [kipoi.cli.main] Initializing a new Kipoi model ... Select dataloader_type: 1 - Dataset 2 - PreloadedDataset 3 - BatchDataset 4 - SampleIterator 5 - SampleGenerator 6 - BatchIterator 7 - BatchGenerator Choose from 1, 2, 3, 4, 5, 6, 7 [1]: The generated model directory will contain a working implementation of a dataloader.","title":"dataloader.py"},{"location":"contributing/04_Writing_dataloader.py/#dataloader","text":"The main aim of a dataloader is to generate batches of data with which a model can be run. It therefore has to return a dictionary with three keys: inputs targets (optional) metadata (optional). As the names suggest, the inputs will get feeded to the model to make the predictions and targets could be used to train the model. The metadata field is used to give additional information about the samples (like sample ID, or genomic ranges for DNA-sequence based models). In a batch of data returned by the dataloader, all three fields can be further nested - i.e. inputs can be a list of numpy arrays or a dictionary of numpy arrays. The only restriction is that the leaf objects are numpy arrays and that the first axis (batch dimension) is the same for all arrays. Note that the inputs and targets have to be compatible with the model you are using. Keras, for instance, can accept as inputs and targets all three options: single numpy array, list of numpy arrays, dictionary of numpy arrays (note: to use as input a dictionary of numpy arrays you have to use the functional API and specify the name fields in the keras.layers.Input layer). On the other hand, the Scikit-learn models only allow the inputs and targets to be a single 2-dimensional numpy array. Conceptionally, there are three ways how you can write a dataloader. The dataloader can either yield: individual samples batches of data whole dataset Note that when a dataloader returns individual samples, the returned numpy arrays shouldn't contain the batch axis. The batch axis will get generated by Kipoi when batching the samples. Also, the samples may contain non-numpy array scalar types like bool , float , int , str . These will later get stacked into a one-dimensional numpy array.","title":"Dataloader"},{"location":"contributing/04_Writing_dataloader.py/#dataloader-types","text":"Specifically, a dataloader has to inherit from one of the following classes defined in kipoi.data : PreloadedDataset Function that returns the whole dataset as a nested dictionary/list of numpy arrays useful when: the dataset is expected to load quickly and fit into the memory Dataset Class that inherits from kipoi.data.Dataset and implements __len__ and __getitem__ methods. __getitem__ returns a single sample from the dataset. useful when: dataset length is easy to infer, there are no significant performance gain when reading data of the disk in batches BatchDataset Class that inherits from kipoi.data.BatchDataset and implements __len__ and __getitem__ methods. __getitem__ returns a single batch of samples from the dataset. useful when: dataset length is easy to infer, and there is a significant performance gain when reading data of the disk in batches SampleIterator Class that inherits from kipoi.data.SampleIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there are no significant performance gain when reading data of the disk in batches BatchIterator Class that inherits from kipoi.data.BatchIterator and implements __iter__ and __next__ ( next in python 2). __next__ returns a single batch of samples sample from the dataset or raises StopIteration if all the samples were already returned. useful when: the dataset length is not know in advance or is difficult to infer, and there is a significant performance gain when reading data of the disk in batches SampleGenerator A generator function that yields a single sample from the dataset and returns when all the samples were yielded. useful when: same as for SampleIterator , but can be typically implemented in fewer lines of code BatchGenerator A generator function that yields a single batch of samples from the dataset and returns when all the samples were yielded. useful when: same as for BatchIterator , but can be typically implemented in fewer lines of code Here is a table showing the (recommended) requirements for each dataloader type: Dataloader type Length known? Significant benefit from loading data in batches? Fits into memory and loads quickly? PreloadedDataset yes yes yes Dataset yes no no BatchDataset yes yes no SampleIterator no no no BatchIterator no yes no SampleGenerator no no no BatchGenerator no yes no","title":"Dataloader types"},{"location":"contributing/04_Writing_dataloader.py/#dataset-example","text":"Here is an example dataloader that gets as input a fasta file and a bed file and returns a one-hot encoded sequence (under 'inputs') along with the used genomic interval (under 'metadata/ranges'). Note that we additionally defined the build method. This is useful when writing dataloaders that will support dataloading in parallel: the build method gets executed on each worker individually. Hence, it's recommended to initialize file handles in the build method instead of the __init__ method. from __future__ import absolute_import, division, print_function import numpy as np from pybedtools import BedTool from genomelake.extractors import FastaExtractor from kipoi.data import Dataset from kipoi.metadata import GenomicRanges class SeqDataset(Dataset): \"\"\" Args: intervals_file: bed3 file containing intervals fasta_file: file path; Genome sequence \"\"\" def __init__(self, intervals_file, fasta_file): self.bt = BedTool(intervals_file) self.fasta_file = fasta_file self.fasta_extractor = None def __len__(self): return len(self.bt) def __getitem__(self, idx): if self.fasta_extractor is None: self.fasta_extractor = FastaExtractor(self.fasta_file) interval = self.bt[idx] seq = np.squeeze(self.fasta_extractor([interval]), axis=0) return { \"inputs\": seq, # lacks targets \"metadata\": { \"ranges\": GenomicRanges.from_interval(interval) } } Note that we have initialized the fasta_extractor on the first call of __getitem__ . The reason for this is that when we use parallel dataloading, each process will get a copy of the SeqDataset(...) object. Upon the first call of __getitem__ the extractor and hence the underlying file-handle will be setup for each worker independently.","title":"Dataset example"},{"location":"contributing/04_Writing_dataloader.py/#further-examples","text":"To see examples of other dataloaders, run kipoi init from the command-line and choose each time a different dataloader_type. $ kipoi init INFO [kipoi.cli.main] Initializing a new Kipoi model ... Select dataloader_type: 1 - Dataset 2 - PreloadedDataset 3 - BatchDataset 4 - SampleIterator 5 - SampleGenerator 6 - BatchIterator 7 - BatchGenerator Choose from 1, 2, 3, 4, 5, 6, 7 [1]: The generated model directory will contain a working implementation of a dataloader.","title":"Further examples"},{"location":"contributing/05_Writing_model.py/","text":"model.py Custom models enable using any other framework or non-deep learning predictive model to be integrated within Kipoi. In general it is highly advisable not to use custom models if there is an implementation for the model that should be integrated, in other words: If your model is a pytorch model, please use the pytorch model type in Kipoi rather than defining your own custom model type. Also, custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! The use of a custom model requires definition of a Kipoi-compliant model object, which can then be referred to by the model.yaml file. The model class has to be a subclass of BaseModel defined in kipoi.model , which in other words means that def predict_on_batch(self, x) has to be implemented. So for example if batch is what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to run the model prediction on the given input. A very simple version of such a model definition that can be stored in for example model_files/model.py may be: from kipoi.model import BaseModel def load_my_model(): # Loading code here return model class MyModel(BaseModel): def __init__(self): self.model = load_my_model() # Execute model prediction for input data def predict_on_batch(self, x): return self.model.predict(x) This can then be integrated in the model.yaml in the following way: type: custom args: file: model_files/model.py object: MyModel ...","title":"model.py"},{"location":"contributing/05_Writing_model.py/#modelpy","text":"Custom models enable using any other framework or non-deep learning predictive model to be integrated within Kipoi. In general it is highly advisable not to use custom models if there is an implementation for the model that should be integrated, in other words: If your model is a pytorch model, please use the pytorch model type in Kipoi rather than defining your own custom model type. Also, custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the predict_on_batch function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo! The use of a custom model requires definition of a Kipoi-compliant model object, which can then be referred to by the model.yaml file. The model class has to be a subclass of BaseModel defined in kipoi.model , which in other words means that def predict_on_batch(self, x) has to be implemented. So for example if batch is what the dataloader returns for a batch then predict_on_batch(batch['inputs']) has to run the model prediction on the given input. A very simple version of such a model definition that can be stored in for example model_files/model.py may be: from kipoi.model import BaseModel def load_my_model(): # Loading code here return model class MyModel(BaseModel): def __init__(self): self.model = load_my_model() # Execute model prediction for input data def predict_on_batch(self, x): return self.model.predict(x) This can then be integrated in the model.yaml in the following way: type: custom args: file: model_files/model.py object: MyModel ...","title":"model.py"},{"location":"contributing/06_dumping_models_programatically/","text":"Contributing multiple very similar models Consider an example where multiple models were trained, each for a different cell-lines (case for CpGenie). Here is the final folder structure of the contributed model group (simplifed from te CpGenie model) cell_line_1 \u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py \u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml \u251c\u2500\u2500 example_files -> ../template/example_files \u251c\u2500\u2500 model_files \u2502 \u2514\u2500\u2500 model.h5 \u2514\u2500\u2500 model.yaml -> ../template/model.yaml cell_line_2 \u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py \u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml \u251c\u2500\u2500 example_files -> ../template/example_files \u251c\u2500\u2500 model_files \u2502 \u2514\u2500\u2500 model.h5 \u2514\u2500\u2500 model.yaml -> ../template/model.yaml template \u251c\u2500\u2500 dataloader.py \u251c\u2500\u2500 dataloader.yaml \u251c\u2500\u2500 example_files \u2502 \u251c\u2500\u2500 hg38_chr22.fa \u2502 \u251c\u2500\u2500 hg38_chr22.fa.fai \u2502 \u2514\u2500\u2500 intervals.bed \u2514\u2500\u2500 model.yaml Makefile test_subset.txt template/ folder The template/ folder should contain all the common files or templates. This directory is ignored when listing models. Softlinks One option to prevent code duplication is to use soft-links. In the simplest case (as shown above), all files except model weights can be shared accross models. When selectively downloading files from git-lfs, Kipoi also considers soft-links and downloads the original files (e.g. when running kipoi predict my_model/cell_line_1 ... , the git-lfs files in my_model/template will also get downloaded). Note Make sure you are using relative soft-links (as shown above). # example code-snippet to dump of multiple Keras models # and to softlink the remaining files def get_model(cell_line): \"\"\"Returns the Keras model\"\"\" pass def write_model(root_path, cell_line): \"\"\"For a particular cell_line: - write out the model - softlink the other files from `template/` \"\"\" model_dir = os.path.join(root_path, cell_line) os.makedirs(os.path.join(model_dir, \"model_files\"), exist_ok=True) model = get_model(cell_line) model.save(os.path.join(model_dir, \"model_files/model.h5\")) symlink_files = [\"model.yaml\", \"example_files\", \"dataloader.yaml\", \"dataloader.py\"] for f in symlink_files: os.symlink(os.path.join(root_path, \"template\", f), os.path.join(model_dir, f)) for cell_line in all_cell_lines: write_model(\"my_model_path\", cell_line) Jinja templating Another option is to use template engines. Template engines are heavily used in web-development to dynamically generate html files. One of the most popular template engines is jinja . Template engines offer more flexibility over softlinks. With softlinks you can only re-use the whole file, while with templating you can choose which pieces of the file are shared and which ones are specific to each model. # template_model.yaml type: keras args: weights: model_files/model.h5 ... info: trained_on: DNase-seq of {{cell_line}} cell line ... schema: ... targets: name: output doc: DNA accessibility in {{cell_line}} cell line # Script to generate <cell line>/model.yaml from template_model.yaml import os from jinja2 import Template def render_template(template_path, output_path, context, mkdir=False): \"\"\"Render template with jinja Args: template_path: path to the jinja template output_path: path where to write the rendered template context: Dictionary containing context variable \"\"\" if mkdir: os.makedirs(os.path.dirname(output_path), exist_ok=True) with open(template_path, \"r\") as f: template = Template(f.read()) out = template.render(**context) with open(output_path, \"w\") as f: f.write(out) def write_model_yaml(root_path, cell_line): \"\"\"For a particular cell_line: - Generate `{cell_line}/model.yaml` \"\"\" render_template(os.path.join(root_path, \"template\", \"template_model.yaml\"), os.path.join(root_path, cell_line, \"model.yaml\"), context={\"cell_line\": cell_line}, mkdir=True) Importing common functions, classes In case the dataloaders or custom models vary between models and we want to re-use python code, we can import objects from modules in the template/ directory: import os import inspect # Get the directory of this python file filename = inspect.getframeinfo(inspect.currentframe()).filename this_path = os.path.dirname(os.path.abspath(filename)) # attach template to pythonpath import sys sys.path.append(os.path.join(this_path, \"../template\")) from model_template import TemplateModel class SpecificModel(TemplateModel): def __init__(self): super(SpecificModel, self).__init__(arg1=\"value\") test_subset.txt - Testing only some models Since many models are essentially the same, the automatic tests should only test one or few models. To specify which models to test, write the test_subset.txt file in the same directory level as the template/ folder and list the models you want to test. Examples: CpGenie/test_subset.txt : GM19239_ENCSR000DGH merged rbp_eclip/test_subset.txt : AARS Reproducible script Regardless of which approch you choose to take, consider writing a single script/Makefile in the model-group root (at the same directory level as template/ ). The script/Makefile should generate or softlink all the files given the template folder, making it easier to update the files later.","title":"Multiple very similar models"},{"location":"contributing/06_dumping_models_programatically/#contributing-multiple-very-similar-models","text":"Consider an example where multiple models were trained, each for a different cell-lines (case for CpGenie). Here is the final folder structure of the contributed model group (simplifed from te CpGenie model) cell_line_1 \u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py \u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml \u251c\u2500\u2500 example_files -> ../template/example_files \u251c\u2500\u2500 model_files \u2502 \u2514\u2500\u2500 model.h5 \u2514\u2500\u2500 model.yaml -> ../template/model.yaml cell_line_2 \u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py \u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml \u251c\u2500\u2500 example_files -> ../template/example_files \u251c\u2500\u2500 model_files \u2502 \u2514\u2500\u2500 model.h5 \u2514\u2500\u2500 model.yaml -> ../template/model.yaml template \u251c\u2500\u2500 dataloader.py \u251c\u2500\u2500 dataloader.yaml \u251c\u2500\u2500 example_files \u2502 \u251c\u2500\u2500 hg38_chr22.fa \u2502 \u251c\u2500\u2500 hg38_chr22.fa.fai \u2502 \u2514\u2500\u2500 intervals.bed \u2514\u2500\u2500 model.yaml Makefile test_subset.txt","title":"Contributing multiple very similar models"},{"location":"contributing/06_dumping_models_programatically/#template-folder","text":"The template/ folder should contain all the common files or templates. This directory is ignored when listing models.","title":"template/ folder"},{"location":"contributing/06_dumping_models_programatically/#softlinks","text":"One option to prevent code duplication is to use soft-links. In the simplest case (as shown above), all files except model weights can be shared accross models. When selectively downloading files from git-lfs, Kipoi also considers soft-links and downloads the original files (e.g. when running kipoi predict my_model/cell_line_1 ... , the git-lfs files in my_model/template will also get downloaded). Note Make sure you are using relative soft-links (as shown above). # example code-snippet to dump of multiple Keras models # and to softlink the remaining files def get_model(cell_line): \"\"\"Returns the Keras model\"\"\" pass def write_model(root_path, cell_line): \"\"\"For a particular cell_line: - write out the model - softlink the other files from `template/` \"\"\" model_dir = os.path.join(root_path, cell_line) os.makedirs(os.path.join(model_dir, \"model_files\"), exist_ok=True) model = get_model(cell_line) model.save(os.path.join(model_dir, \"model_files/model.h5\")) symlink_files = [\"model.yaml\", \"example_files\", \"dataloader.yaml\", \"dataloader.py\"] for f in symlink_files: os.symlink(os.path.join(root_path, \"template\", f), os.path.join(model_dir, f)) for cell_line in all_cell_lines: write_model(\"my_model_path\", cell_line)","title":"Softlinks"},{"location":"contributing/06_dumping_models_programatically/#jinja-templating","text":"Another option is to use template engines. Template engines are heavily used in web-development to dynamically generate html files. One of the most popular template engines is jinja . Template engines offer more flexibility over softlinks. With softlinks you can only re-use the whole file, while with templating you can choose which pieces of the file are shared and which ones are specific to each model. # template_model.yaml type: keras args: weights: model_files/model.h5 ... info: trained_on: DNase-seq of {{cell_line}} cell line ... schema: ... targets: name: output doc: DNA accessibility in {{cell_line}} cell line # Script to generate <cell line>/model.yaml from template_model.yaml import os from jinja2 import Template def render_template(template_path, output_path, context, mkdir=False): \"\"\"Render template with jinja Args: template_path: path to the jinja template output_path: path where to write the rendered template context: Dictionary containing context variable \"\"\" if mkdir: os.makedirs(os.path.dirname(output_path), exist_ok=True) with open(template_path, \"r\") as f: template = Template(f.read()) out = template.render(**context) with open(output_path, \"w\") as f: f.write(out) def write_model_yaml(root_path, cell_line): \"\"\"For a particular cell_line: - Generate `{cell_line}/model.yaml` \"\"\" render_template(os.path.join(root_path, \"template\", \"template_model.yaml\"), os.path.join(root_path, cell_line, \"model.yaml\"), context={\"cell_line\": cell_line}, mkdir=True)","title":"Jinja templating"},{"location":"contributing/06_dumping_models_programatically/#importing-common-functions-classes","text":"In case the dataloaders or custom models vary between models and we want to re-use python code, we can import objects from modules in the template/ directory: import os import inspect # Get the directory of this python file filename = inspect.getframeinfo(inspect.currentframe()).filename this_path = os.path.dirname(os.path.abspath(filename)) # attach template to pythonpath import sys sys.path.append(os.path.join(this_path, \"../template\")) from model_template import TemplateModel class SpecificModel(TemplateModel): def __init__(self): super(SpecificModel, self).__init__(arg1=\"value\")","title":"Importing common functions, classes"},{"location":"contributing/06_dumping_models_programatically/#test_subsettxt-testing-only-some-models","text":"Since many models are essentially the same, the automatic tests should only test one or few models. To specify which models to test, write the test_subset.txt file in the same directory level as the template/ folder and list the models you want to test. Examples: CpGenie/test_subset.txt : GM19239_ENCSR000DGH merged rbp_eclip/test_subset.txt : AARS","title":"test_subset.txt - Testing only some models"},{"location":"contributing/06_dumping_models_programatically/#reproducible-script","text":"Regardless of which approch you choose to take, consider writing a single script/Makefile in the model-group root (at the same directory level as template/ ). The script/Makefile should generate or softlink all the files given the template folder, making it easier to update the files later.","title":"Reproducible script"},{"location":"postprocessing/variant_effect_prediction/","text":"Variant effect prediction Variant effect prediction offers a simple way predict effects of SNVs using any model that uses DNA sequence as an input. Many different scoring methods can be chosen but the principle relies on in-silico mutagenesis (see below). The default input is a VCF and the default output again is a VCF annotated with predictions of variant effects. How it works This sketch highlights the overall functionality of variant effect prediction. More details are given in the chapters below. Dataloader output and a VCF are overlapped and the input DNA sequence is mutated as defined in the VCF. The reference and the alternative set of model inputs is predicted using the model and the differences are evaluated using a scoring function. The results are then stored in an annotated VCF. In-silico mutagenesis The principle relies on generating model predictions twice, once with DNA sequence that contains the reference and once with the alternative allele of a variant. Those predictions can then be compared in different ways to generate an effect prediction. Scoring methods Scoring methods that come with Kipoi are Diff which simply calculates the difference between the two predictions, Logit which calculates the difference of logit(prediction) of the two predictions and a few more. Those scoring methods can also be user-defined in which case they can be submitted with the model. Not all scoring functions are compatible with all model possible model outputs - for example the logit transformation can only be performed on values [0,1]. Model and dataloader requirements The model has to produce predictions at least partly based on DNA sequence and the DNA sequence either has to be as a string (e.g. acgtACGT ) or in a 1-hot encoded way in which A = [1,0,0,0] , C = [0,1,0,0] , G= [0,0,1,0] , T= [0,0,0,1] . Please note that any letter/base that is not in acgtACGT will be regarded and treated as N (in one-hot: [0,0,0,0] )! Requirements for the dataloader are that apart from producing the model input it also has to output information which region of the genome this generated sequence corresponds. On a side note: This region is only used to calculate an overlap with the query VCF, hence as long the dataloader output refers to the same sequence assembly as the VCF file variant scoring will return the desired results. Setting up the model.yaml In order to indicate that a model is compatible with Kipoi postprocessing the definition of postprocessing in the model.yaml file is necessary. The postprocessing section can then mention multiple different ways to interpret a model. Here we will discuss variant effect prediction, a sample section of the model.yaml can look like this: postprocessing: variant_effects: seq_input: - seq use_rc: seq_only This defines that the current model is capable to be used for variant effect prediction ( variant_effects ) and it defines that seq is the name of the model input that contains DNA sequence, which can be mutated and used for effect prediction. seq_input is a mandatory field and variant effect prediction can only be executed if there is at least one model input defined in seq_input . For some models it is necessary that also reverse-complements of DNA sequences are tested / predicted. To indicate that this is the case for the current model add the optional flag use_rc: seq_only . Using seq_only will reverse-complement only the model inputs that are defined in seq_input . Any other model input will remain untouched and exactly the same input will be fed to the model input as for the \"forward\" version of the model input. As mentioned above the DNA sequence input may either be a string or 1-hot encoded. To indicate which format is used the special_type flag is used. The model input may then look like this: schema: inputs: seq: shape: (101, 4) special_type: DNASeq doc: One-hot encoded RNA sequence Here a one-hot encoded sequence ( DNASeq ) is expected to be the model input. Note that the model input label (here: seq ) was used before in the postprocessing section and the same label is expected to be exist in the dataloader output. The special_type flag for using string input sequences is: DNAStringSeq . So the following snippet of a model.yaml file schema: inputs: seq: shape: () special_type: DNAStringSeq doc: RNA sequence as a string indicates that a single sample of seq is np.array(string) where string is a python string. If special_type is not defined for a model input, but it is used in seq_input in the postprocessing section, then by default Kipoi expects one-hot encoded DNA sequences. Setting up the dataloader.yaml Similar to the model.yaml also dataloader.yaml has to have a postprocessing section defined to indicate that it is compatible with variant effect prediction. As a bare minimum the following has to be defined: postprocessing: variant_effects: And equally important every DNA sequence input of a model (here seq ) has to have an associated metadata tag, which could like follows: output_schema: inputs: seq: shape: (101, 4) special_type: DNASeq doc: One-hot encoded RNA sequence associated_metadata: ranges some_other_input: shape: (1, 10) doc: Some description metadata: ranges: type: GenomicRanges doc: Ranges describing inputs.seq Here the associated_metadata flag in the input field seq is set to ranges , which means that for every sample in the model_input['inputs']['seq'] one entry in model_input['metadata']['ranges'] is expected with its type either being GenomicRanges or a dictionary of numpy arrays with the keys chr , start , end , id . The information in the metadata object gives variant effect prediction the possibilty to find the relative position of a variant within a given input sequence. Hence the associated_metadata is mandatory for every entry in seq_input in the model.yaml file. Please note that the coordinates in the metadata are expected to be 0-based, hence comply with .bed file format! The following sketch gives an overview how the different tags play together and how they are used with variant effect prediction. Use-cases This section describes a set of functions which cover most of the common queries for variant effect. All of the functions described below require that the model.yaml and dataloader.yaml files are set up in the way defined above. In literature in-silico mutagenesis-based variant effect predcition is performed in a variant centric way: Starting from a VCF for every variant a sequence centered on said variant is generated. That sequence is then mutated by modifying the central base and setting it to what is defined as reference or alternative allele, generating two sets of sequences. For both the set with the reference allele in the center and the alternative allele in the center the model prediction is run and model outputs are compared. Not all models can predict on aribrary DNA sequences from any region of the genome. Splicing models may for example only be trained on regions surrounding a splice site, hence the variant-centered approach from before will not work. Therefore two more options to run variant effect predicion are offered: restricted variant centered effect prediction and overlap-based effect prediction. Variant effect prediction will try to use variant-centered approaches whenever the bed_input flag is defined in dataloader.yaml (see below). Otherwise the overlap-based effect prediction is used. This is because the variant centered approach is generally faster and for every variant in the VCF one single prediction can be made (assuming the position of variant is in a valid genomic region). For all the methods described below it is essential that genomic coordinates in the VCF and the coordinates used by the dataloader are for the same genome / assembly /etc. Variant centered effect prediction In order to use variant centered effect prediction the dataloader must accept an input bed file based on which it will produce model input. Furthermore the dataloader is required to return the name values (fourth column) of the input bed file in the id field of model_input['metadata']['ranges'] . Additionally the order of samples has to be identical with the order of regions in the input bed file, but regions may be skipped. In order for the variant effect prediction to know which input argument of the dataloader is accepts a bed file three additional lines in dataloader.yaml are necessary, e.g: postprocessing: variant_effects: bed_input: - intervals_file This section indicates that the dataloader function has an argument intervals_file which accepts a bed file path as input which may be used. Restricted-variant centered effect prediction Requirements for the dataloader and dataloader.yaml here are identical to the variant centered effect prediction. The only difference is that this function is designed for models that can't predict on arbitrary regions of the genome, but only in certain regions of the genome. If those regions can be defined in a bed file (further on called 'restriction-bed' file) then this approach can be used. Variant effect prediction will then intersect the VCF with the restriction-bed and generate another bed file that is then passed on to the dataloader. Regions in the restriction-bed file may be larger than the input sequence lenght, in that case the generated seuqence will be centered on the variant position as much as possible - restricted by what is defined in the restrictions-bed file. Overlap-based effect prediction If the dataloader does not support bed input files then variant effect predictions can be run by the overlap of a VCF with the regions defined in the metdata output of the dataloader. If multiple variants overlap with a region then the effect will be predicted inpendently for those variants. If multiple (e.g.: two) model input samples overlap with one variant then the output will contain as many predictions as there were independent overlaps of metadata ranges and variants (e.g.: two). Scoring functions After mutating the model input DNA sequences predictions are created using the models and those predictions then have to compared by scoring methods. Not all scoring methods are compatible with all models depending on the output data range of the model (see below). The compatibility of a scoring function with a given model can be indicated by setting scoring_functions in model.yaml: postprocessing: variant_effects seq_input: - seq scoring_functions: - name: diff type: diff - type: logit default: true The scoring function is identified by the type field in scoring_functions which is the only mandatory field. Allowed values for the type field are: diff , logit , deepsea_effect and custom . Setting default:true for a scoring function indicates that that respective scoring function is executed by variant effect prediction if none is selected by the used on execution time. If multiple scoring functions have set default:true then all of those will be run by default. If default:true is not set for any scoring function defined in scoring_functions then all entries in scoring_functions will be run by default. Scoring functions can be assigned a different name with the name flag by which they are then selected using the command line interface. In general it is not advisable to rename the scoring functions that come with Kipoi. Diff The simplest scoring method is to calculate the difference between predictions for the reference and the alternative allele: prediction(alt) - prediction(ref) . This scoring method is available for all models no matter if it is defined in scoring_functions or not. Logit Calculates the difference of logit-transformed values of the predictions: logit(prediction(alt)) - logit(prediction(ref)) . This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1]. LogitAlt Returns the logits transformed predictions for the sequences carrying the alternative allele: logit(prediction(alt)) . This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1]. LogitRef Returns the logits transformed predictions for the sequences carrying the reference allele: logit(prediction(ref)) . This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1]. Deepsea_effect Calculates the variant scores as defined in the publication of the DeepSEA model (Troyanskaya et al., 2015) by using the absolute value of the logit difference and diff values multiplied together: abs(Logit * Diff) with Logit and Diff defined as above. Custom Custom scoring methods can be defined and shipped with the models. In that case the model.yaml will look similar to this: postprocessing: variant_effects: seq_input: - seq scoring_functions: - name: my_scores type: custom defined_as: postproc.py::myfun args: first_arg: doc: Description of the first argument default: 1 Notice that the selection of type: custom requires that defined_as is set. The value postproc.py::myfun indicates that the callable python object myfun is stored in a file called postproc.py . When executing variant effect prediction in the command line the scoring function can be chosen by it's name - which in this case is: my_scores . All scoring functions are subclasses of Rc_merging_pred_analysis this means that also a custom scoring function must inherit from it. Output The output of variant effect prediction is by default stored in a VCF that is derived from the input VCF. The output VCF only contains variants for which a effect prediction could be generated (e.g. if no model input sample overlapped a variant no prediction could be made for it). The predictions themselves are stored in the INFO field of the VCF, with the ID starting with KPVEP and containing the name of the model. Additional to the predictions themselves a also a region ID will be stored in a second INFO field. The region IDs are the values stored in model_input['metadata']['ranges']['id'] given to a sequence sample generated by the dataloader. This way it is possible to trace back which sequence was mutated by which variant in order to produce a certain effect prediction Since multiple seqeunces generated by the dataloader may overlap one variant - especially when using the overlap-based effect prediction - it is possible that the generated VCF output will contain a variant multiple times, but the different predictions will be destinguishable by their region ID. If variant effect prediction is run programmatically in python then the results are returned as a dictionary of pandas DataFrames. More complex models More complex models may have more than only one DNA sequence input, it may even be that models have DNA sequence inputs taken from different regions of the genome within one sample in a batch. See this sketch for an illustration of the scenario: The dataloader has three sequence outputs which are linked to two metadata ranges. for both ranges objects the beginning of the ranges is displayed. In order to overlap the metadata ranges with variants the input batch is processed one sample at a time. The samples in a batch are displayed in green rectangular boxes: For every sample all the ranges are assembled and overlapped with variants in the VCF. Then the effect is predicted for every single variant in the VCF that overlaps at least one of the region defined in that sample. This means that for the first sample in the batch two variants are investigated: rs1 and rs2. rs1 can only affect seq1a and seq1b, hence those two sequences are mutated, seq2 is not. rs2 overlaps with both ranges in the first sample and hence two sequences are mutated with rs2 to predict its effect. This means that the first sample will be evaluated twice using variants rs1 and rs2, and the second sample only once using rs3.","title":"Variant effect prediction"},{"location":"postprocessing/variant_effect_prediction/#variant-effect-prediction","text":"Variant effect prediction offers a simple way predict effects of SNVs using any model that uses DNA sequence as an input. Many different scoring methods can be chosen but the principle relies on in-silico mutagenesis (see below). The default input is a VCF and the default output again is a VCF annotated with predictions of variant effects.","title":"Variant effect prediction"},{"location":"postprocessing/variant_effect_prediction/#how-it-works","text":"This sketch highlights the overall functionality of variant effect prediction. More details are given in the chapters below. Dataloader output and a VCF are overlapped and the input DNA sequence is mutated as defined in the VCF. The reference and the alternative set of model inputs is predicted using the model and the differences are evaluated using a scoring function. The results are then stored in an annotated VCF.","title":"How it works"},{"location":"postprocessing/variant_effect_prediction/#in-silico-mutagenesis","text":"The principle relies on generating model predictions twice, once with DNA sequence that contains the reference and once with the alternative allele of a variant. Those predictions can then be compared in different ways to generate an effect prediction.","title":"In-silico mutagenesis"},{"location":"postprocessing/variant_effect_prediction/#scoring-methods","text":"Scoring methods that come with Kipoi are Diff which simply calculates the difference between the two predictions, Logit which calculates the difference of logit(prediction) of the two predictions and a few more. Those scoring methods can also be user-defined in which case they can be submitted with the model. Not all scoring functions are compatible with all model possible model outputs - for example the logit transformation can only be performed on values [0,1].","title":"Scoring methods"},{"location":"postprocessing/variant_effect_prediction/#model-and-dataloader-requirements","text":"The model has to produce predictions at least partly based on DNA sequence and the DNA sequence either has to be as a string (e.g. acgtACGT ) or in a 1-hot encoded way in which A = [1,0,0,0] , C = [0,1,0,0] , G= [0,0,1,0] , T= [0,0,0,1] . Please note that any letter/base that is not in acgtACGT will be regarded and treated as N (in one-hot: [0,0,0,0] )! Requirements for the dataloader are that apart from producing the model input it also has to output information which region of the genome this generated sequence corresponds. On a side note: This region is only used to calculate an overlap with the query VCF, hence as long the dataloader output refers to the same sequence assembly as the VCF file variant scoring will return the desired results.","title":"Model and dataloader requirements"},{"location":"postprocessing/variant_effect_prediction/#setting-up-the-modelyaml","text":"In order to indicate that a model is compatible with Kipoi postprocessing the definition of postprocessing in the model.yaml file is necessary. The postprocessing section can then mention multiple different ways to interpret a model. Here we will discuss variant effect prediction, a sample section of the model.yaml can look like this: postprocessing: variant_effects: seq_input: - seq use_rc: seq_only This defines that the current model is capable to be used for variant effect prediction ( variant_effects ) and it defines that seq is the name of the model input that contains DNA sequence, which can be mutated and used for effect prediction. seq_input is a mandatory field and variant effect prediction can only be executed if there is at least one model input defined in seq_input . For some models it is necessary that also reverse-complements of DNA sequences are tested / predicted. To indicate that this is the case for the current model add the optional flag use_rc: seq_only . Using seq_only will reverse-complement only the model inputs that are defined in seq_input . Any other model input will remain untouched and exactly the same input will be fed to the model input as for the \"forward\" version of the model input. As mentioned above the DNA sequence input may either be a string or 1-hot encoded. To indicate which format is used the special_type flag is used. The model input may then look like this: schema: inputs: seq: shape: (101, 4) special_type: DNASeq doc: One-hot encoded RNA sequence Here a one-hot encoded sequence ( DNASeq ) is expected to be the model input. Note that the model input label (here: seq ) was used before in the postprocessing section and the same label is expected to be exist in the dataloader output. The special_type flag for using string input sequences is: DNAStringSeq . So the following snippet of a model.yaml file schema: inputs: seq: shape: () special_type: DNAStringSeq doc: RNA sequence as a string indicates that a single sample of seq is np.array(string) where string is a python string. If special_type is not defined for a model input, but it is used in seq_input in the postprocessing section, then by default Kipoi expects one-hot encoded DNA sequences.","title":"Setting up the model.yaml"},{"location":"postprocessing/variant_effect_prediction/#setting-up-the-dataloaderyaml","text":"Similar to the model.yaml also dataloader.yaml has to have a postprocessing section defined to indicate that it is compatible with variant effect prediction. As a bare minimum the following has to be defined: postprocessing: variant_effects: And equally important every DNA sequence input of a model (here seq ) has to have an associated metadata tag, which could like follows: output_schema: inputs: seq: shape: (101, 4) special_type: DNASeq doc: One-hot encoded RNA sequence associated_metadata: ranges some_other_input: shape: (1, 10) doc: Some description metadata: ranges: type: GenomicRanges doc: Ranges describing inputs.seq Here the associated_metadata flag in the input field seq is set to ranges , which means that for every sample in the model_input['inputs']['seq'] one entry in model_input['metadata']['ranges'] is expected with its type either being GenomicRanges or a dictionary of numpy arrays with the keys chr , start , end , id . The information in the metadata object gives variant effect prediction the possibilty to find the relative position of a variant within a given input sequence. Hence the associated_metadata is mandatory for every entry in seq_input in the model.yaml file. Please note that the coordinates in the metadata are expected to be 0-based, hence comply with .bed file format! The following sketch gives an overview how the different tags play together and how they are used with variant effect prediction.","title":"Setting up the dataloader.yaml"},{"location":"postprocessing/variant_effect_prediction/#use-cases","text":"This section describes a set of functions which cover most of the common queries for variant effect. All of the functions described below require that the model.yaml and dataloader.yaml files are set up in the way defined above. In literature in-silico mutagenesis-based variant effect predcition is performed in a variant centric way: Starting from a VCF for every variant a sequence centered on said variant is generated. That sequence is then mutated by modifying the central base and setting it to what is defined as reference or alternative allele, generating two sets of sequences. For both the set with the reference allele in the center and the alternative allele in the center the model prediction is run and model outputs are compared. Not all models can predict on aribrary DNA sequences from any region of the genome. Splicing models may for example only be trained on regions surrounding a splice site, hence the variant-centered approach from before will not work. Therefore two more options to run variant effect predicion are offered: restricted variant centered effect prediction and overlap-based effect prediction. Variant effect prediction will try to use variant-centered approaches whenever the bed_input flag is defined in dataloader.yaml (see below). Otherwise the overlap-based effect prediction is used. This is because the variant centered approach is generally faster and for every variant in the VCF one single prediction can be made (assuming the position of variant is in a valid genomic region). For all the methods described below it is essential that genomic coordinates in the VCF and the coordinates used by the dataloader are for the same genome / assembly /etc.","title":"Use-cases"},{"location":"postprocessing/variant_effect_prediction/#variant-centered-effect-prediction","text":"In order to use variant centered effect prediction the dataloader must accept an input bed file based on which it will produce model input. Furthermore the dataloader is required to return the name values (fourth column) of the input bed file in the id field of model_input['metadata']['ranges'] . Additionally the order of samples has to be identical with the order of regions in the input bed file, but regions may be skipped. In order for the variant effect prediction to know which input argument of the dataloader is accepts a bed file three additional lines in dataloader.yaml are necessary, e.g: postprocessing: variant_effects: bed_input: - intervals_file This section indicates that the dataloader function has an argument intervals_file which accepts a bed file path as input which may be used.","title":"Variant centered effect prediction"},{"location":"postprocessing/variant_effect_prediction/#restricted-variant-centered-effect-prediction","text":"Requirements for the dataloader and dataloader.yaml here are identical to the variant centered effect prediction. The only difference is that this function is designed for models that can't predict on arbitrary regions of the genome, but only in certain regions of the genome. If those regions can be defined in a bed file (further on called 'restriction-bed' file) then this approach can be used. Variant effect prediction will then intersect the VCF with the restriction-bed and generate another bed file that is then passed on to the dataloader. Regions in the restriction-bed file may be larger than the input sequence lenght, in that case the generated seuqence will be centered on the variant position as much as possible - restricted by what is defined in the restrictions-bed file.","title":"Restricted-variant centered effect prediction"},{"location":"postprocessing/variant_effect_prediction/#overlap-based-effect-prediction","text":"If the dataloader does not support bed input files then variant effect predictions can be run by the overlap of a VCF with the regions defined in the metdata output of the dataloader. If multiple variants overlap with a region then the effect will be predicted inpendently for those variants. If multiple (e.g.: two) model input samples overlap with one variant then the output will contain as many predictions as there were independent overlaps of metadata ranges and variants (e.g.: two).","title":"Overlap-based effect prediction"},{"location":"postprocessing/variant_effect_prediction/#scoring-functions","text":"After mutating the model input DNA sequences predictions are created using the models and those predictions then have to compared by scoring methods. Not all scoring methods are compatible with all models depending on the output data range of the model (see below). The compatibility of a scoring function with a given model can be indicated by setting scoring_functions in model.yaml: postprocessing: variant_effects seq_input: - seq scoring_functions: - name: diff type: diff - type: logit default: true The scoring function is identified by the type field in scoring_functions which is the only mandatory field. Allowed values for the type field are: diff , logit , deepsea_effect and custom . Setting default:true for a scoring function indicates that that respective scoring function is executed by variant effect prediction if none is selected by the used on execution time. If multiple scoring functions have set default:true then all of those will be run by default. If default:true is not set for any scoring function defined in scoring_functions then all entries in scoring_functions will be run by default. Scoring functions can be assigned a different name with the name flag by which they are then selected using the command line interface. In general it is not advisable to rename the scoring functions that come with Kipoi.","title":"Scoring functions"},{"location":"postprocessing/variant_effect_prediction/#diff","text":"The simplest scoring method is to calculate the difference between predictions for the reference and the alternative allele: prediction(alt) - prediction(ref) . This scoring method is available for all models no matter if it is defined in scoring_functions or not.","title":"Diff"},{"location":"postprocessing/variant_effect_prediction/#logit","text":"Calculates the difference of logit-transformed values of the predictions: logit(prediction(alt)) - logit(prediction(ref)) . This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1].","title":"Logit"},{"location":"postprocessing/variant_effect_prediction/#logitalt","text":"Returns the logits transformed predictions for the sequences carrying the alternative allele: logit(prediction(alt)) . This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1].","title":"LogitAlt"},{"location":"postprocessing/variant_effect_prediction/#logitref","text":"Returns the logits transformed predictions for the sequences carrying the reference allele: logit(prediction(ref)) . This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1].","title":"LogitRef"},{"location":"postprocessing/variant_effect_prediction/#deepsea_effect","text":"Calculates the variant scores as defined in the publication of the DeepSEA model (Troyanskaya et al., 2015) by using the absolute value of the logit difference and diff values multiplied together: abs(Logit * Diff) with Logit and Diff defined as above.","title":"Deepsea_effect"},{"location":"postprocessing/variant_effect_prediction/#custom","text":"Custom scoring methods can be defined and shipped with the models. In that case the model.yaml will look similar to this: postprocessing: variant_effects: seq_input: - seq scoring_functions: - name: my_scores type: custom defined_as: postproc.py::myfun args: first_arg: doc: Description of the first argument default: 1 Notice that the selection of type: custom requires that defined_as is set. The value postproc.py::myfun indicates that the callable python object myfun is stored in a file called postproc.py . When executing variant effect prediction in the command line the scoring function can be chosen by it's name - which in this case is: my_scores . All scoring functions are subclasses of Rc_merging_pred_analysis this means that also a custom scoring function must inherit from it.","title":"Custom"},{"location":"postprocessing/variant_effect_prediction/#output","text":"The output of variant effect prediction is by default stored in a VCF that is derived from the input VCF. The output VCF only contains variants for which a effect prediction could be generated (e.g. if no model input sample overlapped a variant no prediction could be made for it). The predictions themselves are stored in the INFO field of the VCF, with the ID starting with KPVEP and containing the name of the model. Additional to the predictions themselves a also a region ID will be stored in a second INFO field. The region IDs are the values stored in model_input['metadata']['ranges']['id'] given to a sequence sample generated by the dataloader. This way it is possible to trace back which sequence was mutated by which variant in order to produce a certain effect prediction Since multiple seqeunces generated by the dataloader may overlap one variant - especially when using the overlap-based effect prediction - it is possible that the generated VCF output will contain a variant multiple times, but the different predictions will be destinguishable by their region ID. If variant effect prediction is run programmatically in python then the results are returned as a dictionary of pandas DataFrames.","title":"Output"},{"location":"postprocessing/variant_effect_prediction/#more-complex-models","text":"More complex models may have more than only one DNA sequence input, it may even be that models have DNA sequence inputs taken from different regions of the genome within one sample in a batch. See this sketch for an illustration of the scenario: The dataloader has three sequence outputs which are linked to two metadata ranges. for both ranges objects the beginning of the ranges is displayed. In order to overlap the metadata ranges with variants the input batch is processed one sample at a time. The samples in a batch are displayed in green rectangular boxes: For every sample all the ranges are assembled and overlapped with variants in the VCF. Then the effect is predicted for every single variant in the VCF that overlaps at least one of the region defined in that sample. This means that for the first sample in the batch two variants are investigated: rs1 and rs2. rs1 can only affect seq1a and seq1b, hence those two sequences are mutated, seq2 is not. rs2 overlaps with both ranges in the first sample and hence two sequences are mutated with rs2 to predict its effect. This means that the first sample will be evaluated twice using variants rs1 and rs2, and the second sample only once using rs3.","title":"More complex models"},{"location":"tutorials/R-api/","text":"Generated from notebooks/R-api.ipynb Using Kipoi from R Thanks to the reticulate R package from RStudio, it is possible to easily call python functions from R. Hence one can use kipoi python API from R. This tutorial will show how to do that. Make sure you have git-lfs and Kipoi correctly installed: Install git-lfs conda install -c conda-forge git-lfs && git lfs install (alternatively see https://git-lfs.github.com/ ) Install kipoi pip install kipoi Please read docs/using/getting started before going through this notebook. Install and load reticulate Make sure you have the reticulate R package installed # install.packages(\"reticulate\") library(reticulate) Reticulate quick intro In general, using Kipoi from R is almost the same as using it from Python: instead of using object.method() or object.attribute as in python, use $ : object$method() , object$attribute . # short reticulate example os <- import(\"os\") os$chdir(\"/tmp\") os$getcwd() '/tmp' Type mapping R <-> python Reticulate translates objects between R and python in the following way: R Python Examples Single-element vector Scalar 1 , 1L , TRUE , \"foo\" Multi-element vector List c(1.0, 2.0, 3.0) , c(1L, 2L, 3L) List of multiple types Tuple list(1L, TRUE, \"foo\") Named list Dict list(a = 1L, b = 2.0) , dict(x = x_data) Matrix/Array NumPy ndarray matrix(c(1,2,3,4), nrow = 2, ncol = 2) Function Python function function(x) x + 1 NULL, TRUE, FALSE None, True, False NULL , TRUE , FALSE For more info on reticulate, please visit https://github.com/rstudio/reticulate/. Setup the python environment With reticulate::py_config() you can check if the python configuration used by reticulate is correct. You can can also choose to use a different conda environment with use_condaenv(...) . This comes handy when using different models depending on different conda environments. reticulate::py_config() python: /home/avsec/bin/anaconda3/bin/python libpython: /home/avsec/bin/anaconda3/lib/libpython3.6m.so pythonhome: /home/avsec/bin/anaconda3:/home/avsec/bin/anaconda3 version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) [GCC 7.2.0] numpy: /home/avsec/bin/anaconda3/lib/python3.6/site-packages/numpy numpy_version: 1.14.0 os: /home/avsec/bin/anaconda3/lib/python3.6/os.py python versions found: /home/avsec/bin/anaconda3/bin/python /usr/bin/python /usr/bin/python3 List all conda environments: reticulate::conda_list() Create a new conda environment for the model: $ kipoi env create HAL Use that environment in R: reticulate::use_condaenv(\"kipoi-HAL') Load kipoi kipoi <- import(\"kipoi\") List models kipoi$list_models()$head() source model version \\ 0 kipoi DeepSEAKeras 0.1 1 kipoi extended_coda 0.1 2 kipoi DeepCpG_DNA/Hou2016_mESC_dna 1.0.4 3 kipoi DeepCpG_DNA/Smallwood2014_2i_dna 1.0.4 4 kipoi DeepCpG_DNA/Hou2016_HepG2_dna 1.0.4 authors \\ 0 [Author(name='Jian Zhou', github=None, email=N... 1 [Author(name='Pang Wei Koh', github='kohpangwe... 2 [Author(name='Christof Angermueller', github='... 3 [Author(name='Christof Angermueller', github='... 4 [Author(name='Christof Angermueller', github='... contributors \\ 0 [Author(name='Lara Urban', github='LaraUrban',... 1 [Author(name='Johnny Israeli', github='jisrael... 2 [Author(name='Roman Kreuzhuber', github='krrom... 3 [Author(name='Roman Kreuzhuber', github='krrom... 4 [Author(name='Roman Kreuzhuber', github='krrom... doc type \\ 0 This CNN is based on the DeepSEA model from Zh... keras 1 Single bp resolution ChIP-seq denoising - http... keras 2 This is the extraction of the DNA-part of the ... keras 3 This is the extraction of the DNA-part of the ... keras 4 This is the extraction of the DNA-part of the ... keras inputs targets \\ 0 seq TFBS_DHS_probs 1 [H3K27AC_subsampled] [H3K27ac] 2 [dna] [cpg/mESC1, cpg/mESC2, cpg/mESC3, cpg/mESC4, c... 3 [dna] [cpg/BS24_1_2I, cpg/BS24_2_2I, cpg/BS24_4_2I, ... 4 [dna] [cpg/HepG21, cpg/HepG22, cpg/HepG23, cpg/HepG2... postproc_score_variants license \\ 0 True MIT 1 False MIT 2 True MIT 3 True MIT 4 True MIT cite_as \\ 0 https://doi.org/10.1038/nmeth.3547 1 https://doi.org/10.1093/bioinformatics/btx243 2 https://doi.org/10.1186/s13059-017-1189-z, htt... 3 https://doi.org/10.1186/s13059-017-1189-z, htt... 4 https://doi.org/10.1186/s13059-017-1189-z, htt... trained_on \\ 0 ENCODE and Roadmap Epigenomics chromatin profi... 1 Described in https://academic.oup.com/bioinfor... 2 scBS-seq and scRRBS-seq datasets, https://geno... 3 scBS-seq and scRRBS-seq datasets, https://geno... 4 scBS-seq and scRRBS-seq datasets, https://geno... training_procedure \\ 0 https://www.nature.com/articles/nmeth.3547#met... 1 Described in https://academic.oup.com/bioinfor... 2 Described in https://genomebiology.biomedcentr... 3 Described in https://genomebiology.biomedcentr... 4 Described in https://genomebiology.biomedcentr... tags 0 [Histone modification, DNA binding, DNA access... 1 [Histone modification] 2 [DNA methylation] 3 [DNA methylation] 4 [DNA methylation] reticulate currently doesn't support direct convertion from pandas.DataFrame to R's data.frame . Let's make a convenience function to create an R dataframe via matrix conversion. #' List models as an R data.frame kipoi_list_models <- function() { df_models <- kipoi$list_models() df <- data.frame(df_models$as_matrix()) colnames(df) = df_models$columns$tolist() return(df) } df <- kipoi_list_models() head(df, 2) source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags kipoi DeepSEAKeras 0.1 <environment: 0x556afc757e38> <environment: 0x556afbb0d538> This CNN is based on the DeepSEA model from Zhou and Troyanskaya (2015). It categorically predicts 918 cell type-specific epigenetic features from DNA sequence. The model is trained on publicly available ENCODE and Roadmap Epigenomics data and on DNA sequences of size 1000bp. The input of the tensor has to be (N, 1000, 4) for N samples, 1000bp window size and 4 nucleotides. Per sample, 918 probabilities of showing a specific epigentic feature will be predicted. keras seq TFBS_DHS_probs TRUE MIT https://doi.org/10.1038/nmeth.3547 ENCODE and Roadmap Epigenomics chromatin profiles https://www.nature.com/articles/nmeth.3547#methods https://www.nature.com/articles/nmeth.3547#methods <environment: 0x556afcddfd50> kipoi extended_coda 0.1 <environment: 0x556afc764260> <environment: 0x556afbaff708> Single bp resolution ChIP-seq denoising - https://github.com/kundajelab/coda keras H3K27AC_subsampled H3K27ac FALSE MIT https://doi.org/10.1093/bioinformatics/btx243 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 <environment: 0x556afcde7f60> Get the kipoi model and make a prediction for the example files To run the following example, make sure you have all the dependencies installed. Run: kipoi$install_model_requirements(\"MaxEntScan/3prime\") from R or kipoi env install MaxEntScan/3prime from the command-line. This will install all the required dependencies for both, the model and the dataloader. kipoi$install_model_requirements(\"MaxEntScan/3prime\") model <- kipoi$get_model(\"MaxEntScan/3prime\") predictions <- model$pipeline$predict_example() head(predictions) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451 -9.52033554891735 9.54342300799607 Use the model and dataloader independently # Get the dataloader setwd('~/.kipoi/models/MaxEntScan/3prime') dl <- model$default_dataloader(gtf_file='example_files/hg19.chr22.gtf', fasta_file='example_files/hg19.chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) it DataLoaderIter # Retrieve a batch of data batch <- iter_next(it) str(batch) List of 2 $ inputs : chr [1:4(1d)] \"TCTTCTCTCCCCAATCTCAGCCT\" \"ATTCTCAGTTGTCTTTACAGTTT\" \"CCTTAGTTTTATTTTTTCAGAGT\" \"ATTTTTGTTTTTAGACATAGGAT\" $ metadata:List of 5 ..$ geneID : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" ..$ transcriptID: chr [1:4(1d)] \"ENST00000424770\" \"ENST00000420638\" \"ENST00000420638\" \"ENST00000420638\" ..$ biotype : chr [1:4(1d)] \"lincRNA\" \"pseudogene\" \"pseudogene\" \"pseudogene\" ..$ order : num [1:4(1d)] 0 0 1 2 ..$ ranges :List of 5 .. ..$ chr : chr [1:4(1d)] \"22\" \"22\" \"22\" \"22\" .. ..$ start : num [1:4(1d)] 16062790 16118910 16101471 16100645 .. ..$ end : num [1:4(1d)] 16062813 16118933 16101494 16100668 .. ..$ id : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" .. ..$ strand: chr [1:4(1d)] \"+\" \"-\" \"-\" \"-\" # make the prediction with a model model$predict_on_batch(batch$inputs) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451 Troubleshooting Since Kipoi is not natively implemented in R, the error messages are cryptic and hence debugging can be a bit of a pain. Run the same code in python or CLI When you encounter an error, try to run the analogous code snippet from the command line or python. A good starting point is to first run $ kipoi test MaxEntScan/3prime --source=kipoi from the command-line first. Dependency issues It's very likely that the error will be due to missing dependencies. Also note that some models will work only with python 3 or python 2. To install all the required dependencies for the model, run: $ kipoi env install MaxEntScan/3prime This will install the dependencies into your current conda environment. If you wish to create a new environment with all the dependencies installed, run $ kipoi env create MaxEntScan/3prime To use that environment in R, run: use_condaenv(\"kipoi-MaxEntScan__3prime\") Make sure you run that code snippet right after importing the reticulate library (i.e. make sure you run it before kipoi <- import('kipoi') ) Float/Double type issues When using a pytorch model: DeepSEA/predict kipoi$install_model_requirements(\"DeepSEA/predict\") # Get the dataloader setwd('~/.kipoi/models/DeepSEA/predict') model <- kipoi$get_model(\"DeepSEA/predict\") dl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) # predict for a batch batch <- iter_next(it) # model$predict_on_batch(batch$inputs) We get an error: Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Input type (CUDADoubleTensor) and weight type (CUDAFloatTensor) should be the same This means that the feeded array is Double instead of Float. R arrays are by default converted to float64 numpy dtype: np <- import(\"numpy\", convert=FALSE) np$array(0.1)$dtype float64 np$array(batch$inputs)$dtype float64 To fix this, we need to explicitly convert them to float32 before passing the batch to the model: model$predict_on_batch(np$array(batch$inputs, dtype='float32')) 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651","title":"R API"},{"location":"tutorials/R-api/#using-kipoi-from-r","text":"Thanks to the reticulate R package from RStudio, it is possible to easily call python functions from R. Hence one can use kipoi python API from R. This tutorial will show how to do that. Make sure you have git-lfs and Kipoi correctly installed: Install git-lfs conda install -c conda-forge git-lfs && git lfs install (alternatively see https://git-lfs.github.com/ ) Install kipoi pip install kipoi Please read docs/using/getting started before going through this notebook.","title":"Using Kipoi from R"},{"location":"tutorials/R-api/#install-and-load-reticulate","text":"Make sure you have the reticulate R package installed # install.packages(\"reticulate\") library(reticulate)","title":"Install and load reticulate"},{"location":"tutorials/R-api/#reticulate-quick-intro","text":"In general, using Kipoi from R is almost the same as using it from Python: instead of using object.method() or object.attribute as in python, use $ : object$method() , object$attribute . # short reticulate example os <- import(\"os\") os$chdir(\"/tmp\") os$getcwd() '/tmp'","title":"Reticulate quick intro"},{"location":"tutorials/R-api/#type-mapping-r-python","text":"Reticulate translates objects between R and python in the following way: R Python Examples Single-element vector Scalar 1 , 1L , TRUE , \"foo\" Multi-element vector List c(1.0, 2.0, 3.0) , c(1L, 2L, 3L) List of multiple types Tuple list(1L, TRUE, \"foo\") Named list Dict list(a = 1L, b = 2.0) , dict(x = x_data) Matrix/Array NumPy ndarray matrix(c(1,2,3,4), nrow = 2, ncol = 2) Function Python function function(x) x + 1 NULL, TRUE, FALSE None, True, False NULL , TRUE , FALSE For more info on reticulate, please visit https://github.com/rstudio/reticulate/.","title":"Type mapping R &lt;-&gt; python"},{"location":"tutorials/R-api/#setup-the-python-environment","text":"With reticulate::py_config() you can check if the python configuration used by reticulate is correct. You can can also choose to use a different conda environment with use_condaenv(...) . This comes handy when using different models depending on different conda environments. reticulate::py_config() python: /home/avsec/bin/anaconda3/bin/python libpython: /home/avsec/bin/anaconda3/lib/libpython3.6m.so pythonhome: /home/avsec/bin/anaconda3:/home/avsec/bin/anaconda3 version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) [GCC 7.2.0] numpy: /home/avsec/bin/anaconda3/lib/python3.6/site-packages/numpy numpy_version: 1.14.0 os: /home/avsec/bin/anaconda3/lib/python3.6/os.py python versions found: /home/avsec/bin/anaconda3/bin/python /usr/bin/python /usr/bin/python3 List all conda environments: reticulate::conda_list() Create a new conda environment for the model: $ kipoi env create HAL Use that environment in R: reticulate::use_condaenv(\"kipoi-HAL')","title":"Setup the python environment"},{"location":"tutorials/R-api/#load-kipoi","text":"kipoi <- import(\"kipoi\")","title":"Load kipoi"},{"location":"tutorials/R-api/#list-models","text":"kipoi$list_models()$head() source model version \\ 0 kipoi DeepSEAKeras 0.1 1 kipoi extended_coda 0.1 2 kipoi DeepCpG_DNA/Hou2016_mESC_dna 1.0.4 3 kipoi DeepCpG_DNA/Smallwood2014_2i_dna 1.0.4 4 kipoi DeepCpG_DNA/Hou2016_HepG2_dna 1.0.4 authors \\ 0 [Author(name='Jian Zhou', github=None, email=N... 1 [Author(name='Pang Wei Koh', github='kohpangwe... 2 [Author(name='Christof Angermueller', github='... 3 [Author(name='Christof Angermueller', github='... 4 [Author(name='Christof Angermueller', github='... contributors \\ 0 [Author(name='Lara Urban', github='LaraUrban',... 1 [Author(name='Johnny Israeli', github='jisrael... 2 [Author(name='Roman Kreuzhuber', github='krrom... 3 [Author(name='Roman Kreuzhuber', github='krrom... 4 [Author(name='Roman Kreuzhuber', github='krrom... doc type \\ 0 This CNN is based on the DeepSEA model from Zh... keras 1 Single bp resolution ChIP-seq denoising - http... keras 2 This is the extraction of the DNA-part of the ... keras 3 This is the extraction of the DNA-part of the ... keras 4 This is the extraction of the DNA-part of the ... keras inputs targets \\ 0 seq TFBS_DHS_probs 1 [H3K27AC_subsampled] [H3K27ac] 2 [dna] [cpg/mESC1, cpg/mESC2, cpg/mESC3, cpg/mESC4, c... 3 [dna] [cpg/BS24_1_2I, cpg/BS24_2_2I, cpg/BS24_4_2I, ... 4 [dna] [cpg/HepG21, cpg/HepG22, cpg/HepG23, cpg/HepG2... postproc_score_variants license \\ 0 True MIT 1 False MIT 2 True MIT 3 True MIT 4 True MIT cite_as \\ 0 https://doi.org/10.1038/nmeth.3547 1 https://doi.org/10.1093/bioinformatics/btx243 2 https://doi.org/10.1186/s13059-017-1189-z, htt... 3 https://doi.org/10.1186/s13059-017-1189-z, htt... 4 https://doi.org/10.1186/s13059-017-1189-z, htt... trained_on \\ 0 ENCODE and Roadmap Epigenomics chromatin profi... 1 Described in https://academic.oup.com/bioinfor... 2 scBS-seq and scRRBS-seq datasets, https://geno... 3 scBS-seq and scRRBS-seq datasets, https://geno... 4 scBS-seq and scRRBS-seq datasets, https://geno... training_procedure \\ 0 https://www.nature.com/articles/nmeth.3547#met... 1 Described in https://academic.oup.com/bioinfor... 2 Described in https://genomebiology.biomedcentr... 3 Described in https://genomebiology.biomedcentr... 4 Described in https://genomebiology.biomedcentr... tags 0 [Histone modification, DNA binding, DNA access... 1 [Histone modification] 2 [DNA methylation] 3 [DNA methylation] 4 [DNA methylation] reticulate currently doesn't support direct convertion from pandas.DataFrame to R's data.frame . Let's make a convenience function to create an R dataframe via matrix conversion. #' List models as an R data.frame kipoi_list_models <- function() { df_models <- kipoi$list_models() df <- data.frame(df_models$as_matrix()) colnames(df) = df_models$columns$tolist() return(df) } df <- kipoi_list_models() head(df, 2) source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags kipoi DeepSEAKeras 0.1 <environment: 0x556afc757e38> <environment: 0x556afbb0d538> This CNN is based on the DeepSEA model from Zhou and Troyanskaya (2015). It categorically predicts 918 cell type-specific epigenetic features from DNA sequence. The model is trained on publicly available ENCODE and Roadmap Epigenomics data and on DNA sequences of size 1000bp. The input of the tensor has to be (N, 1000, 4) for N samples, 1000bp window size and 4 nucleotides. Per sample, 918 probabilities of showing a specific epigentic feature will be predicted. keras seq TFBS_DHS_probs TRUE MIT https://doi.org/10.1038/nmeth.3547 ENCODE and Roadmap Epigenomics chromatin profiles https://www.nature.com/articles/nmeth.3547#methods https://www.nature.com/articles/nmeth.3547#methods <environment: 0x556afcddfd50> kipoi extended_coda 0.1 <environment: 0x556afc764260> <environment: 0x556afbaff708> Single bp resolution ChIP-seq denoising - https://github.com/kundajelab/coda keras H3K27AC_subsampled H3K27ac FALSE MIT https://doi.org/10.1093/bioinformatics/btx243 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 <environment: 0x556afcde7f60>","title":"List models"},{"location":"tutorials/R-api/#get-the-kipoi-model-and-make-a-prediction-for-the-example-files","text":"To run the following example, make sure you have all the dependencies installed. Run: kipoi$install_model_requirements(\"MaxEntScan/3prime\") from R or kipoi env install MaxEntScan/3prime from the command-line. This will install all the required dependencies for both, the model and the dataloader. kipoi$install_model_requirements(\"MaxEntScan/3prime\") model <- kipoi$get_model(\"MaxEntScan/3prime\") predictions <- model$pipeline$predict_example() head(predictions) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451 -9.52033554891735 9.54342300799607","title":"Get the kipoi model and make a prediction for the example files"},{"location":"tutorials/R-api/#use-the-model-and-dataloader-independently","text":"# Get the dataloader setwd('~/.kipoi/models/MaxEntScan/3prime') dl <- model$default_dataloader(gtf_file='example_files/hg19.chr22.gtf', fasta_file='example_files/hg19.chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) it DataLoaderIter # Retrieve a batch of data batch <- iter_next(it) str(batch) List of 2 $ inputs : chr [1:4(1d)] \"TCTTCTCTCCCCAATCTCAGCCT\" \"ATTCTCAGTTGTCTTTACAGTTT\" \"CCTTAGTTTTATTTTTTCAGAGT\" \"ATTTTTGTTTTTAGACATAGGAT\" $ metadata:List of 5 ..$ geneID : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" ..$ transcriptID: chr [1:4(1d)] \"ENST00000424770\" \"ENST00000420638\" \"ENST00000420638\" \"ENST00000420638\" ..$ biotype : chr [1:4(1d)] \"lincRNA\" \"pseudogene\" \"pseudogene\" \"pseudogene\" ..$ order : num [1:4(1d)] 0 0 1 2 ..$ ranges :List of 5 .. ..$ chr : chr [1:4(1d)] \"22\" \"22\" \"22\" \"22\" .. ..$ start : num [1:4(1d)] 16062790 16118910 16101471 16100645 .. ..$ end : num [1:4(1d)] 16062813 16118933 16101494 16100668 .. ..$ id : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\" .. ..$ strand: chr [1:4(1d)] \"+\" \"-\" \"-\" \"-\" # make the prediction with a model model$predict_on_batch(batch$inputs) 6.72899227874919 6.15729433240656 7.14095214875511 2.13760519765451","title":"Use the model and dataloader independently"},{"location":"tutorials/R-api/#troubleshooting","text":"Since Kipoi is not natively implemented in R, the error messages are cryptic and hence debugging can be a bit of a pain.","title":"Troubleshooting"},{"location":"tutorials/R-api/#run-the-same-code-in-python-or-cli","text":"When you encounter an error, try to run the analogous code snippet from the command line or python. A good starting point is to first run $ kipoi test MaxEntScan/3prime --source=kipoi from the command-line first.","title":"Run the same code in python or CLI"},{"location":"tutorials/R-api/#dependency-issues","text":"It's very likely that the error will be due to missing dependencies. Also note that some models will work only with python 3 or python 2. To install all the required dependencies for the model, run: $ kipoi env install MaxEntScan/3prime This will install the dependencies into your current conda environment. If you wish to create a new environment with all the dependencies installed, run $ kipoi env create MaxEntScan/3prime To use that environment in R, run: use_condaenv(\"kipoi-MaxEntScan__3prime\") Make sure you run that code snippet right after importing the reticulate library (i.e. make sure you run it before kipoi <- import('kipoi') )","title":"Dependency issues"},{"location":"tutorials/R-api/#floatdouble-type-issues","text":"When using a pytorch model: DeepSEA/predict kipoi$install_model_requirements(\"DeepSEA/predict\") # Get the dataloader setwd('~/.kipoi/models/DeepSEA/predict') model <- kipoi$get_model(\"DeepSEA/predict\") dl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa') # get a batch iterator it <- dl$batch_iter(batch_size=4) # predict for a batch batch <- iter_next(it) # model$predict_on_batch(batch$inputs) We get an error: Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Input type (CUDADoubleTensor) and weight type (CUDAFloatTensor) should be the same This means that the feeded array is Double instead of Float. R arrays are by default converted to float64 numpy dtype: np <- import(\"numpy\", convert=FALSE) np$array(0.1)$dtype float64 np$array(batch$inputs)$dtype float64 To fix this, we need to explicitly convert them to float32 before passing the batch to the model: model$predict_on_batch(np$array(batch$inputs, dtype='float32')) 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651 0.003497796 0.003443634 0.00475722 0.006346597 0.01217456 0.008442441 0.005778539 0.007471715 0.005652952 0.009384833 \u22ef 0.0003717453 0.001310135 0.01009644 0.008201431 0.0004381537 0.007473897 0.009021533 0.003500142 0.003842842 0.0003947651","title":"Float/Double type issues"},{"location":"tutorials/composing_models/","text":"Generated from notebooks/composing_models.ipynb Composing models by Ziga Avsec Composing models means that we take the predictions of some model and use it as input for another model like this: Three different scenarios can occur when we want to compose models from Kipoi: all models are written in the same framework (say Keras) models are written in different frameworks but can all be executed in the same python environment models are written in different frameworks and can't be executed in the same python environment due to dependency incompatibilities All models in the same framework In case all models are written in the same framework, you can stitch things together in the framework. Here is an example of how to do this in Keras. Let's first dump 4 dummy models: import keras.layers as kl from keras.models import Model from keras.models import load_model # create model 1 inp1 = kl.Input((3,), name=\"input1\") out1 = kl.Dense(4)(inp1) m1 = Model(inp1, out1) m1.save(\"/tmp/m1.h5\") # create model 2 inp2 = kl.Input((7,), name=\"input1_model1\") out2 = kl.Dense(3)(inp2) m2 = Model(inp2, out2) m2.save(\"/tmp/m2.h5\") # create model 3 inp3 = kl.Input((6,), name=\"input2\") out3 = kl.Dense(4)(inp3) m3 = Model(inp3, out3) m3.save(\"/tmp/m3.h5\") # create model 4 inp4 = kl.Input((7,), name=\"model2_model3\") out4 = kl.Dense(1)(inp4) m4 = Model(inp4, out4) m4.save(\"/tmp/m4.h5\") Next, we load the models back: ## Load models m1 = load_model(\"/tmp/m1.h5\") m2 = load_model(\"/tmp/m2.h5\") m3 = load_model(\"/tmp/m3.h5\") m4 = load_model(\"/tmp/m4.h5\") /opt/modules/i12g/anaconda/3-5.0.1/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. warnings.warn('No training configuration found in save file: ' And compose them m2_in = kl.concatenate([m1.output, m1.input]) m2_out = m2(m2_in) m3_in = kl.concatenate([m2_out, m3.output]) out = m4(m3_in) m = Model(inputs=[m1.input, m3.input], outputs=out) from IPython.display import SVG from keras.utils.vis_utils import model_to_dot svg_img = model_to_dot(m, ).create(prog='dot', format='svg') SVG(svg_img) Now we could go ahead, merge the dataloaders from model1 and model3 into a single one (providing input1 and input2) and train this global network for a new task. In case we would like to freeze supparts of the network, we should 'freeze' the underlying models by setting m1.trainable = False . Contributing to Kipoi To contribute such model to Kipoi, we would need to submit the merged dataloader (providing input1 and input2 from raw files) and dump the stitched Keras model. Models in different frameworks There are two scenarios when composing models from different frameworks. Either their dependencies (dataloader, etc) are compatible (say a tensorflow and a keras model) or they are incompatible (one model uses keras=0.3 and and another one keras=2.0 ). Compatible dependencies To compose compatible models, we pack the majority of the models into the dataloader and then have the final ensembling model stored as the model. def new_dataloader(dl1_kwargs, dl2_kwargs, target_file, batch_size=32, num_workers=1): m1 = kipoi.get_model(\"model1\") m2 = kipoi.get_model(\"model2\") m3 = kipoi.get_model(\"model3\") dl1 = m1.default_dataloader(**dl1_kwargs) dl2 = m1.default_dataloader(**dl2_kwargs) target_gen = get_target_gen(target_file) batch_it1 = dl1.batch_iter(batch_size=batch_size, num_workers=num_workers) batch_it2 = dl2.batch_iter(batch_size=batch_size, num_workers=num_workers) while True: batch1 = next(batch_it1)['inputs'] batch2 = next(batch_it2)['inputs'] targets, ids = next(target_gen) m1_pred = m1.predict_on_batch(batch1) m2_pred = m2.predict_on_batch(np.concatenate((batch1, m1_pred), axis=1)) m3_pred = m3.predict_on_batch(batch2) yield {\"inputs\": {\"model2\": m2_pred, \"model3\": m3_pred}, \"targets\": targets, \"metadata\": {\"model1_id\": batch1[\"metadata\"][\"id\"], \"model3_id\": batch2[\"metadata\"][\"id\"], \"targets_id\": ids, } } # create model 4 inp2 = kl.Input((3,), name=\"model2\") inp3 = kl.Input((4,), name=\"model3\") x = kl.concatenate([inp2, inp3]) out4 = kl.Dense(1)(x) m4 = Model([inp2, inp3], out4) m4.compile('rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Train model4 def create_train_gen(**kwargs): while True: gen = new_dataloader(**kwargs) while True: batch = next(gen) yield (batch['inputs'], batch['targets']) train_gen = create_train_gen(...) m4.fit_generator(train_gen, ...) # Dump model4 m4.save(\"model_files/model.h5\") Incompatible dependencies Sometimes, making a prediction for all the models in the same python environment might be difficult or impossible due to the incompatible dependencies. In that case, we should run the prediction of each model in a separate environment and save the predictions to the disk. Luckily, there exist many Make-like tools that can support this kind of a workflow. My favorite is Snakemake http://snakemake.readthedocs.io/ . I'll show you how to do this in snakemake. Let's consider the following case: # Python part of the Snakefile import os import subprocess py_path = subprocess.check_output(['which', 'python']).decode().strip() env_paths = os.path.join(os.path.dirname(py_path), \"../envs\") def get_args(wildcards): \"\"\"Function returning a dictionary of dataloader kwargs for the corresponding model \"\"\" if wildcards.model == \"model3\": return {\"arg1\": 1} elif wildcards.model == \"model3\": return {\"\"} else: return {\"arg2\": 1} # Yaml part of the Snakefile rule all: inputs: expand(\"predictions/{model}.h5\", [\"model1\", \"model2\"]) rule create_evironment: \"\"\"Create a new conda environment for each model\"\"\" output: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") shell: \"kipoi env create {wildcards.model} -e kipoi-{wildcards.model}\" rule run_predictions: \"\"\"Create a new conda environment for each model\"\"\" input: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") output: \"predictions/{model}.h5\" params: dl_args: get_args batch_size: 15 threads: 8 shell: \"\"\" source activate kipoi-{wildcards.model} kipoi predict {wildcards.model} \\ -n {threads} \\ --dataloader_args='{params.dl_args}' \\ --batch_size={params.batch_size} \\ -f hdf5 \\ -o {output} \"\"\" This snakefile will generate the following hdf5 files predictions/model1.h5 predictions/model2.h5 To combine them, let's write new dataloader, taking as input the hdf5 files containing predictions import deepdish def new_dataloader(model1_h5, model2_h5, target_file): d1 = deepdish.io.load(model1_h5) d2 = deepdish.io.load(model2_h5) targets = load_target_file(target_file) return { \"inputs\": { \"model1\": d1[\"predictions\"], \"model2\": d2[\"predictions\"], }, \"targets\": targets, \"metadata\": { \"model1_id\": d1[\"metdata\"][\"id\"], \"model2_id\": d2[\"metdata\"][\"id\"], } } # get the training data ... data_train = new_dataloader(\"predictions/model1.h5\" \"predictions/model1.h5\", \"target_file.h5\") # train the model... m4.fit(data_train['inputs'], data_train['targets']) # Dump the model m4.save(\"model_files/model.h5\") Uploading composite models to Kipoi Since every Kipoi model pipeline consists of a single dataloader and a single model, we have to pack multiple models either into a single model or a single dataloader. Here is the recommendation how to do so: All models in the same framework Dataloader: newly written, combines dataloaders Model: combines models by stitching them together in the framework Different frameworks, compatible dependencies Dataloader: newly written, combines dataloaders and models Model: final ensembling model (model 4) Different frameworks, in-compatible dependencies Dataloader: newly written, loads data from the hdf5 files containing model predictions Model: final ensembling model (model 4)","title":"Composing models"},{"location":"tutorials/composing_models/#composing-models","text":"by Ziga Avsec Composing models means that we take the predictions of some model and use it as input for another model like this: Three different scenarios can occur when we want to compose models from Kipoi: all models are written in the same framework (say Keras) models are written in different frameworks but can all be executed in the same python environment models are written in different frameworks and can't be executed in the same python environment due to dependency incompatibilities","title":"Composing models"},{"location":"tutorials/composing_models/#all-models-in-the-same-framework","text":"In case all models are written in the same framework, you can stitch things together in the framework. Here is an example of how to do this in Keras. Let's first dump 4 dummy models: import keras.layers as kl from keras.models import Model from keras.models import load_model # create model 1 inp1 = kl.Input((3,), name=\"input1\") out1 = kl.Dense(4)(inp1) m1 = Model(inp1, out1) m1.save(\"/tmp/m1.h5\") # create model 2 inp2 = kl.Input((7,), name=\"input1_model1\") out2 = kl.Dense(3)(inp2) m2 = Model(inp2, out2) m2.save(\"/tmp/m2.h5\") # create model 3 inp3 = kl.Input((6,), name=\"input2\") out3 = kl.Dense(4)(inp3) m3 = Model(inp3, out3) m3.save(\"/tmp/m3.h5\") # create model 4 inp4 = kl.Input((7,), name=\"model2_model3\") out4 = kl.Dense(1)(inp4) m4 = Model(inp4, out4) m4.save(\"/tmp/m4.h5\") Next, we load the models back: ## Load models m1 = load_model(\"/tmp/m1.h5\") m2 = load_model(\"/tmp/m2.h5\") m3 = load_model(\"/tmp/m3.h5\") m4 = load_model(\"/tmp/m4.h5\") /opt/modules/i12g/anaconda/3-5.0.1/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. warnings.warn('No training configuration found in save file: ' And compose them m2_in = kl.concatenate([m1.output, m1.input]) m2_out = m2(m2_in) m3_in = kl.concatenate([m2_out, m3.output]) out = m4(m3_in) m = Model(inputs=[m1.input, m3.input], outputs=out) from IPython.display import SVG from keras.utils.vis_utils import model_to_dot svg_img = model_to_dot(m, ).create(prog='dot', format='svg') SVG(svg_img) Now we could go ahead, merge the dataloaders from model1 and model3 into a single one (providing input1 and input2) and train this global network for a new task. In case we would like to freeze supparts of the network, we should 'freeze' the underlying models by setting m1.trainable = False .","title":"All models in the same framework"},{"location":"tutorials/composing_models/#contributing-to-kipoi","text":"To contribute such model to Kipoi, we would need to submit the merged dataloader (providing input1 and input2 from raw files) and dump the stitched Keras model.","title":"Contributing to Kipoi"},{"location":"tutorials/composing_models/#models-in-different-frameworks","text":"There are two scenarios when composing models from different frameworks. Either their dependencies (dataloader, etc) are compatible (say a tensorflow and a keras model) or they are incompatible (one model uses keras=0.3 and and another one keras=2.0 ).","title":"Models in different frameworks"},{"location":"tutorials/composing_models/#compatible-dependencies","text":"To compose compatible models, we pack the majority of the models into the dataloader and then have the final ensembling model stored as the model. def new_dataloader(dl1_kwargs, dl2_kwargs, target_file, batch_size=32, num_workers=1): m1 = kipoi.get_model(\"model1\") m2 = kipoi.get_model(\"model2\") m3 = kipoi.get_model(\"model3\") dl1 = m1.default_dataloader(**dl1_kwargs) dl2 = m1.default_dataloader(**dl2_kwargs) target_gen = get_target_gen(target_file) batch_it1 = dl1.batch_iter(batch_size=batch_size, num_workers=num_workers) batch_it2 = dl2.batch_iter(batch_size=batch_size, num_workers=num_workers) while True: batch1 = next(batch_it1)['inputs'] batch2 = next(batch_it2)['inputs'] targets, ids = next(target_gen) m1_pred = m1.predict_on_batch(batch1) m2_pred = m2.predict_on_batch(np.concatenate((batch1, m1_pred), axis=1)) m3_pred = m3.predict_on_batch(batch2) yield {\"inputs\": {\"model2\": m2_pred, \"model3\": m3_pred}, \"targets\": targets, \"metadata\": {\"model1_id\": batch1[\"metadata\"][\"id\"], \"model3_id\": batch2[\"metadata\"][\"id\"], \"targets_id\": ids, } } # create model 4 inp2 = kl.Input((3,), name=\"model2\") inp3 = kl.Input((4,), name=\"model3\") x = kl.concatenate([inp2, inp3]) out4 = kl.Dense(1)(x) m4 = Model([inp2, inp3], out4) m4.compile('rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # Train model4 def create_train_gen(**kwargs): while True: gen = new_dataloader(**kwargs) while True: batch = next(gen) yield (batch['inputs'], batch['targets']) train_gen = create_train_gen(...) m4.fit_generator(train_gen, ...) # Dump model4 m4.save(\"model_files/model.h5\")","title":"Compatible dependencies"},{"location":"tutorials/composing_models/#incompatible-dependencies","text":"Sometimes, making a prediction for all the models in the same python environment might be difficult or impossible due to the incompatible dependencies. In that case, we should run the prediction of each model in a separate environment and save the predictions to the disk. Luckily, there exist many Make-like tools that can support this kind of a workflow. My favorite is Snakemake http://snakemake.readthedocs.io/ . I'll show you how to do this in snakemake. Let's consider the following case: # Python part of the Snakefile import os import subprocess py_path = subprocess.check_output(['which', 'python']).decode().strip() env_paths = os.path.join(os.path.dirname(py_path), \"../envs\") def get_args(wildcards): \"\"\"Function returning a dictionary of dataloader kwargs for the corresponding model \"\"\" if wildcards.model == \"model3\": return {\"arg1\": 1} elif wildcards.model == \"model3\": return {\"\"} else: return {\"arg2\": 1} # Yaml part of the Snakefile rule all: inputs: expand(\"predictions/{model}.h5\", [\"model1\", \"model2\"]) rule create_evironment: \"\"\"Create a new conda environment for each model\"\"\" output: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") shell: \"kipoi env create {wildcards.model} -e kipoi-{wildcards.model}\" rule run_predictions: \"\"\"Create a new conda environment for each model\"\"\" input: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\") output: \"predictions/{model}.h5\" params: dl_args: get_args batch_size: 15 threads: 8 shell: \"\"\" source activate kipoi-{wildcards.model} kipoi predict {wildcards.model} \\ -n {threads} \\ --dataloader_args='{params.dl_args}' \\ --batch_size={params.batch_size} \\ -f hdf5 \\ -o {output} \"\"\" This snakefile will generate the following hdf5 files predictions/model1.h5 predictions/model2.h5 To combine them, let's write new dataloader, taking as input the hdf5 files containing predictions import deepdish def new_dataloader(model1_h5, model2_h5, target_file): d1 = deepdish.io.load(model1_h5) d2 = deepdish.io.load(model2_h5) targets = load_target_file(target_file) return { \"inputs\": { \"model1\": d1[\"predictions\"], \"model2\": d2[\"predictions\"], }, \"targets\": targets, \"metadata\": { \"model1_id\": d1[\"metdata\"][\"id\"], \"model2_id\": d2[\"metdata\"][\"id\"], } } # get the training data ... data_train = new_dataloader(\"predictions/model1.h5\" \"predictions/model1.h5\", \"target_file.h5\") # train the model... m4.fit(data_train['inputs'], data_train['targets']) # Dump the model m4.save(\"model_files/model.h5\")","title":"Incompatible dependencies"},{"location":"tutorials/composing_models/#uploading-composite-models-to-kipoi","text":"Since every Kipoi model pipeline consists of a single dataloader and a single model, we have to pack multiple models either into a single model or a single dataloader. Here is the recommendation how to do so: All models in the same framework Dataloader: newly written, combines dataloaders Model: combines models by stitching them together in the framework Different frameworks, compatible dependencies Dataloader: newly written, combines dataloaders and models Model: final ensembling model (model 4) Different frameworks, in-compatible dependencies Dataloader: newly written, loads data from the hdf5 files containing model predictions Model: final ensembling model (model 4)","title":"Uploading composite models to Kipoi"},{"location":"tutorials/contributing_models/","text":"Generated from notebooks/contributing_models.ipynb Contributing a model to the Kipoi model repository This notebook will show you how to contribute a model to the Kipoi model repository . For a simple 'model contribution checklist' see also http://kipoi.org/docs/contributing/01_Getting_started/ . Kipoi basics Contributing a model to Kipoi means writing a sub-folder with all the required files to the Kipoi model repository via pull request. Two main components of the model repository are model and dataloader . Model Model takes as input numpy arrays and outputs numpy arrays. In practice, a model needs to implement the predict_on_batch(x) method, where x is dictionary/list of numpy arrays. The model contributor needs to provide one of the following: Serialized Keras model Serialized Sklearn model Custom model inheriting from keras.model.BaseModel . all the required files, i.e. weights need to be loaded in the __init__ See http://kipoi.org/docs/contributing/02_Writing_model.yaml/ and http://kipoi.org/docs/contributing/05_Writing_model.py/ for more info. Dataloader Dataloader takes raw file paths or other parameters as argument and outputs modelling-ready numpy arrays. The dataloading can be done through a generator---batch-by-batch, sample-by-sample---or by just returning the whole dataset. The goal is to work really with raw files (say fasta, bed, vcf, etc in bioinformatics), as this allows to make model predictions on new datasets without going through the burden of running custom pre-processing scripts. The model contributor needs to implement one of the following: PreloadedDataset Dataset BatchDataset SampleIterator BatchIterator SampleGenerator BatchGenerator See http://kipoi.org/docs/contributing/04_Writing_dataloader.py/ for more info. Folder layout Here is an example folder structure of a Kipoi model: \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u251c\u2500\u2500 dataloader_files/ #/ files required by the dataloader \u2502 \u251c\u2500\u2500 x_transfomer.pkl \u2502 \u2514\u2500\u2500 y_transfomer.pkl \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 model_files/ #/ files required by the model \u2502 \u251c\u2500\u2500 model.json \u2502 \u2514\u2500\u2500 weights.h5 \u2514\u2500\u2500 example_files/ #/ small example files \u251c\u2500\u2500 features.csv \u2514\u2500\u2500 targets.csv Two most important files are model.yaml and dataloader.yaml . They provide a complete description about the model, the dataloader and the files they depend on. Contributing a simple Iris-classifier Details about the individual files will be revealed throught the tutorial below. A simple Keras model will be trained to predict the Iris plant class from the well-known Iris dataset. Outline Train the model Generate dataloader_files/ Generate model_files/ Generate example_files/ Write model.yaml Write dataloader.yaml Write dataloader.py Test with the model with $ kipoi test . 1. Train the model Load and pre-process the data import pandas as pd import os from sklearn.preprocessing import LabelBinarizer, StandardScaler from sklearn import datasets iris = datasets.load_iris() # view more info about the dataset # print(iris[\"DESCR\"]) # Data pre-processing y_transformer = LabelBinarizer().fit(iris[\"target\"]) x_transformer = StandardScaler().fit(iris[\"data\"]) x = x_transformer.transform(iris[\"data\"]) y = y_transformer.transform(iris[\"target\"]) x[:3] array([[-0.9007, 1.0321, -1.3413, -1.313 ], [-1.143 , -0.125 , -1.3413, -1.313 ], [-1.3854, 0.3378, -1.3981, -1.313 ]]) y[:3] array([[1, 0, 0], [1, 0, 0], [1, 0, 0]]) Train an example model Let's train a simple linear-regression model using Keras. from keras.models import Model import keras.layers as kl inp = kl.Input(shape=(4, ), name=\"features\") out = kl.Dense(units=3)(inp) model = Model(inp, out) model.compile(\"adam\", \"categorical_crossentropy\") model.fit(x, y, verbose=0) Using TensorFlow backend. <keras.callbacks.History at 0x7ff456b9b9b0> 2. Generate dataloader_files/ Now that we have everything we need, let's start writing the files to model's directory (here model_template/ ). In reality, you would need to Fork the kipoi/models repository Clone your repository fork, ignoring all the git-lfs files $ git lfs clone git@github.com:<your_username>/models.git '-I /' Create a new folder <mynewmodel> containing all the model files in the repostiory root put all the non-code files (serialized models, test data) into a *files/ directory, where * can be anything. These will namely be tracked by git-lfs instead of git . Examples: model_files/ , dataloader_files/ Test your repository locally: $ kipoi test <mynewmodel_folder> Commit, push to your forked remote and submit a pull request to github.com/kipoi/models Dataloader can use some trained transformer (here the LabelBinarizer and StandardScaler transformers form sklearn). These should be written to dataloader_files/ . cd ../examples/sklearn_iris/ /data/nasif12/home_if12/avsec/workspace/kipoi/kipoi/examples/sklearn_iris os.makedirs(\"dataloader_files\", exist_ok=True) ls \u001b[0m\u001b[38;5;27mdataloader_files\u001b[0m/ dataloader.pyc \u001b[38;5;27mexample_files\u001b[0m/ model.yaml dataloader.py dataloader.yaml \u001b[38;5;27mmodel_files\u001b[0m/ \u001b[38;5;27m__pycache__\u001b[0m/ import pickle with open(\"dataloader_files/y_transformer.pkl\", \"wb\") as f: pickle.dump(y_transformer, f, protocol=2) with open(\"dataloader_files/x_transformer.pkl\", \"wb\") as f: pickle.dump(x_transformer, f, protocol=2) ls dataloader_files x_transformer.pkl y_transformer.pkl 3. Generate model_files/ The serialized model weights and architecture go to model_files/ . os.makedirs(\"model_files\", exist_ok=True) # Architecture with open(\"model_files/model.json\", \"w\") as f: f.write(model.to_json()) # Weights model.save_weights(\"model_files/weights.h5\") # Alternatively, for the scikit-learn model we would save the pickle file from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier lr = OneVsRestClassifier(LogisticRegression()) lr.fit(x, y) with open(\"model_files/sklearn_model.pkl\", \"wb\") as f: pickle.dump(lr, f, protocol=2) 4. Generate example_files/ example_files/ should contain a small subset of the raw files the dataloader will read. Numpy arrays -> pd.DataFrame iris.keys() dict_keys(['target_names', 'feature_names', 'data', 'DESCR', 'target']) X = pd.DataFrame(iris[\"data\"][:20], columns=iris[\"feature_names\"]) X.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 y = pd.DataFrame({\"class\": iris[\"target\"][:20]}) y.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } class 0 0 1 0 2 0 3 0 4 0 Save example files os.makedirs(\"example_files\", exist_ok=True) X.to_csv(\"example_files/features.csv\", index=False) y.to_csv(\"example_files/targets.csv\", index=False) !head -n 2 example_files/targets.csv class 0 !head -n 2 example_files/features.csv sepal length (cm),sepal width (cm),petal length (cm),petal width (cm) 5.1,3.5,1.4,0.2 5. Write model.yaml The model.yaml for this model should look like this: type: keras # use `kipoi.model.KerasModel` args: # arguments of `kipoi.model.KerasModel` arch: model_files/model.json weights: model_files/weights.h5 default_dataloader: . # path to the dataloader directory. Here it's defined in the same directory info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species version: 0.1 # optional cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python=3.5 - h5py # - soumith::pytorch # specify packages from other channels via <channel>::<package> pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" All file paths are relative relative to model.yaml . 6. Write dataloader.yaml type: Dataset defined_as: dataloader.py::MyDataset # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: example_files/features.csv # example files targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: example_files/targets.csv optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org version: 0.1 doc: Model predicting the Iris species dependencies: conda: - python=3.5 - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: Features in cm: sepal length, sepal width, petal length, petal width. targets: shape: (3, ) doc: One-hot encoded array of classes: setosa, versicolor, virginica. metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: shape: int doc: Just an example metadata column 7. Write dataloader.py Finally, let's implement MyDataset. We need to implement two methods: __len__ and __getitem__ . __getitem__ will return one item of the dataset. In our case, this is a dictionary with output_schema described in dataloader.yaml . For more information about writing such dataloaders, see the Data Loading and Processing Tutorial from pytorch . import pickle from kipoi.data import Dataset import pandas as pd import numpy as np def read_pickle(f): with open(f, \"rb\") as f: return pickle.load(f) class MyDataset(Dataset): def __init__(self, features_file, targets_file=None): self.features_file = features_file self.targets_file = targets_file self.y_transformer = read_pickle(\"dataloader_files/y_transformer.pkl\") self.x_transformer = read_pickle(\"dataloader_files/x_transformer.pkl\") self.features = pd.read_csv(features_file) if targets_file is not None: self.targets = pd.read_csv(targets_file) assert len(self.targets) == len(self.features) def __len__(self): return len(self.features) def __getitem__(self, idx): x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis])) if self.targets_file is None: y_class = {} else: y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis])) return { \"inputs\": { \"features\": x_features }, \"targets\": y_class, \"metadata\": { \"example_row_number\": idx } } Example usage of the dataset ds = MyDataset(\"example_files/features.csv\", \"example_files/targets.csv\") # call __getitem__ ds[5] {'inputs': {'features': array([-0.5372, 1.9577, -1.1707, -1.05 ])}, 'metadata': {'example_row_number': 5}, 'targets': array([1, 0, 0])} Since MyDatset inherits from kipoi.data.Dataset , it has some additional nice feature. See python-sdk.ipynb for more information. # batch-iterator it = ds.batch_iter(batch_size=3, shuffle=False, num_workers=2) next(it) {'inputs': {'features': array([[-0.9007, 1.0321, -1.3413, -1.313 ], [-1.143 , -0.125 , -1.3413, -1.313 ], [-1.3854, 0.3378, -1.3981, -1.313 ]])}, 'metadata': {'example_row_number': array([0, 1, 2])}, 'targets': array([[1, 0, 0], [1, 0, 0], [1, 0, 0]])} # ds.load_all() # load the whole dataset into memory 8. Test with the model with $ kipoi test . Before we contribute the model to the repository, let's run the test: !kipoi test . \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from dataloader.py::MyDataset\u001b[0m Using TensorFlow backend. 2017-11-29 17:26:21.755321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations. 2017-11-29 17:26:21.755368: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations. 2017-11-29 17:26:21.755385: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. 2017-11-29 17:26:21.755399: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations. 2017-11-29 17:26:21.755414: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations. \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model architecture from <_io.TextIOWrapper name='model_files/model.json' mode='r' encoding='UTF-8'>\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model weights from model_files/weights.h5\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Initialized data generator. Running batches...\u001b[0m /opt/modules/i12g/anaconda/3-4.1.1/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) /opt/modules/i12g/anaconda/3-4.1.1/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Returned data schema correct\u001b[0m 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 89.45it/s] \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m predict_example done!\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Successfully ran test_predict\u001b[0m \u001b[0m This command did the following: validated if output_schema defined in dataloader.yaml matches the shapes of the returned arrays validated that model and dataloader are compatible in inputs and targets executed the model pipeline for the example Accessing the model through kipoi import kipoi reload(kipoi) <module 'kipoi' from '/data/nasif12/home_if12/avsec/projects-work/kipoi/kipoi/__init__.py'> m = kipoi.get_model(\".\", source=\"dir\") # See also python-sdk.ipynb Using TensorFlow backend. m.pipeline.predict({\"features_file\": \"example_files/features.csv\", \"targets_file\": \"example_files/targets.csv\" })[:5] array([[ 1.5356, -0.8118, -0.2712], [ 0.4649, -0.22 , -1.1491], [ 0.6735, -0.1923, -0.8083], [ 0.3958, 0.0178, -0.9159], [ 1.6362, -0.79 , -0.0849]], dtype=float32) m.info Info(authors=[Author(name='Your Name', github='your_github_username', email=None)], doc='Model predicting the Iris species', name=None, version='0.1', tags=[]) m.default_dataloader dataloader.MyDataset m.model <keras.engine.training.Model at 0x7f0dbe68f8d0> m.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f0dc19cf400>> Recap Congrats! You made it through the tutorial! Feel free to use this model for your model template. Alternatively, you can use kipoi init to setup a model directory. Make sure you have read the getting started guide for contributing models.","title":"Contributing models"},{"location":"tutorials/contributing_models/#contributing-a-model-to-the-kipoi-model-repository","text":"This notebook will show you how to contribute a model to the Kipoi model repository . For a simple 'model contribution checklist' see also http://kipoi.org/docs/contributing/01_Getting_started/ .","title":"Contributing a model to the Kipoi model repository"},{"location":"tutorials/contributing_models/#kipoi-basics","text":"Contributing a model to Kipoi means writing a sub-folder with all the required files to the Kipoi model repository via pull request. Two main components of the model repository are model and dataloader .","title":"Kipoi basics"},{"location":"tutorials/contributing_models/#model","text":"Model takes as input numpy arrays and outputs numpy arrays. In practice, a model needs to implement the predict_on_batch(x) method, where x is dictionary/list of numpy arrays. The model contributor needs to provide one of the following: Serialized Keras model Serialized Sklearn model Custom model inheriting from keras.model.BaseModel . all the required files, i.e. weights need to be loaded in the __init__ See http://kipoi.org/docs/contributing/02_Writing_model.yaml/ and http://kipoi.org/docs/contributing/05_Writing_model.py/ for more info.","title":"Model"},{"location":"tutorials/contributing_models/#dataloader","text":"Dataloader takes raw file paths or other parameters as argument and outputs modelling-ready numpy arrays. The dataloading can be done through a generator---batch-by-batch, sample-by-sample---or by just returning the whole dataset. The goal is to work really with raw files (say fasta, bed, vcf, etc in bioinformatics), as this allows to make model predictions on new datasets without going through the burden of running custom pre-processing scripts. The model contributor needs to implement one of the following: PreloadedDataset Dataset BatchDataset SampleIterator BatchIterator SampleGenerator BatchGenerator See http://kipoi.org/docs/contributing/04_Writing_dataloader.py/ for more info.","title":"Dataloader"},{"location":"tutorials/contributing_models/#folder-layout","text":"Here is an example folder structure of a Kipoi model: \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u251c\u2500\u2500 dataloader_files/ #/ files required by the dataloader \u2502 \u251c\u2500\u2500 x_transfomer.pkl \u2502 \u2514\u2500\u2500 y_transfomer.pkl \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 model_files/ #/ files required by the model \u2502 \u251c\u2500\u2500 model.json \u2502 \u2514\u2500\u2500 weights.h5 \u2514\u2500\u2500 example_files/ #/ small example files \u251c\u2500\u2500 features.csv \u2514\u2500\u2500 targets.csv Two most important files are model.yaml and dataloader.yaml . They provide a complete description about the model, the dataloader and the files they depend on.","title":"Folder layout"},{"location":"tutorials/contributing_models/#contributing-a-simple-iris-classifier","text":"Details about the individual files will be revealed throught the tutorial below. A simple Keras model will be trained to predict the Iris plant class from the well-known Iris dataset.","title":"Contributing a simple Iris-classifier"},{"location":"tutorials/contributing_models/#outline","text":"Train the model Generate dataloader_files/ Generate model_files/ Generate example_files/ Write model.yaml Write dataloader.yaml Write dataloader.py Test with the model with $ kipoi test .","title":"Outline"},{"location":"tutorials/contributing_models/#1-train-the-model","text":"","title":"1. Train the model"},{"location":"tutorials/contributing_models/#load-and-pre-process-the-data","text":"import pandas as pd import os from sklearn.preprocessing import LabelBinarizer, StandardScaler from sklearn import datasets iris = datasets.load_iris() # view more info about the dataset # print(iris[\"DESCR\"]) # Data pre-processing y_transformer = LabelBinarizer().fit(iris[\"target\"]) x_transformer = StandardScaler().fit(iris[\"data\"]) x = x_transformer.transform(iris[\"data\"]) y = y_transformer.transform(iris[\"target\"]) x[:3] array([[-0.9007, 1.0321, -1.3413, -1.313 ], [-1.143 , -0.125 , -1.3413, -1.313 ], [-1.3854, 0.3378, -1.3981, -1.313 ]]) y[:3] array([[1, 0, 0], [1, 0, 0], [1, 0, 0]])","title":"Load and pre-process the data"},{"location":"tutorials/contributing_models/#train-an-example-model","text":"Let's train a simple linear-regression model using Keras. from keras.models import Model import keras.layers as kl inp = kl.Input(shape=(4, ), name=\"features\") out = kl.Dense(units=3)(inp) model = Model(inp, out) model.compile(\"adam\", \"categorical_crossentropy\") model.fit(x, y, verbose=0) Using TensorFlow backend. <keras.callbacks.History at 0x7ff456b9b9b0>","title":"Train an example model"},{"location":"tutorials/contributing_models/#2-generate-dataloader_files","text":"Now that we have everything we need, let's start writing the files to model's directory (here model_template/ ). In reality, you would need to Fork the kipoi/models repository Clone your repository fork, ignoring all the git-lfs files $ git lfs clone git@github.com:<your_username>/models.git '-I /' Create a new folder <mynewmodel> containing all the model files in the repostiory root put all the non-code files (serialized models, test data) into a *files/ directory, where * can be anything. These will namely be tracked by git-lfs instead of git . Examples: model_files/ , dataloader_files/ Test your repository locally: $ kipoi test <mynewmodel_folder> Commit, push to your forked remote and submit a pull request to github.com/kipoi/models Dataloader can use some trained transformer (here the LabelBinarizer and StandardScaler transformers form sklearn). These should be written to dataloader_files/ . cd ../examples/sklearn_iris/ /data/nasif12/home_if12/avsec/workspace/kipoi/kipoi/examples/sklearn_iris os.makedirs(\"dataloader_files\", exist_ok=True) ls \u001b[0m\u001b[38;5;27mdataloader_files\u001b[0m/ dataloader.pyc \u001b[38;5;27mexample_files\u001b[0m/ model.yaml dataloader.py dataloader.yaml \u001b[38;5;27mmodel_files\u001b[0m/ \u001b[38;5;27m__pycache__\u001b[0m/ import pickle with open(\"dataloader_files/y_transformer.pkl\", \"wb\") as f: pickle.dump(y_transformer, f, protocol=2) with open(\"dataloader_files/x_transformer.pkl\", \"wb\") as f: pickle.dump(x_transformer, f, protocol=2) ls dataloader_files x_transformer.pkl y_transformer.pkl","title":"2. Generate dataloader_files/"},{"location":"tutorials/contributing_models/#3-generate-model_files","text":"The serialized model weights and architecture go to model_files/ . os.makedirs(\"model_files\", exist_ok=True) # Architecture with open(\"model_files/model.json\", \"w\") as f: f.write(model.to_json()) # Weights model.save_weights(\"model_files/weights.h5\") # Alternatively, for the scikit-learn model we would save the pickle file from sklearn.linear_model import LogisticRegression from sklearn.multiclass import OneVsRestClassifier lr = OneVsRestClassifier(LogisticRegression()) lr.fit(x, y) with open(\"model_files/sklearn_model.pkl\", \"wb\") as f: pickle.dump(lr, f, protocol=2)","title":"3. Generate model_files/"},{"location":"tutorials/contributing_models/#4-generate-example_files","text":"example_files/ should contain a small subset of the raw files the dataloader will read.","title":"4. Generate example_files/"},{"location":"tutorials/contributing_models/#numpy-arrays-pddataframe","text":"iris.keys() dict_keys(['target_names', 'feature_names', 'data', 'DESCR', 'target']) X = pd.DataFrame(iris[\"data\"][:20], columns=iris[\"feature_names\"]) X.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 y = pd.DataFrame({\"class\": iris[\"target\"][:20]}) y.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } class 0 0 1 0 2 0 3 0 4 0","title":"Numpy arrays -&gt; pd.DataFrame"},{"location":"tutorials/contributing_models/#save-example-files","text":"os.makedirs(\"example_files\", exist_ok=True) X.to_csv(\"example_files/features.csv\", index=False) y.to_csv(\"example_files/targets.csv\", index=False) !head -n 2 example_files/targets.csv class 0 !head -n 2 example_files/features.csv sepal length (cm),sepal width (cm),petal length (cm),petal width (cm) 5.1,3.5,1.4,0.2","title":"Save example files"},{"location":"tutorials/contributing_models/#5-write-modelyaml","text":"The model.yaml for this model should look like this: type: keras # use `kipoi.model.KerasModel` args: # arguments of `kipoi.model.KerasModel` arch: model_files/model.json weights: model_files/weights.h5 default_dataloader: . # path to the dataloader directory. Here it's defined in the same directory info: # General information about the model authors: - name: Your Name github: your_github_username email: your_email@host.org doc: Model predicting the Iris species version: 0.1 # optional cite_as: https://doi.org:/... # preferably a doi url to the paper trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description license: MIT # Software License - defaults to MIT dependencies: conda: # install via conda - python=3.5 - h5py # - soumith::pytorch # specify packages from other channels via <channel>::<package> pip: # install via pip - keras>=2.0.4 - tensorflow>=1.0 schema: # Model schema inputs: features: shape: (4,) # array shape of a single sample (omitting the batch dimension) doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\" targets: shape: (3,) doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\" All file paths are relative relative to model.yaml .","title":"5. Write model.yaml"},{"location":"tutorials/contributing_models/#6-write-dataloaderyaml","text":"type: Dataset defined_as: dataloader.py::MyDataset # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py args: features_file: # descr: > allows multi-line fields doc: > Csv file of the Iris Plants Database from http://archive.ics.uci.edu/ml/datasets/Iris features. type: str example: example_files/features.csv # example files targets_file: doc: > Csv file of the Iris Plants Database targets. Not required for making the prediction. type: str example: example_files/targets.csv optional: True # if not present, the `targets` field will not be present in the dataloader output info: authors: - name: Your Name github: your_github_account email: your_email@host.org version: 0.1 doc: Model predicting the Iris species dependencies: conda: - python=3.5 - pandas - numpy - sklearn output_schema: inputs: features: shape: (4,) doc: Features in cm: sepal length, sepal width, petal length, petal width. targets: shape: (3, ) doc: One-hot encoded array of classes: setosa, versicolor, virginica. metadata: # field providing additional information to the samples (not directly required by the model) example_row_number: shape: int doc: Just an example metadata column","title":"6. Write dataloader.yaml"},{"location":"tutorials/contributing_models/#7-write-dataloaderpy","text":"Finally, let's implement MyDataset. We need to implement two methods: __len__ and __getitem__ . __getitem__ will return one item of the dataset. In our case, this is a dictionary with output_schema described in dataloader.yaml . For more information about writing such dataloaders, see the Data Loading and Processing Tutorial from pytorch . import pickle from kipoi.data import Dataset import pandas as pd import numpy as np def read_pickle(f): with open(f, \"rb\") as f: return pickle.load(f) class MyDataset(Dataset): def __init__(self, features_file, targets_file=None): self.features_file = features_file self.targets_file = targets_file self.y_transformer = read_pickle(\"dataloader_files/y_transformer.pkl\") self.x_transformer = read_pickle(\"dataloader_files/x_transformer.pkl\") self.features = pd.read_csv(features_file) if targets_file is not None: self.targets = pd.read_csv(targets_file) assert len(self.targets) == len(self.features) def __len__(self): return len(self.features) def __getitem__(self, idx): x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis])) if self.targets_file is None: y_class = {} else: y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis])) return { \"inputs\": { \"features\": x_features }, \"targets\": y_class, \"metadata\": { \"example_row_number\": idx } }","title":"7. Write dataloader.py"},{"location":"tutorials/contributing_models/#example-usage-of-the-dataset","text":"ds = MyDataset(\"example_files/features.csv\", \"example_files/targets.csv\") # call __getitem__ ds[5] {'inputs': {'features': array([-0.5372, 1.9577, -1.1707, -1.05 ])}, 'metadata': {'example_row_number': 5}, 'targets': array([1, 0, 0])} Since MyDatset inherits from kipoi.data.Dataset , it has some additional nice feature. See python-sdk.ipynb for more information. # batch-iterator it = ds.batch_iter(batch_size=3, shuffle=False, num_workers=2) next(it) {'inputs': {'features': array([[-0.9007, 1.0321, -1.3413, -1.313 ], [-1.143 , -0.125 , -1.3413, -1.313 ], [-1.3854, 0.3378, -1.3981, -1.313 ]])}, 'metadata': {'example_row_number': array([0, 1, 2])}, 'targets': array([[1, 0, 0], [1, 0, 0], [1, 0, 0]])} # ds.load_all() # load the whole dataset into memory","title":"Example usage of the dataset"},{"location":"tutorials/contributing_models/#8-test-with-the-model-with-kipoi-test","text":"Before we contribute the model to the repository, let's run the test: !kipoi test . \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from dataloader.py::MyDataset\u001b[0m Using TensorFlow backend. 2017-11-29 17:26:21.755321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations. 2017-11-29 17:26:21.755368: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations. 2017-11-29 17:26:21.755385: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. 2017-11-29 17:26:21.755399: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations. 2017-11-29 17:26:21.755414: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations. \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model architecture from <_io.TextIOWrapper name='model_files/model.json' mode='r' encoding='UTF-8'>\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model weights from model_files/weights.h5\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Initialized data generator. Running batches...\u001b[0m /opt/modules/i12g/anaconda/3-4.1.1/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) /opt/modules/i12g/anaconda/3-4.1.1/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Returned data schema correct\u001b[0m 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 89.45it/s] \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m predict_example done!\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Successfully ran test_predict\u001b[0m \u001b[0m This command did the following: validated if output_schema defined in dataloader.yaml matches the shapes of the returned arrays validated that model and dataloader are compatible in inputs and targets executed the model pipeline for the example","title":"8. Test with the model with $ kipoi test ."},{"location":"tutorials/contributing_models/#accessing-the-model-through-kipoi","text":"import kipoi reload(kipoi) <module 'kipoi' from '/data/nasif12/home_if12/avsec/projects-work/kipoi/kipoi/__init__.py'> m = kipoi.get_model(\".\", source=\"dir\") # See also python-sdk.ipynb Using TensorFlow backend. m.pipeline.predict({\"features_file\": \"example_files/features.csv\", \"targets_file\": \"example_files/targets.csv\" })[:5] array([[ 1.5356, -0.8118, -0.2712], [ 0.4649, -0.22 , -1.1491], [ 0.6735, -0.1923, -0.8083], [ 0.3958, 0.0178, -0.9159], [ 1.6362, -0.79 , -0.0849]], dtype=float32) m.info Info(authors=[Author(name='Your Name', github='your_github_username', email=None)], doc='Model predicting the Iris species', name=None, version='0.1', tags=[]) m.default_dataloader dataloader.MyDataset m.model <keras.engine.training.Model at 0x7f0dbe68f8d0> m.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f0dc19cf400>>","title":"Accessing the model through kipoi"},{"location":"tutorials/contributing_models/#recap","text":"Congrats! You made it through the tutorial! Feel free to use this model for your model template. Alternatively, you can use kipoi init to setup a model directory. Make sure you have read the getting started guide for contributing models.","title":"Recap"},{"location":"tutorials/python-api/","text":"Generated from notebooks/python-api.ipynb Kipoi python API Quick start There are three basic building blocks in kipoi: Source - provides Models and DataLoaders. Model - makes the prediction given the numpy arrays. Dataloader - loads the data from raw files and transforms them into a form that is directly consumable by the Model List of main commands Get/list sources - kipoi.list_sources() - kipoi.get_source() List models/dataloaders - kipoi.list_models() - kipoi.list_dataloaders() Get model/dataloader - kipoi.get_model() - kipoi.get_dataloader_factory() Load only model/dataloader description from the yaml file without loading the model kipoi.get_model_descr() kipoi.get_dataloader_descr() Install the dependencies - kipoi.install_model_dependencies() - kipoi.install_dataloader_dependencies() import kipoi Source Available sources are specified in the config file located at: ~/.kipoi/config.yaml . Here is an example config file: model_sources: kipoi: # default type: git-lfs # git repository with large file storage (git-lfs) remote_url: git@github.com:kipoi/models.git # git remote local_path: ~/.kipoi/models/ # local storage path gl: type: git-lfs # custom model remote_url: https://i12g-gagneurweb.informatik.tu-muenchen.de/gitlab/gagneurlab/model-zoo.git local_path: /s/project/model-zoo There are three different model sources possible: git-lfs - git repository with source files tracked normally by git and all the binary files like model weights (located in files* directories) are tracked by git-lfs . Requires git-lfs to be installed. git - all the files including weights (not recommended) local - local directory containing models defined in subdirectories For git-lfs source type, larger files tracked by git-lfs will be downloaded into the specified directory local_path only after the model has been requested (when invoking kipoi.get_model() ). Note A particular model/dataloader is defined by its source (say kipoi or my_git_models ) and the relative path of the desired model directory from the model source root (say rbp/ ). A directory is considered a model if it contains a model.yaml file. import kipoi import warnings warnings.filterwarnings('ignore') import logging logging.disable(1000) kipoi.list_sources() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source type location local_size n_models n_dataloaders 0 kipoi git-lfs /home/avsec/.kipoi/mo... 1,2G 780 780 s = kipoi.get_source(\"kipoi\") s GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') kipoi.list_models().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags 0 kipoi DeepSEAKeras 0.1 [Author(name='Jian Zh... [Author(name='Lara Ur... This CNN is based on ... keras seq TFBS_DHS_probs True MIT https://doi.org/10.10... ENCODE and Roadmap Ep... https://www.nature.co... [Histone modification... 1 kipoi extended_coda 0.1 [Author(name='Pang We... [Author(name='Johnny ... Single bp resolution ... keras [H3K27AC_subsampled] [H3K27ac] False MIT https://doi.org/10.10... Described in https://... Described in https://... [Histone modification] 2 kipoi DeepCpG_DNA/Hou2016_m... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/mESC1, cpg/mESC2... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 3 kipoi DeepCpG_DNA/Smallwood... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/BS24_1_2I, cpg/B... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 4 kipoi DeepCpG_DNA/Hou2016_H... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/HepG21, cpg/HepG... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] Model Let's choose to use the rbp_eclip/UPF1 model from kipoi MODEL = \"rbp_eclip/UPF1\" NOTE: If you are using python2, use a different model like MaxEntScan/3prime to following this example. # Note. Install all the dependencies for that model: # add --gpu flag to install gpu-compatible dependencies (e.g. installs tensorflow-gpu instead of tensorflow) !kipoi env install {MODEL} model = kipoi.get_model(MODEL) Available fields: Model type args info authors name version tags doc schema inputs targets default_dataloader - loaded dataloader class predict_on_batch() source source_dir pipeline predict() predict_example() predict_generator() Dataloader type defined_as args info (same as for the model) output_schema inputs targets metadata source source_dir example_kwargs init_example() batch_iter() batch_train_iter() batch_predict_iter() load_all() model <kipoi.model.KerasModel at 0x7f95b27af2b0> model.type 'keras' Info model.info ModelInfo(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='\\'RBP binding model from Avsec et al: \"Modeling positional effects of regulatory sequences with spline transformations increases prediction accuracy of deep neural networks\". \\' ', name=None, version='0.1', license='MIT', tags=['RNA binding'], contributors=[Author(name='Ziga Avsec', github='avsecz', email=None)], cite_as='https://doi.org/10.1093/bioinformatics/btx727', trained_on='RBP occupancy peaks measured by eCLIP-seq (Van Nostrand et al., 2016 - https://doi.org/10.1038/nmeth.3810), https://github.com/gagneurlab/Manuscript_Avsec_Bioinformatics_2017 ', training_procedure='Single task training with ADAM') model.info.version '0.1' Schema dict(model.schema.inputs) {'dist_exon_intron': ArraySchema(shape=(1, 10), doc='Distance the nearest exon_intron (splice donor) site transformed with B-splines', name='dist_exon_intron', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_end': ArraySchema(shape=(1, 10), doc='Distance the nearest gene end transformed with B-splines', name='dist_gene_end', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_start': ArraySchema(shape=(1, 10), doc='Distance the nearest gene start transformed with B-splines', name='dist_gene_start', special_type=None, associated_metadata=[], column_labels=None), 'dist_intron_exon': ArraySchema(shape=(1, 10), doc='Distance the nearest intron_exon (splice acceptor) site transformed with B-splines', name='dist_intron_exon', special_type=None, associated_metadata=[], column_labels=None), 'dist_polya': ArraySchema(shape=(1, 10), doc='Distance the nearest Poly-A site transformed with B-splines', name='dist_polya', special_type=None, associated_metadata=[], column_labels=None), 'dist_start_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest start codon transformed with B-splines', name='dist_start_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_stop_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest stop codon transformed with B-splines', name='dist_stop_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_tss': ArraySchema(shape=(1, 10), doc='Distance the nearest TSS site transformed with B-splines', name='dist_tss', special_type=None, associated_metadata=[], column_labels=None), 'seq': ArraySchema(shape=(101, 4), doc='One-hot encoded RNA sequence', name='seq', special_type=<ArraySpecialType.DNASeq: 'DNASeq'>, associated_metadata=[], column_labels=None)} model.schema.targets ArraySchema(shape=(1,), doc='Predicted binding strength', name=None, special_type=None, associated_metadata=[], column_labels=None) Default dataloader Model already has the default dataloder present. To use it, specify model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/UPF1' model.default_dataloader dataloader.SeqDistDataset model.default_dataloader.info Info(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='RBP binding model taking as input 101nt long sequence as well as 8 distances to nearest genomic landmarks - tss, poly-A, exon-intron boundary, intron-exon boundary, start codon, stop codon, gene start, gene end ', name=None, version='0.1', license='MIT', tags=[]) Predict_on_batch model.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f95b27af2b0>> Others # Model source model.source GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') # model location directory model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/UPF1' DataLoader DataLoader = kipoi.get_dataloader_factory(MODEL) # same as DataLoader = model.default_dataloader A dataloader will most likely require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. There are several options where the dataloader input keyword arguments are displayed: # Display information about the dataloader print(DataLoader.__doc__) Args: intervals_file: file path; tsv file Assumes bed-like `chrom start end id score strand` format. fasta_file: file path; Genome sequence gtf_file: file path; Genome annotation GTF file. filter_protein_coding: Considering genomic landmarks only for protein coding genes preproc_transformer: file path; tranformer used for pre-processing. target_file: file path; path to the targets batch_size: int # Alternatively the dataloader keyword arguments can be displayed using the function: kipoi.print_dl_kwargs(DataLoader) Keyword argument: `intervals_file` doc: bed6 file with `chrom start end id score strand` columns type: str optional: False example: example_files/intervals.bed Keyword argument: `fasta_file` doc: Reference genome sequence type: str optional: False example: example_files/hg38_chr22.fa Keyword argument: `gtf_file` doc: file path; Genome annotation GTF file type: str optional: False example: example_files/gencode.v24.annotation_chr22.gtf Keyword argument: `filter_protein_coding` doc: Considering genomic landmarks only for protein coding genes when computing the distances to the nearest genomic landmark. type: str optional: True example: True Keyword argument: `target_file` doc: path to the targets (txt) file type: str optional: True example: example_files/targets.tsv Keyword argument: `use_linecache` doc: if True, use linecache https://docs.python.org/3/library/linecache.html to access bed file rows type: str optional: True -------------------------------------------------------------------------------- Example keyword arguments are: {'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'} Run dataloader on some examples # each dataloader already provides example files which can be used to illustrate its use: DataLoader.example_kwargs {'fasta_file': 'example_files/hg38_chr22.fa', 'filter_protein_coding': True, 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'intervals_file': 'example_files/intervals.bed', 'target_file': 'example_files/targets.tsv'} import os # cd into the source directory os.chdir(DataLoader.source_dir) !tree . \u251c\u2500\u2500 custom_keras_objects.py -> ../template/custom_keras_objects.py \u251c\u2500\u2500 dataloader_files \u2502 \u2514\u2500\u2500 position_transformer.pkl \u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py \u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml \u251c\u2500\u2500 example_files -> ../template/example_files \u251c\u2500\u2500 model_files \u2502 \u2514\u2500\u2500 model.h5 \u251c\u2500\u2500 model.yaml -> ../template/model.yaml \u2514\u2500\u2500 __pycache__ \u251c\u2500\u2500 custom_keras_objects.cpython-36.pyc \u2514\u2500\u2500 dataloader.cpython-36.pyc 4 directories, 8 files dl = DataLoader(**DataLoader.example_kwargs) # could be also done with DataLoader.init_example() # This particular dataloader is of type Dataset # i.e. it implements the __getitem__ method: dl[0].keys() dict_keys(['inputs', 'targets', 'metadata']) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) len(dl) 14 Get the whole dataset whole_data = dl.load_all() 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.24it/s] whole_data.keys() dict_keys(['inputs', 'targets', 'metadata']) whole_data[\"inputs\"][\"seq\"].shape (14, 101, 4) Get the iterator to run predictions it = dl.batch_iter(batch_size=1, shuffle=False, num_workers=0, drop_last=False) next(it)[\"inputs\"][\"seq\"].shape (1, 101, 4) model.predict_on_batch(next(it)[\"inputs\"]) array([[0.1351]], dtype=float32) Pipeline Pipeline object will take the dataloader arguments and run the whole pipeline directly: dataloader arguments --Dataloader--> numpy arrays --Model--> prediction example_kwargs = model.default_dataloader.example_kwargs preds = model.pipeline.predict_example() preds 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.78it/s] array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32) model.pipeline.predict(example_kwargs) 1it [00:01, 1.56s/it] array([0.4208, 0.0005, 0.0005, 0.4208, 0.4208, 0.4208, 0.0005, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208], dtype=float32) next(model.pipeline.predict_generator(example_kwargs, batch_size=2)) array([[0.4208], [0.0005]], dtype=float32) from kipoi.data_utils import numpy_collate numpy_collate_concat(list(model.pipeline.predict_generator(example_kwargs))) array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32) Re-train the Keras model Keras model is stored under the .model attribute. model.model.compile(\"adam\", \"binary_crossentropy\") train_it = dl.batch_train_iter(batch_size=2) # model.model.summary() model.model.fit_generator(train_it, steps_per_epoch=3, epochs=1) Epoch 1/1 3/3 [==============================] - 1s 291ms/step - loss: 1.3592 <keras.callbacks.History at 0x7f95b0095fd0>","title":"Python API"},{"location":"tutorials/python-api/#kipoi-python-api","text":"","title":"Kipoi python API"},{"location":"tutorials/python-api/#quick-start","text":"There are three basic building blocks in kipoi: Source - provides Models and DataLoaders. Model - makes the prediction given the numpy arrays. Dataloader - loads the data from raw files and transforms them into a form that is directly consumable by the Model","title":"Quick start"},{"location":"tutorials/python-api/#list-of-main-commands","text":"Get/list sources - kipoi.list_sources() - kipoi.get_source() List models/dataloaders - kipoi.list_models() - kipoi.list_dataloaders() Get model/dataloader - kipoi.get_model() - kipoi.get_dataloader_factory() Load only model/dataloader description from the yaml file without loading the model kipoi.get_model_descr() kipoi.get_dataloader_descr() Install the dependencies - kipoi.install_model_dependencies() - kipoi.install_dataloader_dependencies() import kipoi","title":"List of main commands"},{"location":"tutorials/python-api/#source","text":"Available sources are specified in the config file located at: ~/.kipoi/config.yaml . Here is an example config file: model_sources: kipoi: # default type: git-lfs # git repository with large file storage (git-lfs) remote_url: git@github.com:kipoi/models.git # git remote local_path: ~/.kipoi/models/ # local storage path gl: type: git-lfs # custom model remote_url: https://i12g-gagneurweb.informatik.tu-muenchen.de/gitlab/gagneurlab/model-zoo.git local_path: /s/project/model-zoo There are three different model sources possible: git-lfs - git repository with source files tracked normally by git and all the binary files like model weights (located in files* directories) are tracked by git-lfs . Requires git-lfs to be installed. git - all the files including weights (not recommended) local - local directory containing models defined in subdirectories For git-lfs source type, larger files tracked by git-lfs will be downloaded into the specified directory local_path only after the model has been requested (when invoking kipoi.get_model() ).","title":"Source"},{"location":"tutorials/python-api/#note","text":"A particular model/dataloader is defined by its source (say kipoi or my_git_models ) and the relative path of the desired model directory from the model source root (say rbp/ ). A directory is considered a model if it contains a model.yaml file. import kipoi import warnings warnings.filterwarnings('ignore') import logging logging.disable(1000) kipoi.list_sources() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source type location local_size n_models n_dataloaders 0 kipoi git-lfs /home/avsec/.kipoi/mo... 1,2G 780 780 s = kipoi.get_source(\"kipoi\") s GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') kipoi.list_models().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags 0 kipoi DeepSEAKeras 0.1 [Author(name='Jian Zh... [Author(name='Lara Ur... This CNN is based on ... keras seq TFBS_DHS_probs True MIT https://doi.org/10.10... ENCODE and Roadmap Ep... https://www.nature.co... [Histone modification... 1 kipoi extended_coda 0.1 [Author(name='Pang We... [Author(name='Johnny ... Single bp resolution ... keras [H3K27AC_subsampled] [H3K27ac] False MIT https://doi.org/10.10... Described in https://... Described in https://... [Histone modification] 2 kipoi DeepCpG_DNA/Hou2016_m... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/mESC1, cpg/mESC2... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 3 kipoi DeepCpG_DNA/Smallwood... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/BS24_1_2I, cpg/B... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation] 4 kipoi DeepCpG_DNA/Hou2016_H... 1.0.4 [Author(name='Christo... [Author(name='Roman K... This is the extractio... keras [dna] [cpg/HepG21, cpg/HepG... True MIT https://doi.org/10.11... scBS-seq and scRRBS-s... Described in https://... [DNA methylation]","title":"Note"},{"location":"tutorials/python-api/#model","text":"Let's choose to use the rbp_eclip/UPF1 model from kipoi MODEL = \"rbp_eclip/UPF1\" NOTE: If you are using python2, use a different model like MaxEntScan/3prime to following this example. # Note. Install all the dependencies for that model: # add --gpu flag to install gpu-compatible dependencies (e.g. installs tensorflow-gpu instead of tensorflow) !kipoi env install {MODEL} model = kipoi.get_model(MODEL)","title":"Model"},{"location":"tutorials/python-api/#available-fields","text":"","title":"Available fields:"},{"location":"tutorials/python-api/#model_1","text":"type args info authors name version tags doc schema inputs targets default_dataloader - loaded dataloader class predict_on_batch() source source_dir pipeline predict() predict_example() predict_generator()","title":"Model"},{"location":"tutorials/python-api/#dataloader","text":"type defined_as args info (same as for the model) output_schema inputs targets metadata source source_dir example_kwargs init_example() batch_iter() batch_train_iter() batch_predict_iter() load_all() model <kipoi.model.KerasModel at 0x7f95b27af2b0> model.type 'keras'","title":"Dataloader"},{"location":"tutorials/python-api/#info","text":"model.info ModelInfo(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='\\'RBP binding model from Avsec et al: \"Modeling positional effects of regulatory sequences with spline transformations increases prediction accuracy of deep neural networks\". \\' ', name=None, version='0.1', license='MIT', tags=['RNA binding'], contributors=[Author(name='Ziga Avsec', github='avsecz', email=None)], cite_as='https://doi.org/10.1093/bioinformatics/btx727', trained_on='RBP occupancy peaks measured by eCLIP-seq (Van Nostrand et al., 2016 - https://doi.org/10.1038/nmeth.3810), https://github.com/gagneurlab/Manuscript_Avsec_Bioinformatics_2017 ', training_procedure='Single task training with ADAM') model.info.version '0.1'","title":"Info"},{"location":"tutorials/python-api/#schema","text":"dict(model.schema.inputs) {'dist_exon_intron': ArraySchema(shape=(1, 10), doc='Distance the nearest exon_intron (splice donor) site transformed with B-splines', name='dist_exon_intron', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_end': ArraySchema(shape=(1, 10), doc='Distance the nearest gene end transformed with B-splines', name='dist_gene_end', special_type=None, associated_metadata=[], column_labels=None), 'dist_gene_start': ArraySchema(shape=(1, 10), doc='Distance the nearest gene start transformed with B-splines', name='dist_gene_start', special_type=None, associated_metadata=[], column_labels=None), 'dist_intron_exon': ArraySchema(shape=(1, 10), doc='Distance the nearest intron_exon (splice acceptor) site transformed with B-splines', name='dist_intron_exon', special_type=None, associated_metadata=[], column_labels=None), 'dist_polya': ArraySchema(shape=(1, 10), doc='Distance the nearest Poly-A site transformed with B-splines', name='dist_polya', special_type=None, associated_metadata=[], column_labels=None), 'dist_start_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest start codon transformed with B-splines', name='dist_start_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_stop_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest stop codon transformed with B-splines', name='dist_stop_codon', special_type=None, associated_metadata=[], column_labels=None), 'dist_tss': ArraySchema(shape=(1, 10), doc='Distance the nearest TSS site transformed with B-splines', name='dist_tss', special_type=None, associated_metadata=[], column_labels=None), 'seq': ArraySchema(shape=(101, 4), doc='One-hot encoded RNA sequence', name='seq', special_type=<ArraySpecialType.DNASeq: 'DNASeq'>, associated_metadata=[], column_labels=None)} model.schema.targets ArraySchema(shape=(1,), doc='Predicted binding strength', name=None, special_type=None, associated_metadata=[], column_labels=None)","title":"Schema"},{"location":"tutorials/python-api/#default-dataloader","text":"Model already has the default dataloder present. To use it, specify model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/UPF1' model.default_dataloader dataloader.SeqDistDataset model.default_dataloader.info Info(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='RBP binding model taking as input 101nt long sequence as well as 8 distances to nearest genomic landmarks - tss, poly-A, exon-intron boundary, intron-exon boundary, start codon, stop codon, gene start, gene end ', name=None, version='0.1', license='MIT', tags=[])","title":"Default dataloader"},{"location":"tutorials/python-api/#predict_on_batch","text":"model.predict_on_batch <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f95b27af2b0>>","title":"Predict_on_batch"},{"location":"tutorials/python-api/#others","text":"# Model source model.source GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/') # model location directory model.source_dir '/home/avsec/.kipoi/models/rbp_eclip/UPF1'","title":"Others"},{"location":"tutorials/python-api/#dataloader_1","text":"DataLoader = kipoi.get_dataloader_factory(MODEL) # same as DataLoader = model.default_dataloader A dataloader will most likely require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. There are several options where the dataloader input keyword arguments are displayed: # Display information about the dataloader print(DataLoader.__doc__) Args: intervals_file: file path; tsv file Assumes bed-like `chrom start end id score strand` format. fasta_file: file path; Genome sequence gtf_file: file path; Genome annotation GTF file. filter_protein_coding: Considering genomic landmarks only for protein coding genes preproc_transformer: file path; tranformer used for pre-processing. target_file: file path; path to the targets batch_size: int # Alternatively the dataloader keyword arguments can be displayed using the function: kipoi.print_dl_kwargs(DataLoader) Keyword argument: `intervals_file` doc: bed6 file with `chrom start end id score strand` columns type: str optional: False example: example_files/intervals.bed Keyword argument: `fasta_file` doc: Reference genome sequence type: str optional: False example: example_files/hg38_chr22.fa Keyword argument: `gtf_file` doc: file path; Genome annotation GTF file type: str optional: False example: example_files/gencode.v24.annotation_chr22.gtf Keyword argument: `filter_protein_coding` doc: Considering genomic landmarks only for protein coding genes when computing the distances to the nearest genomic landmark. type: str optional: True example: True Keyword argument: `target_file` doc: path to the targets (txt) file type: str optional: True example: example_files/targets.tsv Keyword argument: `use_linecache` doc: if True, use linecache https://docs.python.org/3/library/linecache.html to access bed file rows type: str optional: True -------------------------------------------------------------------------------- Example keyword arguments are: {'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'}","title":"DataLoader"},{"location":"tutorials/python-api/#run-dataloader-on-some-examples","text":"# each dataloader already provides example files which can be used to illustrate its use: DataLoader.example_kwargs {'fasta_file': 'example_files/hg38_chr22.fa', 'filter_protein_coding': True, 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'intervals_file': 'example_files/intervals.bed', 'target_file': 'example_files/targets.tsv'} import os # cd into the source directory os.chdir(DataLoader.source_dir) !tree . \u251c\u2500\u2500 custom_keras_objects.py -> ../template/custom_keras_objects.py \u251c\u2500\u2500 dataloader_files \u2502 \u2514\u2500\u2500 position_transformer.pkl \u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py \u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml \u251c\u2500\u2500 example_files -> ../template/example_files \u251c\u2500\u2500 model_files \u2502 \u2514\u2500\u2500 model.h5 \u251c\u2500\u2500 model.yaml -> ../template/model.yaml \u2514\u2500\u2500 __pycache__ \u251c\u2500\u2500 custom_keras_objects.cpython-36.pyc \u2514\u2500\u2500 dataloader.cpython-36.pyc 4 directories, 8 files dl = DataLoader(**DataLoader.example_kwargs) # could be also done with DataLoader.init_example() # This particular dataloader is of type Dataset # i.e. it implements the __getitem__ method: dl[0].keys() dict_keys(['inputs', 'targets', 'metadata']) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) dl[0][\"inputs\"][\"seq\"][:5] array([[0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25], [0.25, 0.25, 0.25, 0.25]], dtype=float32) len(dl) 14","title":"Run dataloader on some examples"},{"location":"tutorials/python-api/#get-the-whole-dataset","text":"whole_data = dl.load_all() 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.24it/s] whole_data.keys() dict_keys(['inputs', 'targets', 'metadata']) whole_data[\"inputs\"][\"seq\"].shape (14, 101, 4)","title":"Get the whole dataset"},{"location":"tutorials/python-api/#get-the-iterator-to-run-predictions","text":"it = dl.batch_iter(batch_size=1, shuffle=False, num_workers=0, drop_last=False) next(it)[\"inputs\"][\"seq\"].shape (1, 101, 4) model.predict_on_batch(next(it)[\"inputs\"]) array([[0.1351]], dtype=float32)","title":"Get the iterator to run predictions"},{"location":"tutorials/python-api/#pipeline","text":"Pipeline object will take the dataloader arguments and run the whole pipeline directly: dataloader arguments --Dataloader--> numpy arrays --Model--> prediction example_kwargs = model.default_dataloader.example_kwargs preds = model.pipeline.predict_example() preds 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 6.78it/s] array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32) model.pipeline.predict(example_kwargs) 1it [00:01, 1.56s/it] array([0.4208, 0.0005, 0.0005, 0.4208, 0.4208, 0.4208, 0.0005, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208, 0.4208], dtype=float32) next(model.pipeline.predict_generator(example_kwargs, batch_size=2)) array([[0.4208], [0.0005]], dtype=float32) from kipoi.data_utils import numpy_collate numpy_collate_concat(list(model.pipeline.predict_generator(example_kwargs))) array([[0.4208], [0.0005], [0.0005], [0.4208], [0.4208], [0.4208], [0.0005], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208], [0.4208]], dtype=float32)","title":"Pipeline"},{"location":"tutorials/python-api/#re-train-the-keras-model","text":"Keras model is stored under the .model attribute. model.model.compile(\"adam\", \"binary_crossentropy\") train_it = dl.batch_train_iter(batch_size=2) # model.model.summary() model.model.fit_generator(train_it, steps_per_epoch=3, epochs=1) Epoch 1/1 3/3 [==============================] - 1s 291ms/step - loss: 1.3592 <keras.callbacks.History at 0x7f95b0095fd0>","title":"Re-train the Keras model"},{"location":"tutorials/tf_binding_models/","text":"Generated from notebooks/tf_binding_models.ipynb Model benchmarking with Kipoi This tutorial will show to to easily benchmark tf-binding models in Kipoi. By providing a unified access to models, it takes the same effort to run a simple PWM scanning model then to run a more complicated model (DeepBind in this example). Load software tools Let's start by loading software for this tutorial: the kipoi model zoo, import kipoi import numpy as np from sklearn.metrics import roc_auc_score Prepare data files Next, we introduce a labeled BED-format interval file and a genome fasta file intervals_file = 'example_data/chr22.101bp.2000_intervals.JUND.HepG2.tsv' fasta_file = 'example_data/hg19_chr22.fa' dl_kwargs = {'intervals_file': intervals_file, 'fasta_file': fasta_file} Let's look at the first few lines in the intervals file !head $intervals_file chr22 20208963 20209064 0 chr22 29673572 29673673 0 chr22 28193720 28193821 0 chr22 43864274 43864375 0 chr22 18261550 18261651 0 chr22 7869409 7869510 0 chr22 49798024 49798125 0 chr22 43088594 43088695 0 chr22 35147671 35147772 0 chr22 49486843 49486944 0 The four columns in this file contain chromosomes, interval start coordinate, interval end coordinate, and the label. This file contains 2000 examples, 1000 positives and 1000 negatives. Let's load the labels from the last column: labels = np.loadtxt(intervals_file, usecols=(3,)) Next, to evaluate the DeepBind model for JUND, we will 1) install software requirements to run the model, 2) load the model, and 3) get model predictions using our intervals and fasta file. Install DeepBind model software requirements deepbind_model_name = \"DeepBind/D00776.005\" kipoi.install_model_requirements(deepbind_model_name) # Use `$ kipoi env install DeepBind/D00776.005 --gpu` from the command-line to install the gpu version of the dependencies Conda dependencies to be installed: ['python=2.7', 'h5py'] Fetching package metadata ........... Solving package specifications: . # All requested packages already installed. # packages in environment at /users/jisraeli/local/anaconda/envs/kipoi: # h5py 2.7.1 py27h2697762_0 pip dependencies to be installed: ['tensorflow==1.4', 'keras==2.1.4'] Requirement already satisfied: tensorflow==1.4 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages Collecting keras==2.1.4 Using cached Keras-2.1.4-py2.py3-none-any.whl Requirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4) Requirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4) Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4) Requirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4) Requirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow==1.4) Installing collected packages: keras Found existing installation: Keras 2.0.4 Uninstalling Keras-2.0.4: Successfully uninstalled Keras-2.0.4 Successfully installed keras-2.1.4 Load DeepBind model deepbind_model = kipoi.get_model(deepbind_model_name) /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend. Get DeepBind predictions deepbind_predictions = deepbind_model.pipeline.predict(dl_kwargs, batch_size=1000) Evaluate DeepBind predictions Let's check the auROC of deepbind predictions: roc_auc_score(labels, deepbind_predictions) 0.808138 Load, run, and evaluate a HOCOMOCO PWM model pwm_model_name = \"pwm_HOCOMOCO/human/JUND\" kipoi.install_model_requirements(pwm_model_name) # Use `$ kipoi env install pwm_HOCOMOCO/human/JUND --gpu` from the command-line to install the gpu version of the dependencies pwm_model = kipoi.get_model(pwm_model_name) pwm_predictions = pwm_model.pipeline.predict(dl_kwargs, batch_size=1000) print(\"PWM auROC:\") roc_auc_score(labels, pwm_predictions) Conda dependencies to be installed: ['python=3.5', 'h5py'] Fetching package metadata ........... Solving package specifications: . # All requested packages already installed. # packages in environment at /users/jisraeli/local/anaconda/envs/kipoi: # h5py 2.7.1 py27h2697762_0 pip dependencies to be installed: ['tensorflow', 'keras==2.0.4'] Requirement already satisfied: tensorflow in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages Collecting keras==2.0.4 Requirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: theano in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/Theano-1.0.1-py2.7.egg (from keras==2.0.4) Requirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.0.4) Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow) Requirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow) Requirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow) Requirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from theano->keras==2.0.4) Installing collected packages: keras Found existing installation: Keras 2.1.4 Uninstalling Keras-2.1.4: Successfully uninstalled Keras-2.1.4 Successfully installed keras-2.0.4 /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. custom_objects=custom_objects) PWM auROC: 0.6431155 In this example, DeepBind's auROC of 80.8% outperforms the HOCOMOCO PWM auROC of 64.3%","title":"Comparing models"},{"location":"tutorials/tf_binding_models/#model-benchmarking-with-kipoi","text":"This tutorial will show to to easily benchmark tf-binding models in Kipoi. By providing a unified access to models, it takes the same effort to run a simple PWM scanning model then to run a more complicated model (DeepBind in this example).","title":"Model benchmarking with Kipoi"},{"location":"tutorials/tf_binding_models/#load-software-tools","text":"Let's start by loading software for this tutorial: the kipoi model zoo, import kipoi import numpy as np from sklearn.metrics import roc_auc_score","title":"Load software tools"},{"location":"tutorials/tf_binding_models/#prepare-data-files","text":"Next, we introduce a labeled BED-format interval file and a genome fasta file intervals_file = 'example_data/chr22.101bp.2000_intervals.JUND.HepG2.tsv' fasta_file = 'example_data/hg19_chr22.fa' dl_kwargs = {'intervals_file': intervals_file, 'fasta_file': fasta_file} Let's look at the first few lines in the intervals file !head $intervals_file chr22 20208963 20209064 0 chr22 29673572 29673673 0 chr22 28193720 28193821 0 chr22 43864274 43864375 0 chr22 18261550 18261651 0 chr22 7869409 7869510 0 chr22 49798024 49798125 0 chr22 43088594 43088695 0 chr22 35147671 35147772 0 chr22 49486843 49486944 0 The four columns in this file contain chromosomes, interval start coordinate, interval end coordinate, and the label. This file contains 2000 examples, 1000 positives and 1000 negatives. Let's load the labels from the last column: labels = np.loadtxt(intervals_file, usecols=(3,)) Next, to evaluate the DeepBind model for JUND, we will 1) install software requirements to run the model, 2) load the model, and 3) get model predictions using our intervals and fasta file.","title":"Prepare data files"},{"location":"tutorials/tf_binding_models/#install-deepbind-model-software-requirements","text":"deepbind_model_name = \"DeepBind/D00776.005\" kipoi.install_model_requirements(deepbind_model_name) # Use `$ kipoi env install DeepBind/D00776.005 --gpu` from the command-line to install the gpu version of the dependencies Conda dependencies to be installed: ['python=2.7', 'h5py'] Fetching package metadata ........... Solving package specifications: . # All requested packages already installed. # packages in environment at /users/jisraeli/local/anaconda/envs/kipoi: # h5py 2.7.1 py27h2697762_0 pip dependencies to be installed: ['tensorflow==1.4', 'keras==2.1.4'] Requirement already satisfied: tensorflow==1.4 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages Collecting keras==2.1.4 Using cached Keras-2.1.4-py2.py3-none-any.whl Requirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4) Requirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4) Requirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4) Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4) Requirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4) Requirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4) Requirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow==1.4) Installing collected packages: keras Found existing installation: Keras 2.0.4 Uninstalling Keras-2.0.4: Successfully uninstalled Keras-2.0.4 Successfully installed keras-2.1.4","title":"Install DeepBind model software requirements"},{"location":"tutorials/tf_binding_models/#load-deepbind-model","text":"deepbind_model = kipoi.get_model(deepbind_model_name) /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters Using TensorFlow backend.","title":"Load DeepBind model"},{"location":"tutorials/tf_binding_models/#get-deepbind-predictions","text":"deepbind_predictions = deepbind_model.pipeline.predict(dl_kwargs, batch_size=1000)","title":"Get DeepBind predictions"},{"location":"tutorials/tf_binding_models/#evaluate-deepbind-predictions","text":"Let's check the auROC of deepbind predictions: roc_auc_score(labels, deepbind_predictions) 0.808138","title":"Evaluate DeepBind predictions"},{"location":"tutorials/tf_binding_models/#load-run-and-evaluate-a-hocomoco-pwm-model","text":"pwm_model_name = \"pwm_HOCOMOCO/human/JUND\" kipoi.install_model_requirements(pwm_model_name) # Use `$ kipoi env install pwm_HOCOMOCO/human/JUND --gpu` from the command-line to install the gpu version of the dependencies pwm_model = kipoi.get_model(pwm_model_name) pwm_predictions = pwm_model.pipeline.predict(dl_kwargs, batch_size=1000) print(\"PWM auROC:\") roc_auc_score(labels, pwm_predictions) Conda dependencies to be installed: ['python=3.5', 'h5py'] Fetching package metadata ........... Solving package specifications: . # All requested packages already installed. # packages in environment at /users/jisraeli/local/anaconda/envs/kipoi: # h5py 2.7.1 py27h2697762_0 pip dependencies to be installed: ['tensorflow', 'keras==2.0.4'] Requirement already satisfied: tensorflow in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages Collecting keras==2.0.4 Requirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow) Requirement already satisfied: theano in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/Theano-1.0.1-py2.7.egg (from keras==2.0.4) Requirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.0.4) Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow) Requirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow) Requirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow) Requirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow) Requirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from theano->keras==2.0.4) Installing collected packages: keras Found existing installation: Keras 2.1.4 Uninstalling Keras-2.1.4: Successfully uninstalled Keras-2.1.4 Successfully installed keras-2.0.4 /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually. custom_objects=custom_objects) PWM auROC: 0.6431155 In this example, DeepBind's auROC of 80.8% outperforms the HOCOMOCO PWM auROC of 64.3%","title":"Load, run, and evaluate a HOCOMOCO PWM model"},{"location":"tutorials/variant_effect_prediction/","text":"Generated from notebooks/variant_effect_prediction.ipynb Variant effect prediction Variant effect prediction offers a simple way to predict effects of SNVs using any model that uses DNA sequence as an input. Many different scoring methods can be chosen, but the principle relies on in-silico mutagenesis. The default input is a VCF and the default output again is a VCF annotated with predictions of variant effects. For details please take a look at the documentation in Postprocessing/Variant effect prediction. This iPython notebook goes through the basic programmatic steps that are needed to preform variant effect prediction. First a variant-centered approach will be taken and secondly overlap-based variant effect prediction will be presented. For details in how this is done programmatically, please refer to the documentation. Variant centered effect prediction Models that accept input .bed files can make use of variant-centered effect prediction. This procedure starts out from the query VCF and generates genomic regions of the length of the model input, centered on the individual variant in the VCF.The model dataloader is then used to produce the model input samples for those regions, which are then mutated according to the alleles in the VCF: First an instance of SnvCenteredRg generates a temporary bed file with regions matching the input sequence length defined in the model.yaml input schema. Then the model dataloader is used to preduce the model input in batches. These chunks of data are then modified by the effect prediction algorithm, the model batch prediction function is triggered for all mutated sequence sets and finally the scoring method is applied. The selected scoring methods compare model predicitons for sequences carrying the reference or alternative allele. Those scoring methods can be Diff for simple subtraction of prediction, Logit for substraction of logit-transformed model predictions, or DeepSEA_effect which is a combination of Diff and Logit , which was published in the Troyanskaya et al. (2015) publication. This ipython notebook assumes that it is executed in an environment in which all dependencies for the following models are installed: DeepSEA/vaariantEffects , HAL , labranchor , MaxEntScan , and rbp are installed, as well as the --vep flag has to be used during installing the dependencies. Now let's start out by loading the DeepSEA model and dataloader factory: import kipoi model_name = \"DeepSEA/variantEffects\" # get the model model = kipoi.get_model(model_name) # get the dataloader factory Dataloader = kipoi.get_dataloader_factory(model_name) Next we will have to define the variants we want to look at, let's look at a sample VCF in chromosome 22: !head -n 40 example_data/clinvar_donor_acceptor_chr22.vcf ##fileformat=VCFv4.0 ##FILTER=<ID=PASS,Description=\"All filters passed\"> ##contig=<ID=chr1,length=249250621> ##contig=<ID=chr2,length=243199373> ##contig=<ID=chr3,length=198022430> ##contig=<ID=chr4,length=191154276> ##contig=<ID=chr5,length=180915260> ##contig=<ID=chr6,length=171115067> ##contig=<ID=chr7,length=159138663> ##contig=<ID=chr8,length=146364022> ##contig=<ID=chr9,length=141213431> ##contig=<ID=chr10,length=135534747> ##contig=<ID=chr11,length=135006516> ##contig=<ID=chr12,length=133851895> ##contig=<ID=chr13,length=115169878> ##contig=<ID=chr14,length=107349540> ##contig=<ID=chr15,length=102531392> ##contig=<ID=chr16,length=90354753> ##contig=<ID=chr17,length=81195210> ##contig=<ID=chr18,length=78077248> ##contig=<ID=chr19,length=59128983> ##contig=<ID=chr20,length=63025520> ##contig=<ID=chr21,length=48129895> ##contig=<ID=chr22,length=51304566> ##contig=<ID=chrX,length=155270560> ##contig=<ID=chrY,length=59373566> ##contig=<ID=chrMT,length=16569> #CHROM POS ID REF ALT QUAL FILTER INFO chr22 41320486 4 G T . . . chr22 31009031 9 T G . . . chr22 43024150 15 C G . . . chr22 43027392 16 A G . . . chr22 37469571 122 C T . . . chr22 37465112 123 C G . . . chr22 37494466 124 G T . . . chr22 18561373 177 G T . . . chr22 51065593 241 C T . . . chr22 51064006 242 C T . . . chr22 51065269 243 G A . . . chr22 30032866 260 G T . . . Now we will define path variable for vcf input and output paths and instantiate a VcfWriter, which will write out the annotated VCF: import kipoi_veff from kipoi_veff import VcfWriter # The input vcf path vcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\" # The output vcf path, based on the input file name out_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\") # The writer object that will output the annotated VCF writer = VcfWriter(model, vcf_path, out_vcf_fpath) Then we need to instantiate an object that can generate variant-centered regions ( SnvCenteredRg objects). This class needs information on the model input sequence length which is extracted automatically within ModelInfoExtractor objects: # Information extraction from dataloader and model model_info = kipoi_veff.ModelInfoExtractor(model, Dataloader) # vcf_to_region will generate a variant-centered regions when presented a VCF record. vcf_to_region = kipoi_veff.SnvCenteredRg(model_info) Now we can define the required dataloader arguments, omitting the intervals_file as this will be replaced by the automatically generated bed file: dataloader_arguments = {\"fasta_file\": \"example_data/hg19_chr22.fa\"} This is the moment to run the variant effect prediction: import kipoi_veff.snv_predict as sp from kipoi_veff.scores import Diff, DeepSEA_effect sp.predict_snvs(model, Dataloader, vcf_path, batch_size = 32, dataloader_args=dataloader_arguments, vcf_to_region=vcf_to_region, evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\"), 'deepsea_effect': DeepSEA_effect(\"mean\")}}, sync_pred_writer=writer) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:34<00:00, 2.46s/it] In the example above we have used the variant scoring method Diff and DeepSEA_effect from kipoi_veff plug-in. As mentioned above variant scoring methods calculate the difference between predictions for reference and alternative, but there is another dimension to this: Models that have the use_rc: true flag set in their model.yaml file (DeepSEA/variantEffects does) will not only be queried with the reference and alternative carrying input sequences, but also with the reverse complement of the the sequences. In order to know of to combine predictions for forward and reverse sequences there is a initialisation flag (here set to: \"mean\" ) for the variant scoring methods. \"mean\" in this case means that after calculating the effect (e.g.: Difference) the average over the difference between the prediction for the forward and for the reverse sequence should be returned. Setting \"mean\" complies with what was used in the Troyanskaya et al. publication. Now let's look at the output: # Let's print out the first 40 lines of the annotated VCF (up to 80 characters per line maximum) with open(\"example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf\") as ifh: for i,l in enumerate(ifh): long_line = \"\" if len(l)>80: long_line = \"...\" print(l[:80].rstrip() +long_line) if i >=40: break ##fileformat=VCFv4.0 ##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:DIFF,Number=.,Type=String,Description... ##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:DEEPSEA_EFFECT,Number=.,Type=String,D... ##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:rID,Number=.,Type=String,Description=... ##FILTER=<ID=PASS,Description=\"All filters passed\"> ##contig=<ID=chr1,length=249250621> ##contig=<ID=chr2,length=243199373> ##contig=<ID=chr3,length=198022430> ##contig=<ID=chr4,length=191154276> ##contig=<ID=chr5,length=180915260> ##contig=<ID=chr6,length=171115067> ##contig=<ID=chr7,length=159138663> ##contig=<ID=chr8,length=146364022> ##contig=<ID=chr9,length=141213431> ##contig=<ID=chr10,length=135534747> ##contig=<ID=chr11,length=135006516> ##contig=<ID=chr12,length=133851895> ##contig=<ID=chr13,length=115169878> ##contig=<ID=chr14,length=107349540> ##contig=<ID=chr15,length=102531392> ##contig=<ID=chr16,length=90354753> ##contig=<ID=chr17,length=81195210> ##contig=<ID=chr18,length=78077248> ##contig=<ID=chr19,length=59128983> ##contig=<ID=chr20,length=63025520> ##contig=<ID=chr21,length=48129895> ##contig=<ID=chr22,length=51304566> ##contig=<ID=chrX,length=155270560> ##contig=<ID=chrY,length=59373566> ##contig=<ID=chrMT,length=16569> #CHROM POS ID REF ALT QUAL FILTER INFO chr22 41320486 4 G T . . KV:kipoi:DeepSEA/variantEffects:DIFF=-0.00285008|-0.000... chr22 31009031 9 T G . . KV:kipoi:DeepSEA/variantEffects:DIFF=-0.02733281|-0.008... chr22 43024150 15 C G . . KV:kipoi:DeepSEA/variantEffects:DIFF=0.01077350|0.0007... chr22 43027392 16 A G . . KV:kipoi:DeepSEA/variantEffects:DIFF=-0.12174654|-0.24... chr22 37469571 122 C T . . KV:kipoi:DeepSEA/variantEffects:DIFF=-0.00654625|0.00... chr22 37465112 123 C G . . KV:kipoi:DeepSEA/variantEffects:DIFF=0.00574893|0.003... chr22 37494466 124 G T . . KV:kipoi:DeepSEA/variantEffects:DIFF=0.01308702|0.001... chr22 18561373 177 G T . . KV:kipoi:DeepSEA/variantEffects:DIFF=-0.00669485|0.00... chr22 51065593 241 C T . . KV:kipoi:DeepSEA/variantEffects:DIFF=0.00409312|0.001... chr22 51064006 242 C T . . KV:kipoi:DeepSEA/variantEffects:DIFF=0.00095593|0.000... This shows that variants have been annotated with variant effect scores. For every different scoring method a different INFO tag was created and the score of every model output is concantenated with the | separator symbol. A legend is given in the header section of the VCF. The name tag indicates with model was used, wich version of it and it displays the scoring function label ( DIFF ) which is derived from the scoring function label defined in the evaluation_function_kwargs dictionary ( 'diff' ). The most comprehensive representation of effect preditions is in the annotated VCF. Kipoi offers a VCF parser class that enables simple parsing of annotated VCFs: from kipoi_veff.parsers import KipoiVCFParser vcf_reader = KipoiVCFParser(\"example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf\") #We can have a look at the different labels which were created in the VCF print(list(vcf_reader.kipoi_parsed_colnames.values())) [('kipoi', 'DeepSEA/variantEffects', 'DIFF'), ('kipoi', 'DeepSEA/variantEffects', 'DEEPSEA_EFFECT'), ('kipoi', 'DeepSEA/variantEffects', 'rID')] We can see that two scores have been saved - 'DEEPSEA_EFFECT' and 'DIFF' . Additionally there is 'rID' which is the region ID - that is the ID given by the dataloader for a genomic region which was overlapped with the variant to get the prediction that is listed in the effect score columns mentioned before. Let's take a look at the VCF entries: import pandas as pd entries = [el for el in vcf_reader] print(pd.DataFrame(entries).head().iloc[:,:7]) variant_id variant_chr variant_pos variant_ref variant_alt \\ 0 4 chr22 41320486 G T 1 9 chr22 31009031 T G 2 15 chr22 43024150 C G 3 16 chr22 43027392 A G 4 122 chr22 37469571 C T KV_DeepSEA/variantEffects_DIFF_8988T_DNase_None_0 \\ 0 -0.002850 1 -0.027333 2 0.010774 3 -0.121747 4 -0.006546 KV_DeepSEA/variantEffects_DIFF_AoSMC_DNase_None_1 0 -0.000094 1 -0.008740 2 0.000702 3 -0.247321 4 0.000784 Another way to access effect predicitons programmatically is to keep all the results in memory and receive them as a dictionary of pandas dataframes: effects = sp.predict_snvs(model, Dataloader, vcf_path, batch_size = 32, dataloader_args=dataloader_arguments, vcf_to_region=vcf_to_region, evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\"), 'deepsea_effect': DeepSEA_effect(\"mean\")}}, return_predictions=True) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:33<00:00, 2.41s/it] For every key in the evaluation_function_kwargs dictionary there is a key in effects and (the equivalent of an additional INFO tag in the VCF). Now let's take a look at the results: for k in effects: print(k) print(effects[k].head().iloc[:,:4]) print(\"-\"*80) diff 8988T_DNase_None AoSMC_DNase_None \\ chr22:41320486:G:['T'] -0.002850 -0.000094 chr22:31009031:T:['G'] -0.027333 -0.008740 chr22:43024150:C:['G'] 0.010773 0.000702 chr22:43027392:A:['G'] -0.121747 -0.247321 chr22:37469571:C:['T'] -0.006546 0.000784 Chorion_DNase_None CLL_DNase_None chr22:41320486:G:['T'] -0.001533 -0.000353 chr22:31009031:T:['G'] -0.003499 -0.008143 chr22:43024150:C:['G'] 0.004689 -0.000609 chr22:43027392:A:['G'] -0.167689 -0.010695 chr22:37469571:C:['T'] -0.000383 -0.000924 -------------------------------------------------------------------------------- deepsea_effect 8988T_DNase_None AoSMC_DNase_None \\ chr22:41320486:G:['T'] 0.000377 9.663903e-07 chr22:31009031:T:['G'] 0.004129 3.683221e-03 chr22:43024150:C:['G'] 0.001582 1.824510e-04 chr22:43027392:A:['G'] 0.068382 2.689577e-01 chr22:37469571:C:['T'] 0.001174 4.173280e-04 Chorion_DNase_None CLL_DNase_None chr22:41320486:G:['T'] 0.000162 0.000040 chr22:31009031:T:['G'] 0.000201 0.002139 chr22:43024150:C:['G'] 0.000322 0.000033 chr22:43027392:A:['G'] 0.133855 0.000773 chr22:37469571:C:['T'] 0.000008 0.000079 -------------------------------------------------------------------------------- We see that for diff and deepsea_effect there is a dataframe with variant identifiers as rows and model output labels as columns. The DeepSEA model predicts 919 tasks simultaneously hence there are 919 columns in the dataframe. Overlap based prediction Models that cannot predict on every region of the genome might not accept a .bed file as dataloader input. An example of such a model is a splicing model. Those models only work in certain regions of the genome. Here variant effect prediction can be executed based on overlaps between the regions generated by the dataloader and the variants defined in the VCF: The procedure is similar to the variant centered effect prediction explained above, but in this case no temporary bed file is generated and the effect prediction is based on all the regions generated by the dataloader which overlap any variant in the VCF. If a region is overlapped by two variants the effect of the two variants is predicted independently. Here the VCF has to be tabixed so that a regional lookup can be performed efficiently, this can be done by using the ensure_tabixed function, the rest remains the same as before: import kipoi from kipoi_veff import VcfWriter from kipoi_veff import ensure_tabixed_vcf # Use a splicing model model_name = \"HAL\" # get the model model = kipoi.get_model(model_name) # get the dataloader factory Dataloader = kipoi.get_dataloader_factory(model_name) # The input vcf path vcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\" # Make sure that the vcf is bgzipped and tabixed, if not then generate the compressed vcf in the same place vcf_path_tbx = ensure_tabixed_vcf(vcf_path) # The output vcf path, based on the input file name out_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\") # The writer object that will output the annotated VCF writer = VcfWriter(model, vcf_path, out_vcf_fpath) This time we don't need an object that generates regions, hence we can directly define the dataloader arguments and run the prediction: from kipoi_veff import predict_snvs from kipoi_veff.scores import Diff dataloader_arguments = {\"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75.filtered_chr22.gtf\", \"fasta_file\": \"example_data/hg19_chr22.fa\"} effects = predict_snvs(model, Dataloader, vcf_path_tbx, batch_size = 32, dataloader_args=dataloader_arguments, evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\")}}, sync_pred_writer=writer, return_predictions=True) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:04<00:00, 150.84it/s] Let's have a look at the VCF: # A slightly convoluted way of printing out the first 40 lines and up to 80 characters per line maximum with open(\"example_data/clinvar_donor_acceptor_chr22HAL.vcf\") as ifh: for i,l in enumerate(ifh): long_line = \"\" if len(l)>80: long_line = \"...\" print(l[:80].rstrip() +long_line) if i >=40: break ##fileformat=VCFv4.0 ##INFO=<ID=KV:kipoi:HAL:DIFF,Number=.,Type=String,Description=\"DIFF SNV effect p... ##INFO=<ID=KV:kipoi:HAL:rID,Number=.,Type=String,Description=\"Range or region id... ##FILTER=<ID=PASS,Description=\"All filters passed\"> ##contig=<ID=chr1,length=249250621> ##contig=<ID=chr2,length=243199373> ##contig=<ID=chr3,length=198022430> ##contig=<ID=chr4,length=191154276> ##contig=<ID=chr5,length=180915260> ##contig=<ID=chr6,length=171115067> ##contig=<ID=chr7,length=159138663> ##contig=<ID=chr8,length=146364022> ##contig=<ID=chr9,length=141213431> ##contig=<ID=chr10,length=135534747> ##contig=<ID=chr11,length=135006516> ##contig=<ID=chr12,length=133851895> ##contig=<ID=chr13,length=115169878> ##contig=<ID=chr14,length=107349540> ##contig=<ID=chr15,length=102531392> ##contig=<ID=chr16,length=90354753> ##contig=<ID=chr17,length=81195210> ##contig=<ID=chr18,length=78077248> ##contig=<ID=chr19,length=59128983> ##contig=<ID=chr20,length=63025520> ##contig=<ID=chr21,length=48129895> ##contig=<ID=chr22,length=51304566> ##contig=<ID=chrX,length=155270560> ##contig=<ID=chrY,length=59373566> ##contig=<ID=chrMT,length=16569> #CHROM POS ID REF ALT QUAL FILTER INFO chr22 17684454 4461 G A . . KV:kipoi:HAL:DIFF=0.10586491;KV:kipoi:HAL:rID=290 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=293 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=299 chr22 17684454 4461 G A . . KV:kipoi:HAL:DIFF=0.10586491;KV:kipoi:HAL:rID=304 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=307 chr22 17684454 4461 G A . . KV:kipoi:HAL:DIFF=0.10586491;KV:kipoi:HAL:rID=313 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=316 chr22 17684454 4461 G A . . KV:kipoi:HAL:DIFF=0.10586491;KV:kipoi:HAL:rID=322 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=325 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=328 chr22 18561370 7302 C T . . KV:kipoi:HAL:DIFF=-1.33794269;KV:kipoi:HAL:rID=824 And the prediction output this time is less helpful because it's the ids that the dataloader created which are displayed as index. In general it is advisable to use the output VCF for more detailed information on which variant was overlapped with which region fo produce a prediction. for k in effects: print(k) print(effects[k].head()) print(\"-\"*80) diff 0 290 0.105865 293 0.000000 299 0.000000 304 0.105865 307 0.000000 -------------------------------------------------------------------------------- Command-line based effect prediction The above command can also conveniently be executed using the command line: import json import os model_name = \"DeepSEA/variantEffects\" dl_args = json.dumps({\"fasta_file\": \"example_data/hg19_chr22.fa\"}) out_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\") scorings = \"diff deepsea_effect\" command = (\"kipoi veff score_variants {model} \" \"--dataloader_args='{dl_args}' \" \"-i {input_vcf} \" \"-o {output_vcf} \" \"-s {scorings}\").format(model=model_name, dl_args=dl_args, input_vcf=vcf_path, output_vcf=out_vcf_fpath, scorings=scorings) # Print out the command: print(command) kipoi veff score_variants DeepSEA/variantEffects --dataloader_args='{\"fasta_file\": \"example_data/hg19_chr22.fa\"}' -i example_data/clinvar_donor_acceptor_chr22.vcf -o example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf -s diff deepsea_effect ! $command \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m Update /nfs/research1/stegle/users/rkreuzhu/.kipoi/models/\u001b[0m Already up-to-date. \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/variantEffects/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/model_files/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/example_files/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m model DeepSEA/variantEffects loaded\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/variantEffects/./**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/model_files/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/example_files/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m dataloader DeepSEA/variantEffects/. loaded\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from /nfs/research1/stegle/users/rkreuzhu/.kipoi/models/DeepSEA/variantEffects/dataloader.py::SeqDataset\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:35<00:00, 2.53s/it] Batch prediction Since the syntax basically doesn't change for different kinds of models a simple for-loop can be written to do what we just did on many models: import kipoi # Run effect predicton models_df = kipoi.list_models() models_substr = [\"HAL\", \"MaxEntScan\", \"labranchor\", \"rbp\"] models_df_subsets = {ms: models_df.loc[models_df[\"model\"].str.contains(ms)] for ms in models_substr} /nfs/research1/stegle/users/rkreuzhu/opt/model-zoo/kipoi/config.py:110: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass 'sort=False'. To retain the current behavior and silence the warning, pass 'sort=True'. return pd.concat(pd_list)[pd_list[0].columns] # Run variant effect prediction using a basic Diff import kipoi from kipoi_veff import ensure_tabixed_vcf import kipoi_veff.snv_predict as sp from kipoi_veff import VcfWriter from kipoi_veff.scores import Diff splicing_dl_args = {\"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75.filtered_chr22.gtf\", \"fasta_file\": \"example_data/hg19_chr22.fa\"} dataloader_args_dict = {\"HAL\": splicing_dl_args, \"labranchor\": splicing_dl_args, \"MaxEntScan\":splicing_dl_args, \"rbp\": {\"fasta_file\": \"example_data/hg19_chr22.fa\", \"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75_chr22.gtf\"} } for ms in models_substr: model_name = models_df_subsets[ms][\"model\"].iloc[0] #kipoi.pipeline.install_model_requirements(model_name) model = kipoi.get_model(model_name) vcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\" vcf_path_tbx = ensure_tabixed_vcf(vcf_path) out_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\") writer = VcfWriter(model, vcf_path, out_vcf_fpath) print(model_name) Dataloader = kipoi.get_dataloader_factory(model_name) dataloader_arguments = dataloader_args_dict[ms] model_info = kipoi_veff.ModelInfoExtractor(model, Dataloader) vcf_to_region = None if ms == \"rbp\": vcf_to_region = kipoi_veff.SnvCenteredRg(model_info) sp.predict_snvs(model, Dataloader, vcf_path_tbx, batch_size = 32, dataloader_args=dataloader_arguments, vcf_to_region=vcf_to_region, evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\")}}, sync_pred_writer=writer) writer.close() HAL 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:04<00:00, 167.58it/s] MaxEntScan/3prime 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:02<00:00, 329.18it/s] Using TensorFlow backend. WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/models.py:287: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer. warnings.warn('Error in loading the saved optimizer ' labranchor 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:05<00:00, 122.95it/s] 2018-07-25 11:37:28,705 [INFO] successfully loaded the dataloader from /nfs/research1/stegle/users/rkreuzhu/.kipoi/models/rbp_eclip/AARS/dataloader.py::SeqDistDataset WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead 2018-07-25 11:37:28,851 [WARNING] From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/models.py:287: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer. warnings.warn('Error in loading the saved optimizer ' 2018-07-25 11:37:30,996 [INFO] successfully loaded the model from model_files/model.h5 2018-07-25 11:37:30,999 [INFO] dataloader.output_schema is compatible with model.schema 2018-07-25 11:37:31,157 [INFO] git-lfs pull -I rbp_eclip/AARS/** rbp_eclip/AARS 2018-07-25 11:37:32,071 [INFO] git-lfs pull -I rbp_eclip/template/** 2018-07-25 11:37:33,219 [INFO] dataloader rbp_eclip/AARS loaded 2018-07-25 11:37:33,252 [INFO] successfully loaded the dataloader from /nfs/research1/stegle/users/rkreuzhu/.kipoi/models/rbp_eclip/AARS/dataloader.py::SeqDistDataset 2018-07-25 11:37:34,411 [INFO] Extracted GTF attributes: ['gene_id', 'gene_name', 'gene_source', 'gene_biotype', 'transcript_id', 'transcript_name', 'transcript_source', 'exon_number', 'exon_id', 'tag', 'protein_id', 'ccds_id'] /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator Imputer from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) 0%| | 0/14 [00:00<?, ?it/s]INFO:2018-07-25 11:37:34,812:genomelake] Running landmark extractors.. 2018-07-25 11:37:34,812 [INFO] Running landmark extractors.. /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/concise/utils/position.py:55: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order. (\"strand\", gtf.strand)]) /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/concise/utils/position.py:62: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order. (\"strand\", gtf.strand)]) INFO:2018-07-25 11:37:34,975:genomelake] Done! 2018-07-25 11:37:34,975 [INFO] Done! 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:03<00:00, 4.05it/s] let's validate that things have worked: ! wc -l example_data/clinvar_donor_acceptor_chr22*.vcf 450 example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf 2035 example_data/clinvar_donor_acceptor_chr22HAL.vcf 794 example_data/clinvar_donor_acceptor_chr22labranchor.vcf 1176 example_data/clinvar_donor_acceptor_chr22MaxEntScan_3prime.vcf 449 example_data/clinvar_donor_acceptor_chr22rbp_eclip_AARS.vcf 447 example_data/clinvar_donor_acceptor_chr22.vcf 5351 total","title":"Variant effect prediction"},{"location":"tutorials/variant_effect_prediction/#variant-effect-prediction","text":"Variant effect prediction offers a simple way to predict effects of SNVs using any model that uses DNA sequence as an input. Many different scoring methods can be chosen, but the principle relies on in-silico mutagenesis. The default input is a VCF and the default output again is a VCF annotated with predictions of variant effects. For details please take a look at the documentation in Postprocessing/Variant effect prediction. This iPython notebook goes through the basic programmatic steps that are needed to preform variant effect prediction. First a variant-centered approach will be taken and secondly overlap-based variant effect prediction will be presented. For details in how this is done programmatically, please refer to the documentation.","title":"Variant effect prediction"},{"location":"tutorials/variant_effect_prediction/#variant-centered-effect-prediction","text":"Models that accept input .bed files can make use of variant-centered effect prediction. This procedure starts out from the query VCF and generates genomic regions of the length of the model input, centered on the individual variant in the VCF.The model dataloader is then used to produce the model input samples for those regions, which are then mutated according to the alleles in the VCF: First an instance of SnvCenteredRg generates a temporary bed file with regions matching the input sequence length defined in the model.yaml input schema. Then the model dataloader is used to preduce the model input in batches. These chunks of data are then modified by the effect prediction algorithm, the model batch prediction function is triggered for all mutated sequence sets and finally the scoring method is applied. The selected scoring methods compare model predicitons for sequences carrying the reference or alternative allele. Those scoring methods can be Diff for simple subtraction of prediction, Logit for substraction of logit-transformed model predictions, or DeepSEA_effect which is a combination of Diff and Logit , which was published in the Troyanskaya et al. (2015) publication. This ipython notebook assumes that it is executed in an environment in which all dependencies for the following models are installed: DeepSEA/vaariantEffects , HAL , labranchor , MaxEntScan , and rbp are installed, as well as the --vep flag has to be used during installing the dependencies. Now let's start out by loading the DeepSEA model and dataloader factory: import kipoi model_name = \"DeepSEA/variantEffects\" # get the model model = kipoi.get_model(model_name) # get the dataloader factory Dataloader = kipoi.get_dataloader_factory(model_name) Next we will have to define the variants we want to look at, let's look at a sample VCF in chromosome 22: !head -n 40 example_data/clinvar_donor_acceptor_chr22.vcf ##fileformat=VCFv4.0 ##FILTER=<ID=PASS,Description=\"All filters passed\"> ##contig=<ID=chr1,length=249250621> ##contig=<ID=chr2,length=243199373> ##contig=<ID=chr3,length=198022430> ##contig=<ID=chr4,length=191154276> ##contig=<ID=chr5,length=180915260> ##contig=<ID=chr6,length=171115067> ##contig=<ID=chr7,length=159138663> ##contig=<ID=chr8,length=146364022> ##contig=<ID=chr9,length=141213431> ##contig=<ID=chr10,length=135534747> ##contig=<ID=chr11,length=135006516> ##contig=<ID=chr12,length=133851895> ##contig=<ID=chr13,length=115169878> ##contig=<ID=chr14,length=107349540> ##contig=<ID=chr15,length=102531392> ##contig=<ID=chr16,length=90354753> ##contig=<ID=chr17,length=81195210> ##contig=<ID=chr18,length=78077248> ##contig=<ID=chr19,length=59128983> ##contig=<ID=chr20,length=63025520> ##contig=<ID=chr21,length=48129895> ##contig=<ID=chr22,length=51304566> ##contig=<ID=chrX,length=155270560> ##contig=<ID=chrY,length=59373566> ##contig=<ID=chrMT,length=16569> #CHROM POS ID REF ALT QUAL FILTER INFO chr22 41320486 4 G T . . . chr22 31009031 9 T G . . . chr22 43024150 15 C G . . . chr22 43027392 16 A G . . . chr22 37469571 122 C T . . . chr22 37465112 123 C G . . . chr22 37494466 124 G T . . . chr22 18561373 177 G T . . . chr22 51065593 241 C T . . . chr22 51064006 242 C T . . . chr22 51065269 243 G A . . . chr22 30032866 260 G T . . . Now we will define path variable for vcf input and output paths and instantiate a VcfWriter, which will write out the annotated VCF: import kipoi_veff from kipoi_veff import VcfWriter # The input vcf path vcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\" # The output vcf path, based on the input file name out_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\") # The writer object that will output the annotated VCF writer = VcfWriter(model, vcf_path, out_vcf_fpath) Then we need to instantiate an object that can generate variant-centered regions ( SnvCenteredRg objects). This class needs information on the model input sequence length which is extracted automatically within ModelInfoExtractor objects: # Information extraction from dataloader and model model_info = kipoi_veff.ModelInfoExtractor(model, Dataloader) # vcf_to_region will generate a variant-centered regions when presented a VCF record. vcf_to_region = kipoi_veff.SnvCenteredRg(model_info) Now we can define the required dataloader arguments, omitting the intervals_file as this will be replaced by the automatically generated bed file: dataloader_arguments = {\"fasta_file\": \"example_data/hg19_chr22.fa\"} This is the moment to run the variant effect prediction: import kipoi_veff.snv_predict as sp from kipoi_veff.scores import Diff, DeepSEA_effect sp.predict_snvs(model, Dataloader, vcf_path, batch_size = 32, dataloader_args=dataloader_arguments, vcf_to_region=vcf_to_region, evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\"), 'deepsea_effect': DeepSEA_effect(\"mean\")}}, sync_pred_writer=writer) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:34<00:00, 2.46s/it] In the example above we have used the variant scoring method Diff and DeepSEA_effect from kipoi_veff plug-in. As mentioned above variant scoring methods calculate the difference between predictions for reference and alternative, but there is another dimension to this: Models that have the use_rc: true flag set in their model.yaml file (DeepSEA/variantEffects does) will not only be queried with the reference and alternative carrying input sequences, but also with the reverse complement of the the sequences. In order to know of to combine predictions for forward and reverse sequences there is a initialisation flag (here set to: \"mean\" ) for the variant scoring methods. \"mean\" in this case means that after calculating the effect (e.g.: Difference) the average over the difference between the prediction for the forward and for the reverse sequence should be returned. Setting \"mean\" complies with what was used in the Troyanskaya et al. publication. Now let's look at the output: # Let's print out the first 40 lines of the annotated VCF (up to 80 characters per line maximum) with open(\"example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf\") as ifh: for i,l in enumerate(ifh): long_line = \"\" if len(l)>80: long_line = \"...\" print(l[:80].rstrip() +long_line) if i >=40: break ##fileformat=VCFv4.0 ##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:DIFF,Number=.,Type=String,Description... ##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:DEEPSEA_EFFECT,Number=.,Type=String,D... ##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:rID,Number=.,Type=String,Description=... ##FILTER=<ID=PASS,Description=\"All filters passed\"> ##contig=<ID=chr1,length=249250621> ##contig=<ID=chr2,length=243199373> ##contig=<ID=chr3,length=198022430> ##contig=<ID=chr4,length=191154276> ##contig=<ID=chr5,length=180915260> ##contig=<ID=chr6,length=171115067> ##contig=<ID=chr7,length=159138663> ##contig=<ID=chr8,length=146364022> ##contig=<ID=chr9,length=141213431> ##contig=<ID=chr10,length=135534747> ##contig=<ID=chr11,length=135006516> ##contig=<ID=chr12,length=133851895> ##contig=<ID=chr13,length=115169878> ##contig=<ID=chr14,length=107349540> ##contig=<ID=chr15,length=102531392> ##contig=<ID=chr16,length=90354753> ##contig=<ID=chr17,length=81195210> ##contig=<ID=chr18,length=78077248> ##contig=<ID=chr19,length=59128983> ##contig=<ID=chr20,length=63025520> ##contig=<ID=chr21,length=48129895> ##contig=<ID=chr22,length=51304566> ##contig=<ID=chrX,length=155270560> ##contig=<ID=chrY,length=59373566> ##contig=<ID=chrMT,length=16569> #CHROM POS ID REF ALT QUAL FILTER INFO chr22 41320486 4 G T . . KV:kipoi:DeepSEA/variantEffects:DIFF=-0.00285008|-0.000... chr22 31009031 9 T G . . KV:kipoi:DeepSEA/variantEffects:DIFF=-0.02733281|-0.008... chr22 43024150 15 C G . . KV:kipoi:DeepSEA/variantEffects:DIFF=0.01077350|0.0007... chr22 43027392 16 A G . . KV:kipoi:DeepSEA/variantEffects:DIFF=-0.12174654|-0.24... chr22 37469571 122 C T . . KV:kipoi:DeepSEA/variantEffects:DIFF=-0.00654625|0.00... chr22 37465112 123 C G . . KV:kipoi:DeepSEA/variantEffects:DIFF=0.00574893|0.003... chr22 37494466 124 G T . . KV:kipoi:DeepSEA/variantEffects:DIFF=0.01308702|0.001... chr22 18561373 177 G T . . KV:kipoi:DeepSEA/variantEffects:DIFF=-0.00669485|0.00... chr22 51065593 241 C T . . KV:kipoi:DeepSEA/variantEffects:DIFF=0.00409312|0.001... chr22 51064006 242 C T . . KV:kipoi:DeepSEA/variantEffects:DIFF=0.00095593|0.000... This shows that variants have been annotated with variant effect scores. For every different scoring method a different INFO tag was created and the score of every model output is concantenated with the | separator symbol. A legend is given in the header section of the VCF. The name tag indicates with model was used, wich version of it and it displays the scoring function label ( DIFF ) which is derived from the scoring function label defined in the evaluation_function_kwargs dictionary ( 'diff' ). The most comprehensive representation of effect preditions is in the annotated VCF. Kipoi offers a VCF parser class that enables simple parsing of annotated VCFs: from kipoi_veff.parsers import KipoiVCFParser vcf_reader = KipoiVCFParser(\"example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf\") #We can have a look at the different labels which were created in the VCF print(list(vcf_reader.kipoi_parsed_colnames.values())) [('kipoi', 'DeepSEA/variantEffects', 'DIFF'), ('kipoi', 'DeepSEA/variantEffects', 'DEEPSEA_EFFECT'), ('kipoi', 'DeepSEA/variantEffects', 'rID')] We can see that two scores have been saved - 'DEEPSEA_EFFECT' and 'DIFF' . Additionally there is 'rID' which is the region ID - that is the ID given by the dataloader for a genomic region which was overlapped with the variant to get the prediction that is listed in the effect score columns mentioned before. Let's take a look at the VCF entries: import pandas as pd entries = [el for el in vcf_reader] print(pd.DataFrame(entries).head().iloc[:,:7]) variant_id variant_chr variant_pos variant_ref variant_alt \\ 0 4 chr22 41320486 G T 1 9 chr22 31009031 T G 2 15 chr22 43024150 C G 3 16 chr22 43027392 A G 4 122 chr22 37469571 C T KV_DeepSEA/variantEffects_DIFF_8988T_DNase_None_0 \\ 0 -0.002850 1 -0.027333 2 0.010774 3 -0.121747 4 -0.006546 KV_DeepSEA/variantEffects_DIFF_AoSMC_DNase_None_1 0 -0.000094 1 -0.008740 2 0.000702 3 -0.247321 4 0.000784 Another way to access effect predicitons programmatically is to keep all the results in memory and receive them as a dictionary of pandas dataframes: effects = sp.predict_snvs(model, Dataloader, vcf_path, batch_size = 32, dataloader_args=dataloader_arguments, vcf_to_region=vcf_to_region, evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\"), 'deepsea_effect': DeepSEA_effect(\"mean\")}}, return_predictions=True) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:33<00:00, 2.41s/it] For every key in the evaluation_function_kwargs dictionary there is a key in effects and (the equivalent of an additional INFO tag in the VCF). Now let's take a look at the results: for k in effects: print(k) print(effects[k].head().iloc[:,:4]) print(\"-\"*80) diff 8988T_DNase_None AoSMC_DNase_None \\ chr22:41320486:G:['T'] -0.002850 -0.000094 chr22:31009031:T:['G'] -0.027333 -0.008740 chr22:43024150:C:['G'] 0.010773 0.000702 chr22:43027392:A:['G'] -0.121747 -0.247321 chr22:37469571:C:['T'] -0.006546 0.000784 Chorion_DNase_None CLL_DNase_None chr22:41320486:G:['T'] -0.001533 -0.000353 chr22:31009031:T:['G'] -0.003499 -0.008143 chr22:43024150:C:['G'] 0.004689 -0.000609 chr22:43027392:A:['G'] -0.167689 -0.010695 chr22:37469571:C:['T'] -0.000383 -0.000924 -------------------------------------------------------------------------------- deepsea_effect 8988T_DNase_None AoSMC_DNase_None \\ chr22:41320486:G:['T'] 0.000377 9.663903e-07 chr22:31009031:T:['G'] 0.004129 3.683221e-03 chr22:43024150:C:['G'] 0.001582 1.824510e-04 chr22:43027392:A:['G'] 0.068382 2.689577e-01 chr22:37469571:C:['T'] 0.001174 4.173280e-04 Chorion_DNase_None CLL_DNase_None chr22:41320486:G:['T'] 0.000162 0.000040 chr22:31009031:T:['G'] 0.000201 0.002139 chr22:43024150:C:['G'] 0.000322 0.000033 chr22:43027392:A:['G'] 0.133855 0.000773 chr22:37469571:C:['T'] 0.000008 0.000079 -------------------------------------------------------------------------------- We see that for diff and deepsea_effect there is a dataframe with variant identifiers as rows and model output labels as columns. The DeepSEA model predicts 919 tasks simultaneously hence there are 919 columns in the dataframe.","title":"Variant centered effect prediction"},{"location":"tutorials/variant_effect_prediction/#overlap-based-prediction","text":"Models that cannot predict on every region of the genome might not accept a .bed file as dataloader input. An example of such a model is a splicing model. Those models only work in certain regions of the genome. Here variant effect prediction can be executed based on overlaps between the regions generated by the dataloader and the variants defined in the VCF: The procedure is similar to the variant centered effect prediction explained above, but in this case no temporary bed file is generated and the effect prediction is based on all the regions generated by the dataloader which overlap any variant in the VCF. If a region is overlapped by two variants the effect of the two variants is predicted independently. Here the VCF has to be tabixed so that a regional lookup can be performed efficiently, this can be done by using the ensure_tabixed function, the rest remains the same as before: import kipoi from kipoi_veff import VcfWriter from kipoi_veff import ensure_tabixed_vcf # Use a splicing model model_name = \"HAL\" # get the model model = kipoi.get_model(model_name) # get the dataloader factory Dataloader = kipoi.get_dataloader_factory(model_name) # The input vcf path vcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\" # Make sure that the vcf is bgzipped and tabixed, if not then generate the compressed vcf in the same place vcf_path_tbx = ensure_tabixed_vcf(vcf_path) # The output vcf path, based on the input file name out_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\") # The writer object that will output the annotated VCF writer = VcfWriter(model, vcf_path, out_vcf_fpath) This time we don't need an object that generates regions, hence we can directly define the dataloader arguments and run the prediction: from kipoi_veff import predict_snvs from kipoi_veff.scores import Diff dataloader_arguments = {\"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75.filtered_chr22.gtf\", \"fasta_file\": \"example_data/hg19_chr22.fa\"} effects = predict_snvs(model, Dataloader, vcf_path_tbx, batch_size = 32, dataloader_args=dataloader_arguments, evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\")}}, sync_pred_writer=writer, return_predictions=True) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:04<00:00, 150.84it/s] Let's have a look at the VCF: # A slightly convoluted way of printing out the first 40 lines and up to 80 characters per line maximum with open(\"example_data/clinvar_donor_acceptor_chr22HAL.vcf\") as ifh: for i,l in enumerate(ifh): long_line = \"\" if len(l)>80: long_line = \"...\" print(l[:80].rstrip() +long_line) if i >=40: break ##fileformat=VCFv4.0 ##INFO=<ID=KV:kipoi:HAL:DIFF,Number=.,Type=String,Description=\"DIFF SNV effect p... ##INFO=<ID=KV:kipoi:HAL:rID,Number=.,Type=String,Description=\"Range or region id... ##FILTER=<ID=PASS,Description=\"All filters passed\"> ##contig=<ID=chr1,length=249250621> ##contig=<ID=chr2,length=243199373> ##contig=<ID=chr3,length=198022430> ##contig=<ID=chr4,length=191154276> ##contig=<ID=chr5,length=180915260> ##contig=<ID=chr6,length=171115067> ##contig=<ID=chr7,length=159138663> ##contig=<ID=chr8,length=146364022> ##contig=<ID=chr9,length=141213431> ##contig=<ID=chr10,length=135534747> ##contig=<ID=chr11,length=135006516> ##contig=<ID=chr12,length=133851895> ##contig=<ID=chr13,length=115169878> ##contig=<ID=chr14,length=107349540> ##contig=<ID=chr15,length=102531392> ##contig=<ID=chr16,length=90354753> ##contig=<ID=chr17,length=81195210> ##contig=<ID=chr18,length=78077248> ##contig=<ID=chr19,length=59128983> ##contig=<ID=chr20,length=63025520> ##contig=<ID=chr21,length=48129895> ##contig=<ID=chr22,length=51304566> ##contig=<ID=chrX,length=155270560> ##contig=<ID=chrY,length=59373566> ##contig=<ID=chrMT,length=16569> #CHROM POS ID REF ALT QUAL FILTER INFO chr22 17684454 4461 G A . . KV:kipoi:HAL:DIFF=0.10586491;KV:kipoi:HAL:rID=290 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=293 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=299 chr22 17684454 4461 G A . . KV:kipoi:HAL:DIFF=0.10586491;KV:kipoi:HAL:rID=304 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=307 chr22 17684454 4461 G A . . KV:kipoi:HAL:DIFF=0.10586491;KV:kipoi:HAL:rID=313 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=316 chr22 17684454 4461 G A . . KV:kipoi:HAL:DIFF=0.10586491;KV:kipoi:HAL:rID=322 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=325 chr22 17669232 7178 T C . . KV:kipoi:HAL:DIFF=0.00000000;KV:kipoi:HAL:rID=328 chr22 18561370 7302 C T . . KV:kipoi:HAL:DIFF=-1.33794269;KV:kipoi:HAL:rID=824 And the prediction output this time is less helpful because it's the ids that the dataloader created which are displayed as index. In general it is advisable to use the output VCF for more detailed information on which variant was overlapped with which region fo produce a prediction. for k in effects: print(k) print(effects[k].head()) print(\"-\"*80) diff 0 290 0.105865 293 0.000000 299 0.000000 304 0.105865 307 0.000000 --------------------------------------------------------------------------------","title":"Overlap based prediction"},{"location":"tutorials/variant_effect_prediction/#command-line-based-effect-prediction","text":"The above command can also conveniently be executed using the command line: import json import os model_name = \"DeepSEA/variantEffects\" dl_args = json.dumps({\"fasta_file\": \"example_data/hg19_chr22.fa\"}) out_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\") scorings = \"diff deepsea_effect\" command = (\"kipoi veff score_variants {model} \" \"--dataloader_args='{dl_args}' \" \"-i {input_vcf} \" \"-o {output_vcf} \" \"-s {scorings}\").format(model=model_name, dl_args=dl_args, input_vcf=vcf_path, output_vcf=out_vcf_fpath, scorings=scorings) # Print out the command: print(command) kipoi veff score_variants DeepSEA/variantEffects --dataloader_args='{\"fasta_file\": \"example_data/hg19_chr22.fa\"}' -i example_data/clinvar_donor_acceptor_chr22.vcf -o example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf -s diff deepsea_effect ! $command \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m Update /nfs/research1/stegle/users/rkreuzhu/.kipoi/models/\u001b[0m Already up-to-date. \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/variantEffects/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/model_files/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/example_files/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m model DeepSEA/variantEffects loaded\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/variantEffects/./**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/model_files/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m git-lfs pull -I DeepSEA/template/example_files/**\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.sources]\u001b[0m dataloader DeepSEA/variantEffects/. loaded\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from /nfs/research1/stegle/users/rkreuzhu/.kipoi/models/DeepSEA/variantEffects/dataloader.py::SeqDataset\u001b[0m \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:35<00:00, 2.53s/it]","title":"Command-line based effect prediction"},{"location":"tutorials/variant_effect_prediction/#batch-prediction","text":"Since the syntax basically doesn't change for different kinds of models a simple for-loop can be written to do what we just did on many models: import kipoi # Run effect predicton models_df = kipoi.list_models() models_substr = [\"HAL\", \"MaxEntScan\", \"labranchor\", \"rbp\"] models_df_subsets = {ms: models_df.loc[models_df[\"model\"].str.contains(ms)] for ms in models_substr} /nfs/research1/stegle/users/rkreuzhu/opt/model-zoo/kipoi/config.py:110: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default. To accept the future behavior, pass 'sort=False'. To retain the current behavior and silence the warning, pass 'sort=True'. return pd.concat(pd_list)[pd_list[0].columns] # Run variant effect prediction using a basic Diff import kipoi from kipoi_veff import ensure_tabixed_vcf import kipoi_veff.snv_predict as sp from kipoi_veff import VcfWriter from kipoi_veff.scores import Diff splicing_dl_args = {\"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75.filtered_chr22.gtf\", \"fasta_file\": \"example_data/hg19_chr22.fa\"} dataloader_args_dict = {\"HAL\": splicing_dl_args, \"labranchor\": splicing_dl_args, \"MaxEntScan\":splicing_dl_args, \"rbp\": {\"fasta_file\": \"example_data/hg19_chr22.fa\", \"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75_chr22.gtf\"} } for ms in models_substr: model_name = models_df_subsets[ms][\"model\"].iloc[0] #kipoi.pipeline.install_model_requirements(model_name) model = kipoi.get_model(model_name) vcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\" vcf_path_tbx = ensure_tabixed_vcf(vcf_path) out_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\") writer = VcfWriter(model, vcf_path, out_vcf_fpath) print(model_name) Dataloader = kipoi.get_dataloader_factory(model_name) dataloader_arguments = dataloader_args_dict[ms] model_info = kipoi_veff.ModelInfoExtractor(model, Dataloader) vcf_to_region = None if ms == \"rbp\": vcf_to_region = kipoi_veff.SnvCenteredRg(model_info) sp.predict_snvs(model, Dataloader, vcf_path_tbx, batch_size = 32, dataloader_args=dataloader_arguments, vcf_to_region=vcf_to_region, evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\")}}, sync_pred_writer=writer) writer.close() HAL 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:04<00:00, 167.58it/s] MaxEntScan/3prime 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:02<00:00, 329.18it/s] Using TensorFlow backend. WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/models.py:287: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer. warnings.warn('Error in loading the saved optimizer ' labranchor 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:05<00:00, 122.95it/s] 2018-07-25 11:37:28,705 [INFO] successfully loaded the dataloader from /nfs/research1/stegle/users/rkreuzhu/.kipoi/models/rbp_eclip/AARS/dataloader.py::SeqDistDataset WARNING:tensorflow:From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead 2018-07-25 11:37:28,851 [WARNING] From /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version. Instructions for updating: keep_dims is deprecated, use keepdims instead /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/keras/models.py:287: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer. warnings.warn('Error in loading the saved optimizer ' 2018-07-25 11:37:30,996 [INFO] successfully loaded the model from model_files/model.h5 2018-07-25 11:37:30,999 [INFO] dataloader.output_schema is compatible with model.schema 2018-07-25 11:37:31,157 [INFO] git-lfs pull -I rbp_eclip/AARS/** rbp_eclip/AARS 2018-07-25 11:37:32,071 [INFO] git-lfs pull -I rbp_eclip/template/** 2018-07-25 11:37:33,219 [INFO] dataloader rbp_eclip/AARS loaded 2018-07-25 11:37:33,252 [INFO] successfully loaded the dataloader from /nfs/research1/stegle/users/rkreuzhu/.kipoi/models/rbp_eclip/AARS/dataloader.py::SeqDistDataset 2018-07-25 11:37:34,411 [INFO] Extracted GTF attributes: ['gene_id', 'gene_name', 'gene_source', 'gene_biotype', 'transcript_id', 'transcript_name', 'transcript_source', 'exon_number', 'exon_id', 'tag', 'protein_id', 'ccds_id'] /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator Imputer from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk. UserWarning) 0%| | 0/14 [00:00<?, ?it/s]INFO:2018-07-25 11:37:34,812:genomelake] Running landmark extractors.. 2018-07-25 11:37:34,812 [INFO] Running landmark extractors.. /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/concise/utils/position.py:55: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order. (\"strand\", gtf.strand)]) /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi_interpret/lib/python3.6/site-packages/concise/utils/position.py:62: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order. (\"strand\", gtf.strand)]) INFO:2018-07-25 11:37:34,975:genomelake] Done! 2018-07-25 11:37:34,975 [INFO] Done! 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:03<00:00, 4.05it/s] let's validate that things have worked: ! wc -l example_data/clinvar_donor_acceptor_chr22*.vcf 450 example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf 2035 example_data/clinvar_donor_acceptor_chr22HAL.vcf 794 example_data/clinvar_donor_acceptor_chr22labranchor.vcf 1176 example_data/clinvar_donor_acceptor_chr22MaxEntScan_3prime.vcf 449 example_data/clinvar_donor_acceptor_chr22rbp_eclip_AARS.vcf 447 example_data/clinvar_donor_acceptor_chr22.vcf 5351 total","title":"Batch prediction"},{"location":"using/01_Getting_started/","text":"Using Kipoi - Getting started Steps 1. Install and setup Kipoi First install Kipoi and select a model you want to work with using the instructions . 2. Use the model You can use the model from: Python Command-line interface R (via the reticulate package) Python - quick start See the ipython notebook tutorials/python-api for additional information and a working example of the API. Here is a list of most useful python commands. import kipoi List all models kipoi.list_models() Get the model Before getting started with models take a short look what a Kipoi model actually is. Kipoi model have to have the following folder structure in which all the relevant files have their assigned places: \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u251c\u2500\u2500 dataloader_files/ #/ files required by the dataloader \u2502 \u251c\u2500\u2500 x_transfomer.pkl \u2502 \u2514\u2500\u2500 y_transfomer.pkl \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 model_files/ #/ files required by the model \u2502 \u251c\u2500\u2500 model.json \u2502 \u2514\u2500\u2500 weights.h5 \u2514\u2500\u2500 example_files/ #/ small example files \u251c\u2500\u2500 features.csv \u2514\u2500\u2500 targets.csv The core file that defines a model is model.yaml , for more details please look at the docs for contributing models . Now let's get started with the model: model = kipoi.get_model(\"rbp_eclip/UPF1\") Aside: get_model and models versus model groups : get_model expects to receive a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide get_model a path to a model group (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, get_model will throw a ValueError . If you want to access a model that is not part of the Kipoi model zoo, please use: ```python model = kipoi.get_model(\"path/to/my/model\", source=\"dir\") If you wish to access the model for a particular commit, use the github permalink: ```python model = kipoi.get_model(\"https://github.com/kipoi/models/tree/7d3ea7800184de414aac16811deba6c8eefef2b6/pwm_HOCOMOCO/human/CTCF\", source='github-permalink') Access information about the model In the following commands a few properties of the model will be shown: model.info # Information about the author: model.default_dataloader # Access the default dataloader model.model # Access the underlying Keras model Test the model Every Kipoi model comes with a small test dataset, which is used to assert its functionality in the nightly tests. This model test function can be accessed by: pred = model.pipeline.predict_example() Get predictions for the raw files For any generation of the model output the dataloader has to be executed first. A dataloader will require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. One way to display the keyword arguments a dataloader accepts is the following: model.default_dataloader.print_args() The output of the function above will tell you which arguments you can use when running the following command. Alternatively, you can view the dataloader arguments on the model's website ( http://kipoi.org/models/<model> ). Let's assume that model.default_dataloder.print_args() has informed us that the dataloader accepts the arguments dataloader_arg1 and targets_file . You can get the model prediction using kipoi.pipeline.predict : model.pipeline.predict({\"dataloader_arg1\": \"...\", \"targets_file\": \"...\"}) Specifically, for the rbp_eclip/UPF1 model, you would run the following: # Make sure we are in the directory containing the example files import os os.chdir(os.path.expanduser('~/.kipoi/models/rbp_eclip/UPF1')) # Run the prediction model.pipeline.predict({'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'}) Setup the dataloader If you don't want to use the model.pipeline.predict function, but you would rather execute the dataloader yourself then you can do the following: dl = model.default_dataloader(dataloader_arg1=\"...\", targets_file=\"...\") This generates a dataloader object dl . Note: kipoi.get_model(\"<mymodel>\").default_dataloader is the same as kipoi.get_dataloader_factory(\"<mymodel>\") Predict for a single batch Data can be requested from the dataloader through its iterator functionality, which can then be used to perform model predictions. # Get the batch iterator it = dl.batch_iter(batch_size=32) # get a single batch single_batch = next(it) It is important to note that the dataloader can also annotate model inputs with additional metadata. The model.pipeline command therefore selects the values in the inputs key as it is shown in the example: # Make a prediction predictions = model.predict_on_batch(single_batch['inputs']) Re-train the model it_train = dl.batch_train_iter(batch_size=32) # will yield tuples (inputs, targets) indefinitely # Since we are using a Keras model, run: model.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10) Command-line interface - quick start Show help For the command line interface the help command should explain most functionality kipoi -h List all models kipoi ls Get information on how the required dataloader keyword arguments kipoi info -i --source kipoi rbp_eclip/UPF1 Run model prediction cd ~/.kipoi/models/rbp_eclip/UPF1/example_files kipoi predict rbp_eclip/UPF1 \\ --dataloader_args='{'intervals_file': 'intervals.bed', 'fasta_file': 'hg38_chr22.fa', 'gtf_file': 'gencode.v24.annotation_chr22.gtf'}' \\ -o '/tmp/rbp_eclip__UPF1.example_pred.tsv' # check the results head '/tmp/rbp_eclip__UPF1.example_pred.tsv' Test a model Test whether a model is defined correctly and whether is execution using the example files is successful. kipoi test ~/.kipoi/models/rbp_eclip/UPF1/example_files Install all model dependencies kipoi env install rbp_eclip/UPF1 Create a new conda environment for the model kipoi env create rbp_eclip/UPF1 source activate kipoi-rbp_eclip__UPF List all Kipoi environments kipoi env list Use source activate <env> or conda activate <env> to activate the environment. Score variants kipoi postproc score_variant rbp_eclip/UPF1 \\ --batch_size=16 \\ -v input.vcf \\ -o output.vcf R - quick start See tutorials/R-api .","title":"Getting started"},{"location":"using/01_Getting_started/#using-kipoi-getting-started","text":"","title":"Using Kipoi - Getting started"},{"location":"using/01_Getting_started/#steps","text":"","title":"Steps"},{"location":"using/01_Getting_started/#1-install-and-setup-kipoi","text":"First install Kipoi and select a model you want to work with using the instructions .","title":"1. Install and setup Kipoi"},{"location":"using/01_Getting_started/#2-use-the-model","text":"You can use the model from: Python Command-line interface R (via the reticulate package)","title":"2. Use the model"},{"location":"using/01_Getting_started/#python-quick-start","text":"See the ipython notebook tutorials/python-api for additional information and a working example of the API. Here is a list of most useful python commands. import kipoi","title":"Python - quick start"},{"location":"using/01_Getting_started/#list-all-models","text":"kipoi.list_models()","title":"List all models"},{"location":"using/01_Getting_started/#get-the-model","text":"Before getting started with models take a short look what a Kipoi model actually is. Kipoi model have to have the following folder structure in which all the relevant files have their assigned places: \u251c\u2500\u2500 dataloader.py # implements the dataloader \u251c\u2500\u2500 dataloader.yaml # describes the dataloader \u251c\u2500\u2500 dataloader_files/ #/ files required by the dataloader \u2502 \u251c\u2500\u2500 x_transfomer.pkl \u2502 \u2514\u2500\u2500 y_transfomer.pkl \u251c\u2500\u2500 model.yaml # describes the model \u251c\u2500\u2500 model_files/ #/ files required by the model \u2502 \u251c\u2500\u2500 model.json \u2502 \u2514\u2500\u2500 weights.h5 \u2514\u2500\u2500 example_files/ #/ small example files \u251c\u2500\u2500 features.csv \u2514\u2500\u2500 targets.csv The core file that defines a model is model.yaml , for more details please look at the docs for contributing models . Now let's get started with the model: model = kipoi.get_model(\"rbp_eclip/UPF1\") Aside: get_model and models versus model groups : get_model expects to receive a path to a directory containing a model.yaml file. This file specifies the underlying model, data loader, and other model attributes. If instead you provide get_model a path to a model group (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/\"), rather than one model (e.g \"lsgkm-SVM/Tfbs/Ap2alpha/Helas3/Sydh_Std\"), or any other directory without a model.yaml file, get_model will throw a ValueError . If you want to access a model that is not part of the Kipoi model zoo, please use: ```python model = kipoi.get_model(\"path/to/my/model\", source=\"dir\") If you wish to access the model for a particular commit, use the github permalink: ```python model = kipoi.get_model(\"https://github.com/kipoi/models/tree/7d3ea7800184de414aac16811deba6c8eefef2b6/pwm_HOCOMOCO/human/CTCF\", source='github-permalink')","title":"Get the model"},{"location":"using/01_Getting_started/#access-information-about-the-model","text":"In the following commands a few properties of the model will be shown: model.info # Information about the author: model.default_dataloader # Access the default dataloader model.model # Access the underlying Keras model","title":"Access information about the model"},{"location":"using/01_Getting_started/#test-the-model","text":"Every Kipoi model comes with a small test dataset, which is used to assert its functionality in the nightly tests. This model test function can be accessed by: pred = model.pipeline.predict_example()","title":"Test the model"},{"location":"using/01_Getting_started/#get-predictions-for-the-raw-files","text":"For any generation of the model output the dataloader has to be executed first. A dataloader will require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. One way to display the keyword arguments a dataloader accepts is the following: model.default_dataloader.print_args() The output of the function above will tell you which arguments you can use when running the following command. Alternatively, you can view the dataloader arguments on the model's website ( http://kipoi.org/models/<model> ). Let's assume that model.default_dataloder.print_args() has informed us that the dataloader accepts the arguments dataloader_arg1 and targets_file . You can get the model prediction using kipoi.pipeline.predict : model.pipeline.predict({\"dataloader_arg1\": \"...\", \"targets_file\": \"...\"}) Specifically, for the rbp_eclip/UPF1 model, you would run the following: # Make sure we are in the directory containing the example files import os os.chdir(os.path.expanduser('~/.kipoi/models/rbp_eclip/UPF1')) # Run the prediction model.pipeline.predict({'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'})","title":"Get predictions for the raw files"},{"location":"using/01_Getting_started/#setup-the-dataloader","text":"If you don't want to use the model.pipeline.predict function, but you would rather execute the dataloader yourself then you can do the following: dl = model.default_dataloader(dataloader_arg1=\"...\", targets_file=\"...\") This generates a dataloader object dl . Note: kipoi.get_model(\"<mymodel>\").default_dataloader is the same as kipoi.get_dataloader_factory(\"<mymodel>\")","title":"Setup the dataloader"},{"location":"using/01_Getting_started/#predict-for-a-single-batch","text":"Data can be requested from the dataloader through its iterator functionality, which can then be used to perform model predictions. # Get the batch iterator it = dl.batch_iter(batch_size=32) # get a single batch single_batch = next(it) It is important to note that the dataloader can also annotate model inputs with additional metadata. The model.pipeline command therefore selects the values in the inputs key as it is shown in the example: # Make a prediction predictions = model.predict_on_batch(single_batch['inputs'])","title":"Predict for a single batch"},{"location":"using/01_Getting_started/#re-train-the-model","text":"it_train = dl.batch_train_iter(batch_size=32) # will yield tuples (inputs, targets) indefinitely # Since we are using a Keras model, run: model.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10)","title":"Re-train the model"},{"location":"using/01_Getting_started/#command-line-interface-quick-start","text":"","title":"Command-line interface - quick start"},{"location":"using/01_Getting_started/#show-help","text":"For the command line interface the help command should explain most functionality kipoi -h","title":"Show help"},{"location":"using/01_Getting_started/#list-all-models_1","text":"kipoi ls","title":"List all models"},{"location":"using/01_Getting_started/#get-information-on-how-the-required-dataloader-keyword-arguments","text":"kipoi info -i --source kipoi rbp_eclip/UPF1","title":"Get information on how the required dataloader keyword arguments"},{"location":"using/01_Getting_started/#run-model-prediction","text":"cd ~/.kipoi/models/rbp_eclip/UPF1/example_files kipoi predict rbp_eclip/UPF1 \\ --dataloader_args='{'intervals_file': 'intervals.bed', 'fasta_file': 'hg38_chr22.fa', 'gtf_file': 'gencode.v24.annotation_chr22.gtf'}' \\ -o '/tmp/rbp_eclip__UPF1.example_pred.tsv' # check the results head '/tmp/rbp_eclip__UPF1.example_pred.tsv'","title":"Run model prediction"},{"location":"using/01_Getting_started/#test-a-model","text":"Test whether a model is defined correctly and whether is execution using the example files is successful. kipoi test ~/.kipoi/models/rbp_eclip/UPF1/example_files","title":"Test a model"},{"location":"using/01_Getting_started/#install-all-model-dependencies","text":"kipoi env install rbp_eclip/UPF1","title":"Install all model dependencies"},{"location":"using/01_Getting_started/#create-a-new-conda-environment-for-the-model","text":"kipoi env create rbp_eclip/UPF1 source activate kipoi-rbp_eclip__UPF","title":"Create a new conda environment for the model"},{"location":"using/01_Getting_started/#list-all-kipoi-environments","text":"kipoi env list Use source activate <env> or conda activate <env> to activate the environment.","title":"List all Kipoi environments"},{"location":"using/01_Getting_started/#score-variants","text":"kipoi postproc score_variant rbp_eclip/UPF1 \\ --batch_size=16 \\ -v input.vcf \\ -o output.vcf","title":"Score variants"},{"location":"using/01_Getting_started/#r-quick-start","text":"See tutorials/R-api .","title":"R - quick start"},{"location":"using/02_Postprocessing/","text":"Postprocessing Kipoi offers a set of postprocessing tools that enable to calculate variant effects, create mutation maps, inspect activation of hidden model layers and to calculate the gradient of layer activation with respect to a given input. Variant effect prediction and mutation map generation is available for all models where the variant_effects parameter in the model.yaml (and dataloader.yaml) is set (see here)[http://kipoi.org/docs/postprocessing/variant_effect_prediction]. Inspection of the activation of hidden model layers and calculation of gradients is available for all deep learning models: Currently supported are Keras, PyTorch and Tensorflow models. For a detailed description and examples of how to use tose features please take a look at: (Variant effect prediction)[# Using variant effect prediction] (Mutation maps)[# Mutation maps] ((Intermediate) layer activation extraction)[# Layer activation extraction] (Gradient calculation)[# Gradient calculation] Using variant effect prediction This chapter describes how to run variant prediction using a model in the zoo either using the python functionality or using the command line. A prerequesite is that the model is compatible with variant effect prediction (see: Variant effect prediction prerequesites for the model.yaml and dataloader.yaml) Variant effect prediction in python Using variant effect prediction within python allows more flexibility in the finegrain details compared to using the command line interface. The core function of variant effect prediction is score_variants , which on the one hand requires a model with its dataloader as well as a valid VCF. The easiest way to run variant effect prediction is the following: from kipoi.postprocessing.variant_effects import score_variants dataloader_arguments = {...} score_variants(model = \"my_model_name\", dl_args = dataloader_arguments, input_vcf = \"path/to/my_vcf.vcf\", output_vcf = \"path/to/my_annotated_vcf.vcf\",) Where model is a kipoi model - replace my_model_name by a valid model name. dataloader_arguments contains all the kwargs that are necessary to run the dataloader. The coordinates in the input_vcf have to match the genome / assembly etc. of the raw input files used by the dataloader. The output of score_variants is an annotated VCF - output_vcf . For more details please look at the detailed function description of score_variants . For details on the different scoring methods please take a look at the detailed explanation of variant effect prediction or the API defintion. The above code will run the dataloader based with dataloader_arguments and try to overlap the input VCF with the sequences generated by the dataloader. If a model dataloader accepts bed files input to control the generated regions, then a temporary bed file with variant-centered regions will be generated. If the dataloader does not offer a bed file input then the inputs generated by the dataloader will automatically be overlapped with the positions in the VCF and only the overlapping regions / variants are tested. For more control over the region generation please use kipoi.postprocessing.variant_effects.predict_snvs function's vcf_to_region argument with SnvCenteredRg , SnvPosRestrictedRg , or None . In the following section features of both functions score_variants and predict_snvs will be explained - please keep in mind that score_variants is a wrapper function around predict_snvs to cover most frequent use cases and reduce complexity of the user's code. Test region generation based on VCFs: Variant-centered effect prediction In the above example the regions were defined by the dataloader arguments, but if the dataloader supports bed file input (see dataloader.yaml definition for variant effect prediction) then the SnvCenteredRg class can generate a temporary bed file using a VCF and information on the required input sequence length from the model.yaml which is extracted by the ModelInfoExtractor instance model_info : from kipoi.postprocessing.variant_effects import SnvCenteredRg vcf_to_region = SnvCenteredRg(model_info) The resulting vcf_to_region object can then be used as the vcf_to_region argument when calling predict_snvs . Restricted variant-centered effect prediction This funcionality is similar to variant-centered effect prediction - the only difference is that this function is designed for models that can't predict on arbitrary regions of the genome, but only in certain regions of the genome. If those regions can be defined in a bed file (further on called 'restriction-bed' file) then this approach can be used. Variant effect prediction will then intersect the VCF with the restriction-bed and generate another bed file that is then passed on to the dataloader. Regions in the restriction-bed file may be larger than the input sequence lenght, in that case the generated seuqence will be centered on the variant position as much as possible - restricted by what is defined in the restrictions-bed file. The SnvPosRestrictedRg class can generate a temporary bed file using a VCF, the restrictions-bed file ( restricted_regions_fpath in the example below) and information on the required input sequence length from the model.yaml which is extracted by the ModelInfoExtractor instance model_info : from kipoi.postprocessing.variant_effects import SnvPosRestrictedRg import pybedtools as pb pbd = pb.BedTool(restricted_regions_fpath) vcf_to_region = SnvPosRestrictedRg(model_info, pbd) The resulting vcf_to_region object can then be used as the vcf_to_region argument when calling predict_snvs . Scoring functions Scoring functions perform calculations on the model predictions for the reference and alternative sequences. Default scoring functions are: Logit , LogitAlt , LogitRef , Diff , DeepSEA_effect . These functions are described in more detail in the variant effect prediction pages. These and custom scoring functions can be used in the score_variants function by setting the scores as a list of strings, for example: [\"logit\", \"diff\"] . This list can contain strings of the implemented scoring functions ( \"diff\" , \"ref\" , \"alt\" , \"logit\" , \"logit_ref\" , \"logit_alt\" , \"deepsea_effect\" ) or callables that are inherited from RCScore . Fine-tuning scoring functions The default scoring functions ( Logit , LogitAlt , LogitRef , Diff , DeepSEA_effect ) offer different options on how the forward and the reverse complement sequences are merged together. They have an rc_merging argument which can be \"min\" , \"max\" , \"mean\" , \"median\" or \"absmax\" . So when using the score_variants function the maximum between forward and reverse complement sequences for the alt-ref prediction differences should be returned, then the scores_kargs argument would be: [{\"rc_merging\": \"max\"}] and scores would be [\"diff\"] . The default rc_merging value is \"mean\" . Saving mutated sequence sets to a file A specialised feature of the predict_snvs function that is only available when using the python functions is to save the mutated sequence sets in a file. This can be useful for quality control or if a non-deeplearning model outside the model zoo should be run using the same data. For those cases instances of the SyncHdf5SeqWriter can be used. If they are passed to predict_snvs as the argument generated_seq_writer then the respective sequences are written to a file. Keep in mind that when defining a generated_seq_writer then no actual effect prediction is performed, but only the reference/alternative sequence sets are generated and saved. Return predictions By default effect predicions are not kept in memory, but only written to the output VCF to ensure a low memory profile. By setting the parameter return_predictions = True in predict_snvs or in score_variants the effect predictions are accumulated in memory and the results are returned as a dictionary of DataFrames, where the keys are the labels of the used scoring functions and the DataFrames have the shape (number effect predictions, number of output tasks of the model). Using the command line interface Similar to kipoi predict variant effect prediction can be run by executing: kipoi postproc score_variants my_model_name \\ --dataloader_args '{...}' \\ --vcf_path path/to/my_vcf.vcf \\ --out_vcf_fpath path/to/my_annotated_vcf.vcf Exceptions are that if the dataloader of the model allows the definition of a bed input file, then the respective field in the --dataloader_args JSON will be replaced by a bed file that consists in regions that are centered on the variant position. That is, if in the dataloader.yaml file of the respective model the bed_input flag is set then the respective argument in the --dataloader_args will be overwritten. When using variant effect prediction from the command line and using --source dir , keep in mind that whatever the path is that you put where my_model_name stands in the above command is treated as your model name. Since the annotated VCF INFO tags contain the model name as an identifier, executing kipoi postproc score_variants ./ --source dir ... will result in an annotated VCF with the model name \".\", which is most probably not desired. For those cases kipoi postproc score_variants ... should be executed in at least one directory level higher than the one where the model.yaml file lies. Then the command will look similar to this kipoi postproc score_variants ./my_model --source dir ... and the annotated VCF INFO tags will contain './my_model'. Scoring functions Scoring functions perform calculations on the model predictions for the reference and alternative sequences. Default scoring functions are: logit , logit_alt , logit_ref , diff , deepsea_effect . These functions are described in more detail in the variant effect prediction pages. Given a model is compatible with said scoring functions one or more of those can be selected by using the --scoring argument, e.g.: --scoring diff logit . The model.yaml file defines which scoring functions are available for a model, with the exception that the diff scoring function is available for all models. In the model.yaml also additional custom scoring functions can be defined, for details on please see the variant effect prediction pages. The labels by which the different scoring functions are made available can also be defined in the model.yaml file using the name tag. Fine-tuning scoring functions Scoring functions may have or may even require arguments at instantiation. Those arguments can be passed as JSON dictionaries to scoring functions by using the --scoring_kwargs argument. If --scoring_kwargs is used then for every label set in --scoring there must be a --scoring_kwargs JSON in the exact same order. If the degault values should be used or no arguments are required then an empty dictionary ( {} ) can be used. For example: --scoring diff my_scr --scoring_kwargs '{}' '{my_arg:2}' will use diff with the default parameters and will instantiate my_scr(my_arg=2) . The default scoring functions ( logit , logit_alt , logit_ref , diff , deepsea_effect ) offer different options on how the forward and the reverse complement sequences are merged together. They have an rc_merging argument which can be \"min\" , \"max\" , \"mean\" , \"median\" or \"absmax\" . So if the maximum between forward and reverse complement sequences for the alt-ref prediction differences should be returned, then the command would be: --scoring diff --scoring_kwargs '{rc_merging:\"max\"}' . By default rc_merging is set to \"mean\" . Mutation maps Mutation maps are related to variant effect prediction discussed above. Mutation maps are the application of SNV variant effect prediction on every position of the input sequence with all three alternative alleles. Therefore mutation maps can only be generated for models that support variant effect prediction. Mutation maps can be used to give an overview over the effect scores in a selected region. This region may be centered on a variant of interest or any other region in the genome for which the model can produce a prediction. It is therefore complementary to the variant effect prediction functionality and is intended for use with less variants / regions of interest as the variant effect prediction itself. Typically a mutation map should be calculated for only a handful of regions or query variants (that each tag a region), because for every region many effect predictions have to calculated resulting in calculation time and memory requirements: For a single query variant / query region N = model_sequence_length * 3 * model_output_tasks * effect_scoring_functions effect predictions have to be performed. The workflow is desinged in two steps: In a first step the aforementioned calculation is performed and results are stored in an hdf5 file with standardised format. These files can then be imported into the visualisation part of mutation maps. Both steps are available in python, R as well as the command line. Calculating mutation maps Python / R API The core element of mutation maps is the MutationMap class that is instantiated with a Kipoi model object, a dataloader object and the dataloader arguments: import kipoi from kipoi.postprocessing.variant_effects import MutationMap model = kipoi.get_model(<model_name>) dataloader = model.default_dataloader dataloader_arguments = {...} mm = MutationMap(model, dataloader, dataloader_arguments) MutationMap instances have the following methods to calculate mutation maps for the given query regions / variants: query_region , query_bed , query_vcf . All those functions return an instance of MutationMapPlotter, which can be stored as a hdf5 file or directly be used for generating mutation map plots. query_region The query_region command can be used to generate a mutation map for a selected genomic region: mmp = mm.query_region(\"chr22\", 25346, 25357) The query region has to be transformed to match model input sequence length as well as it has to lie in a genomic region for which the model can produce prediction. All this is taken care of automatically just like in the score_variants function of kipoi.postprocessing.variant_effects . For more details please see below . query_bed The query_bed command can be used to generate mutation maps for genomic regions defined in the bed file: mmp = mm.query_region(\"path/to/my/file.bed\") The query regions have to be transformed to match model input sequence length as well as they hasveto lie in a genomic region for which the model can produce prediction. All this is taken care of automatically just like in the score_variants function of kipoi.postprocessing.variant_effects . For more details please see below . query_vcf The query_vcf command can be used to generate mutation maps based on variants defined in the vcf file: mmp = mm.query_vcf(\"path/to/my/file.vcf\") The regions for query variants are generated analogously to the score_variants function of kipoi.postprocessing.variant_effects . For more details please see below . MutationMapPlotter Instances of the MutationMapPlotter class are generated by the query_* methods of the MutationMap class. They contain all the effect predictions plus some additional meta data necessary to produce mutation map plots. Those objects can be stored in hdf5 files. For plotting the plot_mutmap function can be used. The required arguments select the input sequence by a numerical index ( input_entry ), the name of the DNA sequence model input ( model_seq_input ), the name of the scoring function for which the results should be displayed ( scoring_key ) and finally the model output task ( model_output ). A combination of those four values directly link to one set of mutation map predictions. input_entry input_entry is a numerical index indicating which set of input data should be used. This relates back to how query regions are turned into model input data, see below . Since this link depends model sequence length as well as whether the model can only predict for a restricted subset of he genome, the meaning of an index value may vary from model to model. For a combination of models with highly different model input specifications it is therefore advisable to only query a single variant or region in order to avoid confusion. model_seq_input Many models will only have a single model input key, so this parameter might seem superfluous, but in general a model can have multiple DNA sequence inputs which are all being tested for variant effects. scoring_key The scoring key is one of the labels passed to the query_* function in the scores argument. model_output model_output is a model output task label. Additional to the required arguments the plots can be generated for a subset of the model input sequence using the limit_region_genomic . The plot can be generated with reverse-complementation of the sequence by using rc_plot . There are additional features available for the python/R API which are described in the method definition, some of which are also used in the mutation_map.ipynb . The CLI In the CLI mutation maps can be calculated for bed files or for VCF files. Both file formats are accepted by the --regions_file argument of the CLI command: kipoi postproc create_mutation_map <my_model_name> --dataloader_args '{...}' --regions_file path/to/my/file.vcf --output path/to/the/results/file.hdf5 Plotting Plotting in the command line works analogously as using the python API: kipoi postproc plot_mutation_map --input_file path/to/the/results/file.hdf5 --input_entry 0 --model_seq_input seq --scoring_key diff --model_output my_model_task --output path/to/the/plot/file.png The meaning of the parameters is identical to the ones in the python API mentioned above. The plotting functionality in the CLI is limited to zooming into genomic region and reverse-complementation of sequences. For examaples please take a look at the mutation_map.ipynb . Transformation of queries to model input In order to perform a query on a model the query input must be transformed into genomic regions compatible with the model. Similar to variant effect prediction using the score_variants the automatically chosen region generation method will be chosen based on whether a dataloader offers a bed file input for postprocessing. dataloader.yaml > postprocessing > variant_effects > bed_input . By setting this value the mutation map method will automatically generate a temporary bed input file requesting model input for genomic regions. The path of this temporary bed file is then passed on to the dataloader by resetting the respective argument in the datalaoder_arguments . For some models it is not possible to freely define the genomic region for which model input data should be generated - in that case the dataloader.yaml does not have the dataloader.yaml > postprocessing > variant_effects > bed_input set. In those cases the datalaoder is executed without modifying the dataloader_arguments . The metadata generated alongside the model input is then used to identify model input that overlaps a query region / query variant. For cases when genomic regions can be defined freely for a model, the input samples will always have to generated matching the model input sequence length. This means that for query variants a region of the length of the model input will be centered on the query variant position. For query regions (e.g.: bed input file) every region is overlapped with windows of length of the model input. The first of those regions will start at the same position as the selected query region. Regions of the length of the model input sequence length will then be generated consecutively in order to cover the full region defined by the respective query region - see this schematic: In the top bit of this schematic on can see the case in which the dataloader accepts a bed file as an input to generate model input data. This also requires the correct setup of the dataloader.yaml in postprocessing > variant_effects > bed_input as described in more detail (here)[../postprocessing/variant_effect_prediction]. When the dataloader doesn't support bed input files for region defintion then all the regions generated by the dataloader will be overlapped with the query regions and any overlapping data generated from the dataloader will be used for mutation maps. The same as for bed query files holds true for VCF query files. As mentioned above all of those model input sequences are the subjected to variant effect prediction for every base and ever possible allele. The integer numbers displayed in the green or orange boxes are te order in which model input data is processed by the mutation map calculation algorithm. The numbers represent the index by which the predictions can then be accessed for plotting ( input_entry in plot_mutmap method of MutationMapPlotter or --input_entry CLI argument). Layer activation extraction Similar to model prediction on a batch of input data it is possible to predict the activation of intermediate layers. The layer can be selected using the layer argument and the value has to be the identifier of the respective layer. python / R API In python and R intermediate (hidden) layer activation can be calculated using the predict_activation_on_batch method of Kipoi models implementing the LayerActivationMixin . All deep learning frameworks supported by Kipoi implement this mixin: import kipoi # Get the model model = kipoi.get_model(\"my_model_name\") dataloader_arguments = {...} # Get the dataloader iterator dl_iterator = model.default_dataloader(**dataloader_arguments).iterator() # Get layer activation res = model.predict_activation_on_batch(dl_iterator.__next__(), layer = \"layer_of_interest\") CLI The prediction of layer activation using the command line works by adding the --layer argument to the prediction command: ``` kipoi predict my_model_name --dataloader_arguments '{...}' --layer 'layer_of_interest' --output path/to/output/file. Gradient calculation The calculation of gradients of the activation of a layer with respect to input data has proven to be a powerful tool for model and data interpretation. This approach can explain which portions of the model input were important for a given model output or for a given activation of a hidden layer. This feature is known as saliency maps . A successor, which relates gradients to the respective model input is the grad input_ score, which is - as its name implies - a multiplication of the first order gradient on the input data with the input data itself. _grad input for genomics models is seen as an alterntiave / complement to perturbation-based approaches like variant effect prediction. The advantage of gradient-based approaches being that in a single calculation step the importance of all bases in an input DNA sqeuence is established at the same time. Additionally gradient based approaches are not limited to Kipoi models that support variant effect prediction, and therefore are usable on all deep learning models. Finally gradient-based approaches highlight the input feature importance for all model inputs at once, also taking interactions between input into account, which cannot be done in variant effect prediction (perturbation-based approaches). Even though deep learning frameworks come with automatic gradient calculation as one of their core features, the accessibility of this functionality to the user is generally not straight forward and may vary from framework version to version. Kipoi comes with a consistent API to extract gradients for the Keras, PyTorch, and Tensorflow frameworks. The API takes advantage of the individual implementations of the automatic differentiation algorithm and in the frameworks, supporting multiple versions of those frameworks. To offer a consistent design the gradient can only be calculated with respect to model input - not with respect to the input of a hidden layer. What is essentially needed in terms of parameters to perform the calculation is: the Kipoi model instance, the Kipoi model input data (generated by a dataloader), an identifier of the (hidden) layer from which (backwards) the gradient should be calculated ( layer ). The gradient can in most frameworks only be calculated on a scalar. To ensure compatibility Kipoi therefore uses averaging functions ( avg_func ) that average across the outputs of a layer. Amongst those the summation ( sum ), maximum ( max ), minimum ( min ) and the maximum of absolute values ( absmax ) are available. To allow more fine-grain control the user can subset outputs ( filter_idx ) of a selected layer that are then passed to the averaging function. The filter_idx arguments takes integers, slice objects, and any tuple consisting in combinations of integers and slice objects. What is passed to filter_idx will be used to select from the model layer output, it must therefore be compatible with the layer output shape. To simplify the process for users who want to calculate the gradient starting from the model output (final layer) the final_layer argument can be used instead of explicitely selecting the layer by its ids with layer . This functionality has to be used with caution, as models may contain post-processing layers or non-linear activation functions as their final layer. Gradient calculation with respect to those values is generally not recommended. To overcome the problem of the last layer being a non-linear activation function another argument pre_nonlinearity was imlemented that tries to traverse the model graph from the selected layer towards model input in case the selected layer is a non-linear activation function. The pre_nonlinearity cannot be implemented for all models and using it may raise Exceptions in case the selected Kipoi model cannot support that feature. It is generally advisable to select a layer explicitely by using the ( layer ) to ensure that the desired calculations are being performed. Gradient visualisation To complete the model gradient calculation Kipoi comes with gradient visualisation tools, that can distinguish between 1-hot encoded DNA sequence model input and other model input. The default plotting function for 1-hot encoded DNA sequence is a seq-logo plot displaying the prediction output as letter height. In case type of a model input is unknown or it is not DNA sequence a heatmap will be generated. In any case the visualisation tools are only available for one- or two-dimensional input data. An additional feature is the generation of a bedgraph file instead of a heatmap/seqlogo plot. In the general case a seq_dim parameter has to be set defining the dimension in which every entry corresponds to one genomic position in a consecutive order. The GradPlotter class which is repsonsible for gradient visualisations has a plot function that has only one required argument sample . sample is the integer index of dataloader sample for which the plot should be produced. Python / R API All of the arguments mentioned above are available within the grad_input method of Kipoi models that implement GradientMixin . Namely those are KerasModel , PyTorchModel , and TensorflowModel . For an example please refer to the grad_input.ipynb . The returned output from the grad_input matches the structre of the inputs entry of the data batch generated by the model dataloader. CLI The command line interface is designed in analogy with the model prediction functionality plus the arguments exlpained above. The output can be stored in a tsv or an hdf5 file. In case a hdf5 output is chosen the gradient visualisation methods that come with Kipoi can be used. For an example please refer to the grad_input.ipynb .","title":"Postprocessing"},{"location":"using/02_Postprocessing/#postprocessing","text":"Kipoi offers a set of postprocessing tools that enable to calculate variant effects, create mutation maps, inspect activation of hidden model layers and to calculate the gradient of layer activation with respect to a given input. Variant effect prediction and mutation map generation is available for all models where the variant_effects parameter in the model.yaml (and dataloader.yaml) is set (see here)[http://kipoi.org/docs/postprocessing/variant_effect_prediction]. Inspection of the activation of hidden model layers and calculation of gradients is available for all deep learning models: Currently supported are Keras, PyTorch and Tensorflow models. For a detailed description and examples of how to use tose features please take a look at: (Variant effect prediction)[# Using variant effect prediction] (Mutation maps)[# Mutation maps] ((Intermediate) layer activation extraction)[# Layer activation extraction] (Gradient calculation)[# Gradient calculation]","title":"Postprocessing"},{"location":"using/02_Postprocessing/#using-variant-effect-prediction","text":"This chapter describes how to run variant prediction using a model in the zoo either using the python functionality or using the command line. A prerequesite is that the model is compatible with variant effect prediction (see: Variant effect prediction prerequesites for the model.yaml and dataloader.yaml)","title":"Using variant effect prediction"},{"location":"using/02_Postprocessing/#variant-effect-prediction-in-python","text":"Using variant effect prediction within python allows more flexibility in the finegrain details compared to using the command line interface. The core function of variant effect prediction is score_variants , which on the one hand requires a model with its dataloader as well as a valid VCF. The easiest way to run variant effect prediction is the following: from kipoi.postprocessing.variant_effects import score_variants dataloader_arguments = {...} score_variants(model = \"my_model_name\", dl_args = dataloader_arguments, input_vcf = \"path/to/my_vcf.vcf\", output_vcf = \"path/to/my_annotated_vcf.vcf\",) Where model is a kipoi model - replace my_model_name by a valid model name. dataloader_arguments contains all the kwargs that are necessary to run the dataloader. The coordinates in the input_vcf have to match the genome / assembly etc. of the raw input files used by the dataloader. The output of score_variants is an annotated VCF - output_vcf . For more details please look at the detailed function description of score_variants . For details on the different scoring methods please take a look at the detailed explanation of variant effect prediction or the API defintion. The above code will run the dataloader based with dataloader_arguments and try to overlap the input VCF with the sequences generated by the dataloader. If a model dataloader accepts bed files input to control the generated regions, then a temporary bed file with variant-centered regions will be generated. If the dataloader does not offer a bed file input then the inputs generated by the dataloader will automatically be overlapped with the positions in the VCF and only the overlapping regions / variants are tested. For more control over the region generation please use kipoi.postprocessing.variant_effects.predict_snvs function's vcf_to_region argument with SnvCenteredRg , SnvPosRestrictedRg , or None . In the following section features of both functions score_variants and predict_snvs will be explained - please keep in mind that score_variants is a wrapper function around predict_snvs to cover most frequent use cases and reduce complexity of the user's code.","title":"Variant effect prediction in python"},{"location":"using/02_Postprocessing/#test-region-generation-based-on-vcfs","text":"","title":"Test region generation based on VCFs:"},{"location":"using/02_Postprocessing/#variant-centered-effect-prediction","text":"In the above example the regions were defined by the dataloader arguments, but if the dataloader supports bed file input (see dataloader.yaml definition for variant effect prediction) then the SnvCenteredRg class can generate a temporary bed file using a VCF and information on the required input sequence length from the model.yaml which is extracted by the ModelInfoExtractor instance model_info : from kipoi.postprocessing.variant_effects import SnvCenteredRg vcf_to_region = SnvCenteredRg(model_info) The resulting vcf_to_region object can then be used as the vcf_to_region argument when calling predict_snvs .","title":"Variant-centered effect prediction"},{"location":"using/02_Postprocessing/#restricted-variant-centered-effect-prediction","text":"This funcionality is similar to variant-centered effect prediction - the only difference is that this function is designed for models that can't predict on arbitrary regions of the genome, but only in certain regions of the genome. If those regions can be defined in a bed file (further on called 'restriction-bed' file) then this approach can be used. Variant effect prediction will then intersect the VCF with the restriction-bed and generate another bed file that is then passed on to the dataloader. Regions in the restriction-bed file may be larger than the input sequence lenght, in that case the generated seuqence will be centered on the variant position as much as possible - restricted by what is defined in the restrictions-bed file. The SnvPosRestrictedRg class can generate a temporary bed file using a VCF, the restrictions-bed file ( restricted_regions_fpath in the example below) and information on the required input sequence length from the model.yaml which is extracted by the ModelInfoExtractor instance model_info : from kipoi.postprocessing.variant_effects import SnvPosRestrictedRg import pybedtools as pb pbd = pb.BedTool(restricted_regions_fpath) vcf_to_region = SnvPosRestrictedRg(model_info, pbd) The resulting vcf_to_region object can then be used as the vcf_to_region argument when calling predict_snvs .","title":"Restricted variant-centered effect prediction"},{"location":"using/02_Postprocessing/#scoring-functions","text":"Scoring functions perform calculations on the model predictions for the reference and alternative sequences. Default scoring functions are: Logit , LogitAlt , LogitRef , Diff , DeepSEA_effect . These functions are described in more detail in the variant effect prediction pages. These and custom scoring functions can be used in the score_variants function by setting the scores as a list of strings, for example: [\"logit\", \"diff\"] . This list can contain strings of the implemented scoring functions ( \"diff\" , \"ref\" , \"alt\" , \"logit\" , \"logit_ref\" , \"logit_alt\" , \"deepsea_effect\" ) or callables that are inherited from RCScore .","title":"Scoring functions"},{"location":"using/02_Postprocessing/#fine-tuning-scoring-functions","text":"The default scoring functions ( Logit , LogitAlt , LogitRef , Diff , DeepSEA_effect ) offer different options on how the forward and the reverse complement sequences are merged together. They have an rc_merging argument which can be \"min\" , \"max\" , \"mean\" , \"median\" or \"absmax\" . So when using the score_variants function the maximum between forward and reverse complement sequences for the alt-ref prediction differences should be returned, then the scores_kargs argument would be: [{\"rc_merging\": \"max\"}] and scores would be [\"diff\"] . The default rc_merging value is \"mean\" .","title":"Fine-tuning scoring functions"},{"location":"using/02_Postprocessing/#saving-mutated-sequence-sets-to-a-file","text":"A specialised feature of the predict_snvs function that is only available when using the python functions is to save the mutated sequence sets in a file. This can be useful for quality control or if a non-deeplearning model outside the model zoo should be run using the same data. For those cases instances of the SyncHdf5SeqWriter can be used. If they are passed to predict_snvs as the argument generated_seq_writer then the respective sequences are written to a file. Keep in mind that when defining a generated_seq_writer then no actual effect prediction is performed, but only the reference/alternative sequence sets are generated and saved.","title":"Saving mutated sequence sets to a file"},{"location":"using/02_Postprocessing/#return-predictions","text":"By default effect predicions are not kept in memory, but only written to the output VCF to ensure a low memory profile. By setting the parameter return_predictions = True in predict_snvs or in score_variants the effect predictions are accumulated in memory and the results are returned as a dictionary of DataFrames, where the keys are the labels of the used scoring functions and the DataFrames have the shape (number effect predictions, number of output tasks of the model).","title":"Return predictions"},{"location":"using/02_Postprocessing/#using-the-command-line-interface","text":"Similar to kipoi predict variant effect prediction can be run by executing: kipoi postproc score_variants my_model_name \\ --dataloader_args '{...}' \\ --vcf_path path/to/my_vcf.vcf \\ --out_vcf_fpath path/to/my_annotated_vcf.vcf Exceptions are that if the dataloader of the model allows the definition of a bed input file, then the respective field in the --dataloader_args JSON will be replaced by a bed file that consists in regions that are centered on the variant position. That is, if in the dataloader.yaml file of the respective model the bed_input flag is set then the respective argument in the --dataloader_args will be overwritten. When using variant effect prediction from the command line and using --source dir , keep in mind that whatever the path is that you put where my_model_name stands in the above command is treated as your model name. Since the annotated VCF INFO tags contain the model name as an identifier, executing kipoi postproc score_variants ./ --source dir ... will result in an annotated VCF with the model name \".\", which is most probably not desired. For those cases kipoi postproc score_variants ... should be executed in at least one directory level higher than the one where the model.yaml file lies. Then the command will look similar to this kipoi postproc score_variants ./my_model --source dir ... and the annotated VCF INFO tags will contain './my_model'.","title":"Using the command line interface"},{"location":"using/02_Postprocessing/#scoring-functions_1","text":"Scoring functions perform calculations on the model predictions for the reference and alternative sequences. Default scoring functions are: logit , logit_alt , logit_ref , diff , deepsea_effect . These functions are described in more detail in the variant effect prediction pages. Given a model is compatible with said scoring functions one or more of those can be selected by using the --scoring argument, e.g.: --scoring diff logit . The model.yaml file defines which scoring functions are available for a model, with the exception that the diff scoring function is available for all models. In the model.yaml also additional custom scoring functions can be defined, for details on please see the variant effect prediction pages. The labels by which the different scoring functions are made available can also be defined in the model.yaml file using the name tag.","title":"Scoring functions"},{"location":"using/02_Postprocessing/#fine-tuning-scoring-functions_1","text":"Scoring functions may have or may even require arguments at instantiation. Those arguments can be passed as JSON dictionaries to scoring functions by using the --scoring_kwargs argument. If --scoring_kwargs is used then for every label set in --scoring there must be a --scoring_kwargs JSON in the exact same order. If the degault values should be used or no arguments are required then an empty dictionary ( {} ) can be used. For example: --scoring diff my_scr --scoring_kwargs '{}' '{my_arg:2}' will use diff with the default parameters and will instantiate my_scr(my_arg=2) . The default scoring functions ( logit , logit_alt , logit_ref , diff , deepsea_effect ) offer different options on how the forward and the reverse complement sequences are merged together. They have an rc_merging argument which can be \"min\" , \"max\" , \"mean\" , \"median\" or \"absmax\" . So if the maximum between forward and reverse complement sequences for the alt-ref prediction differences should be returned, then the command would be: --scoring diff --scoring_kwargs '{rc_merging:\"max\"}' . By default rc_merging is set to \"mean\" .","title":"Fine-tuning scoring functions"},{"location":"using/02_Postprocessing/#mutation-maps","text":"Mutation maps are related to variant effect prediction discussed above. Mutation maps are the application of SNV variant effect prediction on every position of the input sequence with all three alternative alleles. Therefore mutation maps can only be generated for models that support variant effect prediction. Mutation maps can be used to give an overview over the effect scores in a selected region. This region may be centered on a variant of interest or any other region in the genome for which the model can produce a prediction. It is therefore complementary to the variant effect prediction functionality and is intended for use with less variants / regions of interest as the variant effect prediction itself. Typically a mutation map should be calculated for only a handful of regions or query variants (that each tag a region), because for every region many effect predictions have to calculated resulting in calculation time and memory requirements: For a single query variant / query region N = model_sequence_length * 3 * model_output_tasks * effect_scoring_functions effect predictions have to be performed. The workflow is desinged in two steps: In a first step the aforementioned calculation is performed and results are stored in an hdf5 file with standardised format. These files can then be imported into the visualisation part of mutation maps. Both steps are available in python, R as well as the command line.","title":"Mutation maps"},{"location":"using/02_Postprocessing/#calculating-mutation-maps","text":"","title":"Calculating mutation maps"},{"location":"using/02_Postprocessing/#python-r-api","text":"The core element of mutation maps is the MutationMap class that is instantiated with a Kipoi model object, a dataloader object and the dataloader arguments: import kipoi from kipoi.postprocessing.variant_effects import MutationMap model = kipoi.get_model(<model_name>) dataloader = model.default_dataloader dataloader_arguments = {...} mm = MutationMap(model, dataloader, dataloader_arguments) MutationMap instances have the following methods to calculate mutation maps for the given query regions / variants: query_region , query_bed , query_vcf . All those functions return an instance of MutationMapPlotter, which can be stored as a hdf5 file or directly be used for generating mutation map plots.","title":"Python / R API"},{"location":"using/02_Postprocessing/#query_region","text":"The query_region command can be used to generate a mutation map for a selected genomic region: mmp = mm.query_region(\"chr22\", 25346, 25357) The query region has to be transformed to match model input sequence length as well as it has to lie in a genomic region for which the model can produce prediction. All this is taken care of automatically just like in the score_variants function of kipoi.postprocessing.variant_effects . For more details please see below .","title":"query_region"},{"location":"using/02_Postprocessing/#query_bed","text":"The query_bed command can be used to generate mutation maps for genomic regions defined in the bed file: mmp = mm.query_region(\"path/to/my/file.bed\") The query regions have to be transformed to match model input sequence length as well as they hasveto lie in a genomic region for which the model can produce prediction. All this is taken care of automatically just like in the score_variants function of kipoi.postprocessing.variant_effects . For more details please see below .","title":"query_bed"},{"location":"using/02_Postprocessing/#query_vcf","text":"The query_vcf command can be used to generate mutation maps based on variants defined in the vcf file: mmp = mm.query_vcf(\"path/to/my/file.vcf\") The regions for query variants are generated analogously to the score_variants function of kipoi.postprocessing.variant_effects . For more details please see below .","title":"query_vcf"},{"location":"using/02_Postprocessing/#mutationmapplotter","text":"Instances of the MutationMapPlotter class are generated by the query_* methods of the MutationMap class. They contain all the effect predictions plus some additional meta data necessary to produce mutation map plots. Those objects can be stored in hdf5 files. For plotting the plot_mutmap function can be used. The required arguments select the input sequence by a numerical index ( input_entry ), the name of the DNA sequence model input ( model_seq_input ), the name of the scoring function for which the results should be displayed ( scoring_key ) and finally the model output task ( model_output ). A combination of those four values directly link to one set of mutation map predictions.","title":"MutationMapPlotter"},{"location":"using/02_Postprocessing/#input_entry","text":"input_entry is a numerical index indicating which set of input data should be used. This relates back to how query regions are turned into model input data, see below . Since this link depends model sequence length as well as whether the model can only predict for a restricted subset of he genome, the meaning of an index value may vary from model to model. For a combination of models with highly different model input specifications it is therefore advisable to only query a single variant or region in order to avoid confusion.","title":"input_entry"},{"location":"using/02_Postprocessing/#model_seq_input","text":"Many models will only have a single model input key, so this parameter might seem superfluous, but in general a model can have multiple DNA sequence inputs which are all being tested for variant effects.","title":"model_seq_input"},{"location":"using/02_Postprocessing/#scoring_key","text":"The scoring key is one of the labels passed to the query_* function in the scores argument.","title":"scoring_key"},{"location":"using/02_Postprocessing/#model_output","text":"model_output is a model output task label. Additional to the required arguments the plots can be generated for a subset of the model input sequence using the limit_region_genomic . The plot can be generated with reverse-complementation of the sequence by using rc_plot . There are additional features available for the python/R API which are described in the method definition, some of which are also used in the mutation_map.ipynb .","title":"model_output"},{"location":"using/02_Postprocessing/#the-cli","text":"In the CLI mutation maps can be calculated for bed files or for VCF files. Both file formats are accepted by the --regions_file argument of the CLI command: kipoi postproc create_mutation_map <my_model_name> --dataloader_args '{...}' --regions_file path/to/my/file.vcf --output path/to/the/results/file.hdf5","title":"The CLI"},{"location":"using/02_Postprocessing/#plotting","text":"Plotting in the command line works analogously as using the python API: kipoi postproc plot_mutation_map --input_file path/to/the/results/file.hdf5 --input_entry 0 --model_seq_input seq --scoring_key diff --model_output my_model_task --output path/to/the/plot/file.png The meaning of the parameters is identical to the ones in the python API mentioned above. The plotting functionality in the CLI is limited to zooming into genomic region and reverse-complementation of sequences. For examaples please take a look at the mutation_map.ipynb .","title":"Plotting"},{"location":"using/02_Postprocessing/#transformation-of-queries-to-model-input","text":"In order to perform a query on a model the query input must be transformed into genomic regions compatible with the model. Similar to variant effect prediction using the score_variants the automatically chosen region generation method will be chosen based on whether a dataloader offers a bed file input for postprocessing. dataloader.yaml > postprocessing > variant_effects > bed_input . By setting this value the mutation map method will automatically generate a temporary bed input file requesting model input for genomic regions. The path of this temporary bed file is then passed on to the dataloader by resetting the respective argument in the datalaoder_arguments . For some models it is not possible to freely define the genomic region for which model input data should be generated - in that case the dataloader.yaml does not have the dataloader.yaml > postprocessing > variant_effects > bed_input set. In those cases the datalaoder is executed without modifying the dataloader_arguments . The metadata generated alongside the model input is then used to identify model input that overlaps a query region / query variant. For cases when genomic regions can be defined freely for a model, the input samples will always have to generated matching the model input sequence length. This means that for query variants a region of the length of the model input will be centered on the query variant position. For query regions (e.g.: bed input file) every region is overlapped with windows of length of the model input. The first of those regions will start at the same position as the selected query region. Regions of the length of the model input sequence length will then be generated consecutively in order to cover the full region defined by the respective query region - see this schematic: In the top bit of this schematic on can see the case in which the dataloader accepts a bed file as an input to generate model input data. This also requires the correct setup of the dataloader.yaml in postprocessing > variant_effects > bed_input as described in more detail (here)[../postprocessing/variant_effect_prediction]. When the dataloader doesn't support bed input files for region defintion then all the regions generated by the dataloader will be overlapped with the query regions and any overlapping data generated from the dataloader will be used for mutation maps. The same as for bed query files holds true for VCF query files. As mentioned above all of those model input sequences are the subjected to variant effect prediction for every base and ever possible allele. The integer numbers displayed in the green or orange boxes are te order in which model input data is processed by the mutation map calculation algorithm. The numbers represent the index by which the predictions can then be accessed for plotting ( input_entry in plot_mutmap method of MutationMapPlotter or --input_entry CLI argument).","title":"Transformation of queries to model input"},{"location":"using/02_Postprocessing/#layer-activation-extraction","text":"Similar to model prediction on a batch of input data it is possible to predict the activation of intermediate layers. The layer can be selected using the layer argument and the value has to be the identifier of the respective layer.","title":"Layer activation extraction"},{"location":"using/02_Postprocessing/#python-r-api_1","text":"In python and R intermediate (hidden) layer activation can be calculated using the predict_activation_on_batch method of Kipoi models implementing the LayerActivationMixin . All deep learning frameworks supported by Kipoi implement this mixin: import kipoi # Get the model model = kipoi.get_model(\"my_model_name\") dataloader_arguments = {...} # Get the dataloader iterator dl_iterator = model.default_dataloader(**dataloader_arguments).iterator() # Get layer activation res = model.predict_activation_on_batch(dl_iterator.__next__(), layer = \"layer_of_interest\")","title":"python / R API"},{"location":"using/02_Postprocessing/#cli","text":"The prediction of layer activation using the command line works by adding the --layer argument to the prediction command: ``` kipoi predict my_model_name --dataloader_arguments '{...}' --layer 'layer_of_interest' --output path/to/output/file.","title":"CLI"},{"location":"using/02_Postprocessing/#gradient-calculation","text":"The calculation of gradients of the activation of a layer with respect to input data has proven to be a powerful tool for model and data interpretation. This approach can explain which portions of the model input were important for a given model output or for a given activation of a hidden layer. This feature is known as saliency maps . A successor, which relates gradients to the respective model input is the grad input_ score, which is - as its name implies - a multiplication of the first order gradient on the input data with the input data itself. _grad input for genomics models is seen as an alterntiave / complement to perturbation-based approaches like variant effect prediction. The advantage of gradient-based approaches being that in a single calculation step the importance of all bases in an input DNA sqeuence is established at the same time. Additionally gradient based approaches are not limited to Kipoi models that support variant effect prediction, and therefore are usable on all deep learning models. Finally gradient-based approaches highlight the input feature importance for all model inputs at once, also taking interactions between input into account, which cannot be done in variant effect prediction (perturbation-based approaches). Even though deep learning frameworks come with automatic gradient calculation as one of their core features, the accessibility of this functionality to the user is generally not straight forward and may vary from framework version to version. Kipoi comes with a consistent API to extract gradients for the Keras, PyTorch, and Tensorflow frameworks. The API takes advantage of the individual implementations of the automatic differentiation algorithm and in the frameworks, supporting multiple versions of those frameworks. To offer a consistent design the gradient can only be calculated with respect to model input - not with respect to the input of a hidden layer. What is essentially needed in terms of parameters to perform the calculation is: the Kipoi model instance, the Kipoi model input data (generated by a dataloader), an identifier of the (hidden) layer from which (backwards) the gradient should be calculated ( layer ). The gradient can in most frameworks only be calculated on a scalar. To ensure compatibility Kipoi therefore uses averaging functions ( avg_func ) that average across the outputs of a layer. Amongst those the summation ( sum ), maximum ( max ), minimum ( min ) and the maximum of absolute values ( absmax ) are available. To allow more fine-grain control the user can subset outputs ( filter_idx ) of a selected layer that are then passed to the averaging function. The filter_idx arguments takes integers, slice objects, and any tuple consisting in combinations of integers and slice objects. What is passed to filter_idx will be used to select from the model layer output, it must therefore be compatible with the layer output shape. To simplify the process for users who want to calculate the gradient starting from the model output (final layer) the final_layer argument can be used instead of explicitely selecting the layer by its ids with layer . This functionality has to be used with caution, as models may contain post-processing layers or non-linear activation functions as their final layer. Gradient calculation with respect to those values is generally not recommended. To overcome the problem of the last layer being a non-linear activation function another argument pre_nonlinearity was imlemented that tries to traverse the model graph from the selected layer towards model input in case the selected layer is a non-linear activation function. The pre_nonlinearity cannot be implemented for all models and using it may raise Exceptions in case the selected Kipoi model cannot support that feature. It is generally advisable to select a layer explicitely by using the ( layer ) to ensure that the desired calculations are being performed.","title":"Gradient calculation"},{"location":"using/02_Postprocessing/#gradient-visualisation","text":"To complete the model gradient calculation Kipoi comes with gradient visualisation tools, that can distinguish between 1-hot encoded DNA sequence model input and other model input. The default plotting function for 1-hot encoded DNA sequence is a seq-logo plot displaying the prediction output as letter height. In case type of a model input is unknown or it is not DNA sequence a heatmap will be generated. In any case the visualisation tools are only available for one- or two-dimensional input data. An additional feature is the generation of a bedgraph file instead of a heatmap/seqlogo plot. In the general case a seq_dim parameter has to be set defining the dimension in which every entry corresponds to one genomic position in a consecutive order. The GradPlotter class which is repsonsible for gradient visualisations has a plot function that has only one required argument sample . sample is the integer index of dataloader sample for which the plot should be produced.","title":"Gradient visualisation"},{"location":"using/02_Postprocessing/#python-r-api_2","text":"All of the arguments mentioned above are available within the grad_input method of Kipoi models that implement GradientMixin . Namely those are KerasModel , PyTorchModel , and TensorflowModel . For an example please refer to the grad_input.ipynb . The returned output from the grad_input matches the structre of the inputs entry of the data batch generated by the model dataloader.","title":"Python / R API"},{"location":"using/02_Postprocessing/#cli_1","text":"The command line interface is designed in analogy with the model prediction functionality plus the arguments exlpained above. The output can be stored in a tsv or an hdf5 file. In case a hdf5 output is chosen the gradient visualisation methods that come with Kipoi can be used. For an example please refer to the grad_input.ipynb .","title":"CLI"},{"location":"using/03_Model_sources/","text":"~/.kipoi/config.yaml kipoi package has a config file located at ~/.kipoi/config.yaml . By default, it will look like this (without comments): model_sources: kipoi: # source name type: git-lfs # git repository with large file storage (lfs) remote_url: git@github.com:kipoi/models.git # git remote local_path: /home/avsec/.kipoi/models/ # local storage path # special model source storing models accessed via github permalinks github-permalink: type: github-permalink local_path: /home/avsec/.kipoi/github-permalink/ model_sources defines all the places where kipoi will search for models and pull them to a local directory. By default, it contains the model-zoo from github.com/kipoi/models . It is not a normal git repository, since bigger files are stored with git large file storage (git-lfs) . This repository will be stored locally under local_path . Advantage of using git-lfs is that only the files tracked by git will be downloaded first. Larger files stored in git-lfs will be downloaded individually for each model upon request (say when a user invokes a kipoi predict command). All possible model source types In addition to the default kipoi source, you can modify ~/.kipoi/config.yaml and add additional (private or public) model sources. Available model source types are: git-lfs - As for kipoi model source. Model weights will get downloaded upon request (say when running kipoi predict ). git - Normal git repository, all the files will be downloaded on checkout. local - Local directory. Example: model_sources: kipoi: type: git-lfs remote_url: git@github.com:kipoi/models.git local_path: /home/avsec/.kipoi/models/ my_git_models: type: git remote_url: git@github.com:asd/other_models.git local_path: ~/.kipoi/other_models/ my_local_models: type: local local_path: /data/mymodels/ About model definition A particular model is defined by its source (key under model_sources , say kipoi ) and the relative path of the desired model directory from the model source root (say rbp_eclip/UPF1 ). A directory is considered a model if it contains a model.yaml file.","title":"Private and public model sources"},{"location":"using/03_Model_sources/#kipoiconfigyaml","text":"kipoi package has a config file located at ~/.kipoi/config.yaml . By default, it will look like this (without comments): model_sources: kipoi: # source name type: git-lfs # git repository with large file storage (lfs) remote_url: git@github.com:kipoi/models.git # git remote local_path: /home/avsec/.kipoi/models/ # local storage path # special model source storing models accessed via github permalinks github-permalink: type: github-permalink local_path: /home/avsec/.kipoi/github-permalink/ model_sources defines all the places where kipoi will search for models and pull them to a local directory. By default, it contains the model-zoo from github.com/kipoi/models . It is not a normal git repository, since bigger files are stored with git large file storage (git-lfs) . This repository will be stored locally under local_path . Advantage of using git-lfs is that only the files tracked by git will be downloaded first. Larger files stored in git-lfs will be downloaded individually for each model upon request (say when a user invokes a kipoi predict command).","title":"~/.kipoi/config.yaml"},{"location":"using/03_Model_sources/#all-possible-model-source-types","text":"In addition to the default kipoi source, you can modify ~/.kipoi/config.yaml and add additional (private or public) model sources. Available model source types are: git-lfs - As for kipoi model source. Model weights will get downloaded upon request (say when running kipoi predict ). git - Normal git repository, all the files will be downloaded on checkout. local - Local directory. Example: model_sources: kipoi: type: git-lfs remote_url: git@github.com:kipoi/models.git local_path: /home/avsec/.kipoi/models/ my_git_models: type: git remote_url: git@github.com:asd/other_models.git local_path: ~/.kipoi/other_models/ my_local_models: type: local local_path: /data/mymodels/","title":"All possible model source types"},{"location":"using/03_Model_sources/#about-model-definition","text":"A particular model is defined by its source (key under model_sources , say kipoi ) and the relative path of the desired model directory from the model source root (say rbp_eclip/UPF1 ). A directory is considered a model if it contains a model.yaml file.","title":"About model definition"},{"location":"using/04_Installing_on_OSX/","text":"Using Kipoi - Installing on OSX Depending on the versino of OSX you are using there is python pre-installed or not. On OSX Sierra it is not, but on OSX High Sierra it is. For Kipoi to work fully you will need a version of python (2.7, 3.5 or 3.6) installed, preferably you will also have an installation of conda. We have seen problems when conda environments were re-used so we strongly recommend that you create a new environment e.g. kipoi where you install Kipoi. Steps Make sure you have python installed: You can try by just execting python in your Terminal, if nothing is found you will want to install python (not pythonw ). There are some good explanations on how python 2 can be installed on OSX Sierra and if you are using High Sierra and you prefer python 3 you can follow this . After completing the steps and installing conda or miniconda please procede as described in getting started .","title":"Installing on OSX"},{"location":"using/04_Installing_on_OSX/#using-kipoi-installing-on-osx","text":"Depending on the versino of OSX you are using there is python pre-installed or not. On OSX Sierra it is not, but on OSX High Sierra it is. For Kipoi to work fully you will need a version of python (2.7, 3.5 or 3.6) installed, preferably you will also have an installation of conda. We have seen problems when conda environments were re-used so we strongly recommend that you create a new environment e.g. kipoi where you install Kipoi.","title":"Using Kipoi - Installing on OSX"},{"location":"using/04_Installing_on_OSX/#steps","text":"","title":"Steps"},{"location":"using/04_Installing_on_OSX/#make-sure-you-have-python-installed","text":"You can try by just execting python in your Terminal, if nothing is found you will want to install python (not pythonw ). There are some good explanations on how python 2 can be installed on OSX Sierra and if you are using High Sierra and you prefer python 3 you can follow this . After completing the steps and installing conda or miniconda please procede as described in getting started .","title":"Make sure you have python installed:"},{"location":"using/old_04_cli/","text":"This docs shows how our API should ideally look like. CLI definition Main arguments <model_dir> : can be a git repository, local file path, remote file path or a short model string. Pre-process usage: kipoi preproc [-h] [--source {kipoi,dir}] [--extractor_args EXTRACTOR_ARGS] [--batch_size BATCH_SIZE] [-i] [-o OUTPUT] model Run the extractor and save the output to an hdf5 file. positional arguments: model Model name. optional arguments: -h, --help show this help message and exit --source {kipoi,dir} Model source to use. Specified in ~/.kipoi/config.yaml under model_sources --extractor_args EXTRACTOR_ARGS Extractor arguments either as a json string:'{\"arg1\": 1} or as a file path to a json file --batch_size BATCH_SIZE Batch size to use in prediction -i, --install-req Install required packages from requirements.txt -o OUTPUT, --output OUTPUT Output hdf5 file Predict usage: kipoi predict [-h] [--source {kipoi,dir}] [--extractor_args EXTRACTOR_ARGS] [-f {tsv,bed,hdf5}] [--batch_size BATCH_SIZE] [-n NUM_WORKERS] [-i] [-k] [-o OUTPUT] model Run the model prediction. positional arguments: model Model name. optional arguments: -h, --help show this help message and exit --source {kipoi,dir} Model source to use. Specified in ~/.kipoi/config.yaml under model_sources --extractor_args EXTRACTOR_ARGS Extractor arguments either as a json string:'{\"arg1\": 1} or as a file path to a json file -f {tsv,bed,hdf5}, --file_format {tsv,bed,hdf5} File format. --batch_size BATCH_SIZE Batch size to use in prediction -n NUM_WORKERS, --num_workers NUM_WORKERS Number of parallel workers for loading the dataset -i, --install-req Install required packages from requirements.txt -k, --keep_inputs Keep the inputs in the output file. Only compatible with hdf5 file format -o OUTPUT, --output OUTPUT Output hdf5 file Test the model Runs a set of unit-tests for the model usage: kipoi test [-h] [--source {kipoi,dir}] [--batch_size BATCH_SIZE] [-i] model script to test model zoo submissions positional arguments: model Model name. optional arguments: -h, --help show this help message and exit --source {kipoi,dir} Model source to use. Specified in ~/.kipoi/config.yaml under model_sources --batch_size BATCH_SIZE Batch size to use in prediction -i, --install-req Install required packages from requirements.txt Score variants Not implemented model_zoo score_variants <model> <preprocessor inputs...> <vcf file> -o <output> Pull the model Downloads the directory associated with the model $ kipoi pull -h usage: kipoi pull [-h] [--source {kipoi}] model Downloads the directory associated with the model. positional arguments: model Model name. optional arguments: -h, --help show this help message and exit --source {kipoi} Model source to use. Specified in ~/.kipoi/config.yaml under model_sources","title":"Old 04 cli"},{"location":"using/old_04_cli/#cli-definition","text":"Main arguments <model_dir> : can be a git repository, local file path, remote file path or a short model string.","title":"CLI definition"},{"location":"using/old_04_cli/#pre-process","text":"usage: kipoi preproc [-h] [--source {kipoi,dir}] [--extractor_args EXTRACTOR_ARGS] [--batch_size BATCH_SIZE] [-i] [-o OUTPUT] model Run the extractor and save the output to an hdf5 file. positional arguments: model Model name. optional arguments: -h, --help show this help message and exit --source {kipoi,dir} Model source to use. Specified in ~/.kipoi/config.yaml under model_sources --extractor_args EXTRACTOR_ARGS Extractor arguments either as a json string:'{\"arg1\": 1} or as a file path to a json file --batch_size BATCH_SIZE Batch size to use in prediction -i, --install-req Install required packages from requirements.txt -o OUTPUT, --output OUTPUT Output hdf5 file","title":"Pre-process"},{"location":"using/old_04_cli/#predict","text":"usage: kipoi predict [-h] [--source {kipoi,dir}] [--extractor_args EXTRACTOR_ARGS] [-f {tsv,bed,hdf5}] [--batch_size BATCH_SIZE] [-n NUM_WORKERS] [-i] [-k] [-o OUTPUT] model Run the model prediction. positional arguments: model Model name. optional arguments: -h, --help show this help message and exit --source {kipoi,dir} Model source to use. Specified in ~/.kipoi/config.yaml under model_sources --extractor_args EXTRACTOR_ARGS Extractor arguments either as a json string:'{\"arg1\": 1} or as a file path to a json file -f {tsv,bed,hdf5}, --file_format {tsv,bed,hdf5} File format. --batch_size BATCH_SIZE Batch size to use in prediction -n NUM_WORKERS, --num_workers NUM_WORKERS Number of parallel workers for loading the dataset -i, --install-req Install required packages from requirements.txt -k, --keep_inputs Keep the inputs in the output file. Only compatible with hdf5 file format -o OUTPUT, --output OUTPUT Output hdf5 file","title":"Predict"},{"location":"using/old_04_cli/#test-the-model","text":"Runs a set of unit-tests for the model usage: kipoi test [-h] [--source {kipoi,dir}] [--batch_size BATCH_SIZE] [-i] model script to test model zoo submissions positional arguments: model Model name. optional arguments: -h, --help show this help message and exit --source {kipoi,dir} Model source to use. Specified in ~/.kipoi/config.yaml under model_sources --batch_size BATCH_SIZE Batch size to use in prediction -i, --install-req Install required packages from requirements.txt","title":"Test the model"},{"location":"using/old_04_cli/#score-variants","text":"Not implemented model_zoo score_variants <model> <preprocessor inputs...> <vcf file> -o <output>","title":"Score variants"},{"location":"using/old_04_cli/#pull-the-model","text":"Downloads the directory associated with the model $ kipoi pull -h usage: kipoi pull [-h] [--source {kipoi}] model Downloads the directory associated with the model. positional arguments: model Model name. optional arguments: -h, --help show this help message and exit --source {kipoi} Model source to use. Specified in ~/.kipoi/config.yaml under model_sources","title":"Pull the model"}]}