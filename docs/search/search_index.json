{
    "docs": [
        {
            "location": "/",
            "text": "Kipoi: model zoo for genomics\n\n\nLinks\n\n\n\n\nkipoi.org\n - Main website\n\n\nkipoi.org/docs\n - Documentation\n\n\ngithub.com/kipoi/models\n - Model zoo for genomics maintained by the Kipoi team\n\n\n\n\nInstallation\n\n\n1. Install miniconda/anaconda\n\n\nKipoi requires \nconda\n to manage model dependencies.\nMake sure you have either anaconda (\ndownload page\n) or miniconda (\ndownload page\n) installed. If you are using OSX, see \nInstalling python on OSX\n.\n\n\n2. Install Git LFS\n\n\nFor downloading models, Kipoi uses \nGit Large File Storage\n (LFS). To install it on Ubuntu, run:\n\n\n# on Ubuntu\ncurl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\nsudo apt-get install -y git-lfs\ngit-lfs install\n\n\n\n\nAlternatively, install it through conda:\n\n\nconda install -c conda-forge git-lfs && git lfs install\n\n\n\n\n3. Install Kipoi\n\n\nNext, install Kipoi using \npip\n:\n\n\npip install kipoi\n\n\n\n\nDevelopment\n\n\nIf you wish to develop \nkipoi\n, run instead:\n\n\nconda install pytorch-cpu\npip install -e '.[develop]'\n\n\n\n\nThis will install some additional packages like \npytest\n. You can test the package by running \npy.test\n. \n\n\nIf you wish to run tests in parallel, run \npy.test -n 6\n.\n\n\nQuick start\n\n\n\n\nPython\n\n\nList available models\n\n\nimport kipoi\n\nkipoi.list_models()\n\n\n\n\nHint: For an overview over the available models also check the \nmodel overview\n on our website, where you can see example commands for how to use the models on the CLI, python and R.\n\n\nLoad the model from model source or local directory\n\n\n# Load the model from github.com/kipoi/models/rbp\nmodel = kipoi.get_model(\"rbp_eclip/UPF1\", source=\"kipoi\") # source=\"kipoi\" is the default\n\n# Load the model from a local directory\nmodel = kipoi.get_model(\"~/mymodels/rbp\", source=\"dir\")  \n# Note: Custom model sources are defined in ~/.kipoi/config.yaml\n\n\n\n\nMain model attributes and methods\n\n\n# See the information about the author:\nmodel.info\n\n# Access the default dataloader\nmodel.default_dataloader\n\n# Access the Keras model\nmodel.model\n\n# Predict on batch - implemented by all the models regardless of the framework\n# (i.e. works with sklearn, Keras, tensorflow, ...)\nmodel.predict_on_batch(x)\n\n# Get predictions for the raw files\n# Kipoi runs: raw files -[dataloader]-> numpy arrays -[model]-> predictions \nmodel.pipeline.predict({\"dataloader_arg1\": \"inputs.csv\"})\n\n\n\n\nLoad the dataloader\n\n\nDl = kipoi.get_dataloader_factory(\"rbp_eclip/UPF1\") # returns a class that needs to be instantiated\ndl = Dl(dataloader_arg1=\"inputs.csv\")  # Create/instantiate an object\n\n\n\n\nDataloader attributes and methods\n\n\n# batch_iter - common to all dataloaders\n# Returns an iterator generating batches of model-ready numpy.arrays\nit = dl.batch_iter(batch_size=32)\nout = next(it)  # {\"inputs\": np.array, (optional) \"targets\": np.arrays.., \"metadata\": np.arrays...}\n\n# To get predictions, run\nmodel.predict_on_batch(out['inputs'])\n\n# load the whole dataset into memory\ndl.load_all()\n\n\n\n\nRe-train the model\n\n\n# re-train example for Keras\ndl = Dl(dataloader_arg1=\"inputs.csv\", targets_file=\"mytargets.csv\")\nit_train = dl.batch_train_iter(batch_size=32)  \n# batch_train_iter is a convenience wrapper of batch_iter\n# yielding (inputs, targets) tuples indefinitely\nmodel.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10)\n\n\n\n\nFor more information see: \nnbs/python-api.ipynb\n and \ndocs/using getting started\n\n\nCommand-line\n\n\n$ kipoi\nusage: kipoi <command> [-h] ...\n\n    # Kipoi model-zoo command line tool. Available sub-commands:\n    # - using models:\n    ls               List all the available models\n    predict          Run the model prediction\n    pull             Download the directory associated with the model\n    preproc          Run the dataloader and save the results to an hdf5 array\n    postproc         Tools for model postprocessing like variant effect prediction\n    env              Tools for managing Kipoi conda environments\n\n    # - contribuing models:\n    init             Initialize a new Kipoi model\n    test             Runs a set of unit-tests for the model\n    test-source      Runs a set of unit-tests for many/all models in a source\n\n\n\n\nExplore the CLI usage by running \nkipoi <command> -h\n. Also, see \ndocs/using/getting started cli\n for more information.\n\n\nConfigure Kipoi in \n.kipoi/config.yaml\n\n\nYou can add your own (private) model sources. See \ndocs/using/03_Model_sources/\n.\n\n\nContributing models\n\n\nSee \ndocs/contributing getting started\n and \ndocs/tutorials/contributing/models\n for more information.\n\n\nPostprocessing\n\n\nSNV effect prediction\n\n\nFunctionality to predict the effect of SNVs is available in the API as well as in the command line interface. The input\nis a VCF which can then be annotated with effect predictions and returned in the process. For more details on the requirements for the models and\n dataloaders please check \ndocs/using/02_Variant_effect_prediction\n\n\nDocumentation\n\n\nDocumentation can be found here: \nkipoi.org/docs",
            "title": "Home"
        },
        {
            "location": "/#kipoi-model-zoo-for-genomics",
            "text": "",
            "title": "Kipoi: model zoo for genomics"
        },
        {
            "location": "/#links",
            "text": "kipoi.org  - Main website  kipoi.org/docs  - Documentation  github.com/kipoi/models  - Model zoo for genomics maintained by the Kipoi team",
            "title": "Links"
        },
        {
            "location": "/#installation",
            "text": "",
            "title": "Installation"
        },
        {
            "location": "/#1-install-minicondaanaconda",
            "text": "Kipoi requires  conda  to manage model dependencies.\nMake sure you have either anaconda ( download page ) or miniconda ( download page ) installed. If you are using OSX, see  Installing python on OSX .",
            "title": "1. Install miniconda/anaconda"
        },
        {
            "location": "/#2-install-git-lfs",
            "text": "For downloading models, Kipoi uses  Git Large File Storage  (LFS). To install it on Ubuntu, run:  # on Ubuntu\ncurl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\nsudo apt-get install -y git-lfs\ngit-lfs install  Alternatively, install it through conda:  conda install -c conda-forge git-lfs && git lfs install",
            "title": "2. Install Git LFS"
        },
        {
            "location": "/#3-install-kipoi",
            "text": "Next, install Kipoi using  pip :  pip install kipoi",
            "title": "3. Install Kipoi"
        },
        {
            "location": "/#development",
            "text": "If you wish to develop  kipoi , run instead:  conda install pytorch-cpu\npip install -e '.[develop]'  This will install some additional packages like  pytest . You can test the package by running  py.test .   If you wish to run tests in parallel, run  py.test -n 6 .",
            "title": "Development"
        },
        {
            "location": "/#quick-start",
            "text": "",
            "title": "Quick start"
        },
        {
            "location": "/#python",
            "text": "List available models  import kipoi\n\nkipoi.list_models()  Hint: For an overview over the available models also check the  model overview  on our website, where you can see example commands for how to use the models on the CLI, python and R.  Load the model from model source or local directory  # Load the model from github.com/kipoi/models/rbp\nmodel = kipoi.get_model(\"rbp_eclip/UPF1\", source=\"kipoi\") # source=\"kipoi\" is the default\n\n# Load the model from a local directory\nmodel = kipoi.get_model(\"~/mymodels/rbp\", source=\"dir\")  \n# Note: Custom model sources are defined in ~/.kipoi/config.yaml  Main model attributes and methods  # See the information about the author:\nmodel.info\n\n# Access the default dataloader\nmodel.default_dataloader\n\n# Access the Keras model\nmodel.model\n\n# Predict on batch - implemented by all the models regardless of the framework\n# (i.e. works with sklearn, Keras, tensorflow, ...)\nmodel.predict_on_batch(x)\n\n# Get predictions for the raw files\n# Kipoi runs: raw files -[dataloader]-> numpy arrays -[model]-> predictions \nmodel.pipeline.predict({\"dataloader_arg1\": \"inputs.csv\"})  Load the dataloader  Dl = kipoi.get_dataloader_factory(\"rbp_eclip/UPF1\") # returns a class that needs to be instantiated\ndl = Dl(dataloader_arg1=\"inputs.csv\")  # Create/instantiate an object  Dataloader attributes and methods  # batch_iter - common to all dataloaders\n# Returns an iterator generating batches of model-ready numpy.arrays\nit = dl.batch_iter(batch_size=32)\nout = next(it)  # {\"inputs\": np.array, (optional) \"targets\": np.arrays.., \"metadata\": np.arrays...}\n\n# To get predictions, run\nmodel.predict_on_batch(out['inputs'])\n\n# load the whole dataset into memory\ndl.load_all()  Re-train the model  # re-train example for Keras\ndl = Dl(dataloader_arg1=\"inputs.csv\", targets_file=\"mytargets.csv\")\nit_train = dl.batch_train_iter(batch_size=32)  \n# batch_train_iter is a convenience wrapper of batch_iter\n# yielding (inputs, targets) tuples indefinitely\nmodel.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10)  For more information see:  nbs/python-api.ipynb  and  docs/using getting started",
            "title": "Python"
        },
        {
            "location": "/#command-line",
            "text": "$ kipoi\nusage: kipoi <command> [-h] ...\n\n    # Kipoi model-zoo command line tool. Available sub-commands:\n    # - using models:\n    ls               List all the available models\n    predict          Run the model prediction\n    pull             Download the directory associated with the model\n    preproc          Run the dataloader and save the results to an hdf5 array\n    postproc         Tools for model postprocessing like variant effect prediction\n    env              Tools for managing Kipoi conda environments\n\n    # - contribuing models:\n    init             Initialize a new Kipoi model\n    test             Runs a set of unit-tests for the model\n    test-source      Runs a set of unit-tests for many/all models in a source  Explore the CLI usage by running  kipoi <command> -h . Also, see  docs/using/getting started cli  for more information.",
            "title": "Command-line"
        },
        {
            "location": "/#configure-kipoi-in-kipoiconfigyaml",
            "text": "You can add your own (private) model sources. See  docs/using/03_Model_sources/ .",
            "title": "Configure Kipoi in .kipoi/config.yaml"
        },
        {
            "location": "/#contributing-models",
            "text": "See  docs/contributing getting started  and  docs/tutorials/contributing/models  for more information.",
            "title": "Contributing models"
        },
        {
            "location": "/#postprocessing",
            "text": "",
            "title": "Postprocessing"
        },
        {
            "location": "/#snv-effect-prediction",
            "text": "Functionality to predict the effect of SNVs is available in the API as well as in the command line interface. The input\nis a VCF which can then be annotated with effect predictions and returned in the process. For more details on the requirements for the models and\n dataloaders please check  docs/using/02_Variant_effect_prediction",
            "title": "SNV effect prediction"
        },
        {
            "location": "/#documentation",
            "text": "Documentation can be found here:  kipoi.org/docs",
            "title": "Documentation"
        },
        {
            "location": "/using/01_Getting_started/",
            "text": "Using Kipoi - Getting started\n\n\nSteps\n\n\n1. Install Kipoi\n\n\n\n\nInstall git-lfs\n\n\nconda install -c conda-forge git-lfs && git lfs install\n\n\nFor alternative installation options  see \nhttps://git-lfs.github.com/\n.\n\n\n\n\n\n\nInstall kipoi\n\n\npip install kipoi\n\n\nif you wish to use variant effect prediction, run \npip install kipoi[vep]\n\n\n\n\n\n\n\n\n2. Choose the model from \nhttp://kipoi.org/models\n\n\n\n\nNavigate to \nhttp://kipoi.org/models\n and select a model of your interest\n\n\nOn model's page (\nhttp://kipoi.org/models/<model>\n), check the required arguments in the dataloader section and copy the code snippets\n\n\n\n\n3. Use the model\n\n\nYou can use the model from:\n\n\n\n\nPython\n\n\nCommand-line interface\n\n\nR (via the \nreticulate\n package)\n\n\n\n\n\n\nPython - quick start\n\n\nSee the ipython notebook \ntutorials/python-api\n for additional information and a working example of the API. Here is a list of most useful python commands.\n\n\nimport kipoi\n\n\n\n\nList all models\n\n\nkipoi.list_models()\n\n\n\n\nGet the model\n\n\nmodel = kipoi.get_model(\"rbp_eclip/UPF1\")\n\n\n\n\nAccess information about the model\n\n\nmodel.info # Information about the author:\n\nmodel.default_dataloader # Access the default dataloader\n\nmodel.model # Access the underlying Keras model\n\n\n\n\nTest the model\n\n\npred = model.pipeline.predict_example()\n\n\n\n\nGet predictions for the raw files\n\n\nFor any generation of the model output the dataloader has to be executed first. A dataloader will require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. One way to display the keyword arguments a dataloader accepts is the following:\n\n\nmodel.default_dataloader.print_args()\n\n\n\n\nThe output of the function above will tell you which arguments you can use when running the following command. Alternatively, you can view the dataloader arguments on the model's website (\nhttp://kipoi.org/models/<model>\n). Let's assume that \nmodel.default_dataloder.print_args()\n has informed us that the dataloader accepts the arguments \ndataloader_arg1\n and \ntargets_file\n. You can get the model prediction using \nkipoi.pipeline.predict\n:\n\n\nmodel.pipeline.predict({\"dataloader_arg1\": \"...\", \"targets_file\": \"...\"})\n\n\n\n\nSpecifically, for the \nrbp_eclip/UPF1\n model, you would run the following:\n\n\n# Make sure we are in the directory containing the example files\nimport os\nos.chdir(os.path.expanduser('~/.kipoi/models/rbp_eclip/UPF1'))\n\n# Run the prediction\nmodel.pipeline.predict({'intervals_file': 'example_files/intervals.bed', \n                        'fasta_file': 'example_files/hg38_chr22.fa', \n                    'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', \n                    'filter_protein_coding': True, \n                    'target_file': 'example_files/targets.tsv'})\n\n\n\n\nSetup the dataloader\n\n\ndl = model.default_dataloader(dataloader_arg1=\"...\", targets_file=\"...\")\n\n\n\n\nNote: \nkipoi.get_model(\"<mymodel>\").default_dataloader\n is the same as \nkipoi.get_dataloader_factory(\"<mymodel>\")\n\n\nPredict for a single batch\n\n\n# Get the batch iterator\nit = dl.batch_iter(batch_size=32)\n\n# get a single batch\nsingle_batch = next(it)\n\n# Make a prediction\npredictions = model.predict_on_batch(single_batch['inputs'])\n\n\n\n\nRe-train the model\n\n\nit_train = dl.batch_train_iter(batch_size=32)  # will yield tuples (inputs, targets) indefinitely\n\n# Since we are using a Keras model, run:\nmodel.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10)\n\n\n\n\n\n\nCommand-line interface - quick start\n\n\nShow help\n\n\nkipoi -h\n\n\n\n\nList all models\n\n\nkipoi ls\n\n\n\n\nGet information on how the required dataloader keyword arguments\n\n\nkipoi info -i --source kipoi rbp_eclip/UPF1\n\n\n\n\nRun model prediction\n\n\ncd ~/.kipoi/models/rbp_eclip/UPF1/example_files\n\nkipoi predict rbp_eclip/UPF1 \\\n  --dataloader_args='{'intervals_file': 'intervals.bed', 'fasta_file': 'hg38_chr22.fa', 'gtf_file': 'gencode.v24.annotation_chr22.gtf'}' \\\n  -o '/tmp/rbp_eclip__UPF1.example_pred.tsv'\n\n# check the results\nhead '/tmp/rbp_eclip__UPF1.example_pred.tsv'\n\n\n\n\nTest a model\n\n\nkipoi test ~/.kipoi/models/rbp_eclip/UPF1/example_files\n\n\n\n\nInstall all model dependencies\n\n\nkipoi env install rbp_eclip/UPF1\n\n\n\n\nCreate a new conda environment for the model\n\n\nkipoi env create rbp_eclip/UPF1\nsource activate kipoi-rbp_eclip__UPF\n\n\n\n\nList all Kipoi environments\n\n\nkipoi env list\n\n\n\n\nUse \nsource activate <env>\n or \nconda activate <env>\n to activate the environment.\n\n\nScore variants\n\n\nkipoi postproc score_variant rbp_eclip/UPF1 \\\n    --batch_size=16 \\\n    -v input.vcf \\\n    -o output.vcf\n\n\n\n\nR - quick start\n\n\nSee \ntutorials/R-api\n.",
            "title": "Getting started"
        },
        {
            "location": "/using/01_Getting_started/#using-kipoi-getting-started",
            "text": "",
            "title": "Using Kipoi - Getting started"
        },
        {
            "location": "/using/01_Getting_started/#steps",
            "text": "",
            "title": "Steps"
        },
        {
            "location": "/using/01_Getting_started/#1-install-kipoi",
            "text": "Install git-lfs  conda install -c conda-forge git-lfs && git lfs install  For alternative installation options  see  https://git-lfs.github.com/ .    Install kipoi  pip install kipoi  if you wish to use variant effect prediction, run  pip install kipoi[vep]",
            "title": "1. Install Kipoi"
        },
        {
            "location": "/using/01_Getting_started/#2-choose-the-model-from-httpkipoiorgmodels",
            "text": "Navigate to  http://kipoi.org/models  and select a model of your interest  On model's page ( http://kipoi.org/models/<model> ), check the required arguments in the dataloader section and copy the code snippets",
            "title": "2. Choose the model from http://kipoi.org/models"
        },
        {
            "location": "/using/01_Getting_started/#3-use-the-model",
            "text": "You can use the model from:   Python  Command-line interface  R (via the  reticulate  package)",
            "title": "3. Use the model"
        },
        {
            "location": "/using/01_Getting_started/#python-quick-start",
            "text": "See the ipython notebook  tutorials/python-api  for additional information and a working example of the API. Here is a list of most useful python commands.  import kipoi",
            "title": "Python - quick start"
        },
        {
            "location": "/using/01_Getting_started/#list-all-models",
            "text": "kipoi.list_models()",
            "title": "List all models"
        },
        {
            "location": "/using/01_Getting_started/#get-the-model",
            "text": "model = kipoi.get_model(\"rbp_eclip/UPF1\")",
            "title": "Get the model"
        },
        {
            "location": "/using/01_Getting_started/#access-information-about-the-model",
            "text": "model.info # Information about the author:\n\nmodel.default_dataloader # Access the default dataloader\n\nmodel.model # Access the underlying Keras model",
            "title": "Access information about the model"
        },
        {
            "location": "/using/01_Getting_started/#test-the-model",
            "text": "pred = model.pipeline.predict_example()",
            "title": "Test the model"
        },
        {
            "location": "/using/01_Getting_started/#get-predictions-for-the-raw-files",
            "text": "For any generation of the model output the dataloader has to be executed first. A dataloader will require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. One way to display the keyword arguments a dataloader accepts is the following:  model.default_dataloader.print_args()  The output of the function above will tell you which arguments you can use when running the following command. Alternatively, you can view the dataloader arguments on the model's website ( http://kipoi.org/models/<model> ). Let's assume that  model.default_dataloder.print_args()  has informed us that the dataloader accepts the arguments  dataloader_arg1  and  targets_file . You can get the model prediction using  kipoi.pipeline.predict :  model.pipeline.predict({\"dataloader_arg1\": \"...\", \"targets_file\": \"...\"})  Specifically, for the  rbp_eclip/UPF1  model, you would run the following:  # Make sure we are in the directory containing the example files\nimport os\nos.chdir(os.path.expanduser('~/.kipoi/models/rbp_eclip/UPF1'))\n\n# Run the prediction\nmodel.pipeline.predict({'intervals_file': 'example_files/intervals.bed', \n                        'fasta_file': 'example_files/hg38_chr22.fa', \n                    'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', \n                    'filter_protein_coding': True, \n                    'target_file': 'example_files/targets.tsv'})",
            "title": "Get predictions for the raw files"
        },
        {
            "location": "/using/01_Getting_started/#setup-the-dataloader",
            "text": "dl = model.default_dataloader(dataloader_arg1=\"...\", targets_file=\"...\")  Note:  kipoi.get_model(\"<mymodel>\").default_dataloader  is the same as  kipoi.get_dataloader_factory(\"<mymodel>\")",
            "title": "Setup the dataloader"
        },
        {
            "location": "/using/01_Getting_started/#predict-for-a-single-batch",
            "text": "# Get the batch iterator\nit = dl.batch_iter(batch_size=32)\n\n# get a single batch\nsingle_batch = next(it)\n\n# Make a prediction\npredictions = model.predict_on_batch(single_batch['inputs'])",
            "title": "Predict for a single batch"
        },
        {
            "location": "/using/01_Getting_started/#re-train-the-model",
            "text": "it_train = dl.batch_train_iter(batch_size=32)  # will yield tuples (inputs, targets) indefinitely\n\n# Since we are using a Keras model, run:\nmodel.model.fit_generator(it_train, steps_per_epoch=len(dl)//32, epochs=10)",
            "title": "Re-train the model"
        },
        {
            "location": "/using/01_Getting_started/#command-line-interface-quick-start",
            "text": "",
            "title": "Command-line interface - quick start"
        },
        {
            "location": "/using/01_Getting_started/#show-help",
            "text": "kipoi -h",
            "title": "Show help"
        },
        {
            "location": "/using/01_Getting_started/#list-all-models_1",
            "text": "kipoi ls",
            "title": "List all models"
        },
        {
            "location": "/using/01_Getting_started/#get-information-on-how-the-required-dataloader-keyword-arguments",
            "text": "kipoi info -i --source kipoi rbp_eclip/UPF1",
            "title": "Get information on how the required dataloader keyword arguments"
        },
        {
            "location": "/using/01_Getting_started/#run-model-prediction",
            "text": "cd ~/.kipoi/models/rbp_eclip/UPF1/example_files\n\nkipoi predict rbp_eclip/UPF1 \\\n  --dataloader_args='{'intervals_file': 'intervals.bed', 'fasta_file': 'hg38_chr22.fa', 'gtf_file': 'gencode.v24.annotation_chr22.gtf'}' \\\n  -o '/tmp/rbp_eclip__UPF1.example_pred.tsv'\n\n# check the results\nhead '/tmp/rbp_eclip__UPF1.example_pred.tsv'",
            "title": "Run model prediction"
        },
        {
            "location": "/using/01_Getting_started/#test-a-model",
            "text": "kipoi test ~/.kipoi/models/rbp_eclip/UPF1/example_files",
            "title": "Test a model"
        },
        {
            "location": "/using/01_Getting_started/#install-all-model-dependencies",
            "text": "kipoi env install rbp_eclip/UPF1",
            "title": "Install all model dependencies"
        },
        {
            "location": "/using/01_Getting_started/#create-a-new-conda-environment-for-the-model",
            "text": "kipoi env create rbp_eclip/UPF1\nsource activate kipoi-rbp_eclip__UPF",
            "title": "Create a new conda environment for the model"
        },
        {
            "location": "/using/01_Getting_started/#list-all-kipoi-environments",
            "text": "kipoi env list  Use  source activate <env>  or  conda activate <env>  to activate the environment.",
            "title": "List all Kipoi environments"
        },
        {
            "location": "/using/01_Getting_started/#score-variants",
            "text": "kipoi postproc score_variant rbp_eclip/UPF1 \\\n    --batch_size=16 \\\n    -v input.vcf \\\n    -o output.vcf",
            "title": "Score variants"
        },
        {
            "location": "/using/01_Getting_started/#r-quick-start",
            "text": "See  tutorials/R-api .",
            "title": "R - quick start"
        },
        {
            "location": "/using/02_Variant_effect_prediction/",
            "text": "Using variant effect prediction\n\n\nThis chapter describes how to run variant prediction using a model in the zoo either using the python functionality or using the command line. A prerequesite is that the model is compatible with variant effect prediction (see: Variant effect prediction prerequesites for the model.yaml and dataloader.yaml)\n\n\nVariant effect prediction in python\n\n\nUsing variant effect prediction within python allows more flexibility in the finegrain details compared to using the command line interface. The core function of variant effect prediction is \npredict_snvs\n, which on the one hand requires a model with its dataloader as well as a valid VCF.\n\n\nThe easiest way to run variant effect prediction is the following:\n\n\nimport kipoi.postprocessing.variant_effects.snv_predict as sp\nfrom kipoi.postprocessing.variant_effects import VcfWriter\nfrom kipoi.postprocessing.variant_effects import Diff\n\nmodel = kipoi.get_model(\"my_model_name\")\n\nDataloader = kipoi.get_dataloader_factory(model_dir, source=\"dir\")\n\ndataloader_arguments = {...}\n\nvcf_path = \"path/to/my_vcf.vcf\"\nout_vcf_fpath = \"path/to/my_annotated_vcf.vcf\"\n\nwriter = VcfWriter(model, vcf_path, out_vcf_fpath)\n\nsp.predict_snvs(model, Dataloader, vcf_path, batch_size = 32,\n                      dataloader_args=dataloader_arguments,\n                      evaluation_function_kwargs={'diff_types': {'diff': Diff()}},\n                      sync_pred_writer=writer)\n\n\n\n\nWhere \nmodel\n is a kipoi model - replace \nmy_model_name\n by a valid model name and \nDataloader\n is a dataloader factory that fits to the model. \ndataloader_arguments\n contains all the kwargs that are necessary to run the dataloader. In \nvcf_path\n the path to a valid VCF has to be set. Coordinates have to match the genome / assembly etc. of the raw input files used by the dataloader. In order to generate an output VCF the \nVcfWriter\n class is used. For it work it needs to be initialised with the model object, the inpput VCF path and the path where the annotated VCF should be stored. The last missing element that has to be set is the scoring function. This is the function that defines how to compare reference with alternative predictions of the model. For details on the different scoring methods please take a look at the detailed explanation of variant effect prediction or the API defintion.\n\n\nThe above code will run the dataloader based with \ndataloader_arguments\n and try to overlap the input VCF with the sequences generated by the dataloader. Then the predicitons are compared using whichever scoring functions ae defined in \nevaluation_function_kwargs\n. The results are then stored in the output VCF using the \nwriter\n object.\n\n\nModel info extractor\n\n\nThe model info extractor class which will be used for further fine-grain control of variant effect prediction. It encapsulates functionality to extract model configuration and setup, such as input sequence length of the model, ability to handle reverse complement sequences and much more. It is instaciated as follows and its objects are then used as arguments to other functions and classes:\n\n\nfrom kipoi.postprocessing.variant_effects import ModelInfoExtractor\nmodel_info = ModelInfoExtractor(model, Dataloader)\n\n\n\n\nVariant-centered effect prediction\n\n\nIn the above example the regions were defined by the dataloader arguments, but if the dataloader supports bed file input (see dataloader.yaml definition for variant effect prediction) then the \nSnvCenteredRg\n class can generate a temporary bed file using a VCF and information on the required input sequence length from the model.yaml which is extracted by the \nModelInfoExtractor\n instance \nmodel_info\n:\n\n\nfrom kipoi.postprocessing.variant_effects import SnvCenteredRg\nvcf_to_region = SnvCenteredRg(model_info)\n\n\n\n\nThe resulting \nvcf_to_region\n object can then be used as the \nvcf_to_region\n argument when calling \npredict_snvs\n.\n\n\nRestricted variant-centered effect prediction\n\n\nThis funcionality is similar to variant-centered effect prediction - the only difference is that this function is designed for models that can't predict on arbitrary regions of the genome, but only in certain regions of the genome. If those regions can be defined in a bed file (further on called 'restriction-bed' file) then this approach can be used. Variant effect prediction will then intersect the VCF with the restriction-bed and generate another bed file that is then passed on to the dataloader. Regions in the restriction-bed file may be larger than the input sequence lenght, in that case the generated seuqence will be centered on the variant position as much as possible - restricted by what is defined in the restrictions-bed file. The \nSnvPosRestrictedRg\n class can generate a temporary bed file using a VCF, the restrictions-bed file (\nrestricted_regions_fpath\n in the example below) and information on the required input sequence length from the model.yaml which is extracted by the \nModelInfoExtractor\n instance \nmodel_info\n:\n\n\nfrom kipoi.postprocessing.variant_effects import SnvPosRestrictedRg\nimport pybedtools as pb\n\npbd = pb.BedTool(restricted_regions_fpath)\nvcf_to_region = SnvPosRestrictedRg(model_info, pbd)\n\n\n\n\nThe resulting \nvcf_to_region\n object can then be used as the \nvcf_to_region\n argument when calling \npredict_snvs\n.\n\n\nScoring functions\n\n\nScoring functions perform calculations on the model predictions for the reference and alternative sequences. Default scoring functions are: \nLogit\n, \nLogitAlt\n, \nLogitRef\n, \nDiff\n, \nDeepSEA_effect\n. These functions are described in more detail in the variant effect prediction pages.\n\n\nThese and custom scoring functions can be used by setting the \nevaluation_function_kwargs\n accoringly, for example:\n\n{'diff_types': {'diff': Diff()}}\n which was used before defines that \ndiff_types\n, a.k.a. scoring function contains a dictionary with scoring function labels as keys and callable scoring functions as values. In the example above only the scoring function \nDiff\n will be used, which is initialised with its default parameters. It is assigned the label \ndiff\n. This label is then used in the annotated VCF and in the \npredict_snvs\n return dictionary, details see below. If a custom scoring function should be defined it has to be a subclass of \nRc_merging_pred_analysis\n.\n\n\nFine-tuning scoring functions\n\n\nThe default scoring functions (\nLogit\n, \nLogitAlt\n, \nLogitRef\n, \nDiff\n, \nDeepSEA_effect\n) offer different options on how the forward and the reverse complement sequences are merged together. They have an \nrc_merging\n argument which can be \n\"min\"\n, \n\"max\"\n, \n\"mean\"\n, \n\"median\"\n or \n\"absmax\"\n. So if the maximum between forward and reverse complement sequences for the alt-ref prediction differences should be returned, then the \nevaluation_function_kwargs\n argument would be: \n{'diff_types': {'diff': Diff('max')}}\n. By default \n\"mean\"\n is used.\n\n\nSaving mutated sequence sets to a file\n\n\nA feature that is only available in using the python functions is to save the mutated sequence sets in a file. This can be useful for quality control or if a non-deeplearning model outside the model zoo should be run using the same data. For those cases instances of the \nSyncHdf5SeqWriter\n can be used. If they are passed to \npredict_snvs\n as the argument \ngenerated_seq_writer\n then the respective sequences are written to a file. Keep in mind that when defining a \ngenerated_seq_writer\n then no actual effect prediction is performed, but only the reference/alternative sequence sets are generated and saved.\n\n\nReturn predictions\n\n\nBy default effect predicions are not kept in memory, but only written to the output VCF to ensure a low memory profile. By setting the parameter \nreturn_predictions = True\n in \npredict_snvs\n the effect predictions are accumulated in memory and the results are returned as a dictionary of DataFrames, where the keys are the labels of the used scoring functions and the DataFrames have the shape (number effect predictions, number of output tasks of the model).\n\n\nUsing the command line interface\n\n\nSimilar to \nkipoi predict\n variant effect prediction can be run by executing:\n\n\nkipoi postproc score_variants my_model_name \\\n    --dataloader_args '{...}' \\\n    --vcf_path path/to/my_vcf.vcf \\\n    --out_vcf_fpath path/to/my_annotated_vcf.vcf\n\n\n\n\nExceptions are that if the dataloader of the model allows the definition of a bed input file, then the respective field in the \n--dataloader_args\n JSON will be replaced by a bad file that consists in regions that are centered on the variant position. That is, if in the dataloader.yaml file of the respective model the \nbed_input\n flag is set then the respective argument in the \n--dataloader_args\n will be overwritten.\n\n\nWhen using variant effect prediction from the command line and using \n--source dir\n, keep in mind that whatever the path is that you put where \nmy_model_name\n stands in the above command is treated as your model name. Since the annotated VCF INFO tags contain the model name as an identifier, executing \nkipoi postproc score_variants ./ --source dir ...\n will result in an annotated VCF with the model name \".\", which is most probably not desired. For those cases \nkipoi postproc score_variants ...\n should be executed in at least one directory level higher than the one where the model.yaml file lies. Then the command will look similar to this \nkipoi postproc score_variants ./my_model --source dir ...\n and the annotated VCF INFO tags will contain './my_model'.\n\n\nScoring functions\n\n\nScoring functions perform calculations on the model predictions for the reference and alternative sequences. Default scoring functions are: \nlogit\n, \nlogit_alt\n, \nlogit_ref\n, \ndiff\n, \ndeepsea_effect\n. These functions are described in more detail in the variant effect prediction pages.\n\n\nGiven a model is compatible with said scoring functions one or more of those can be selected by using the \n--scoring\n argument, e.g.: \n--scoring diff logit\n. The model.yaml file defines which scoring functions are available for a model, with the exception that the \ndiff\n scoring function is available for all models. In the model.yaml also additional custom scoring functions can be defined, for details on please see the variant effect prediction pages. The labels by which the different scoring functions are made available can also be defined in the model.yaml file using the \nname\n tag.\n\n\nFine-tuning scoring functions\n\n\nScoring functions may have or may even require arguments at instantiation. Those arguments can be passed as JSON dictionaries to scoring functions by using the \n--scoring_kwargs\n argument. If \n--scoring_kwargs\n is used then for every label set in \n--scoring\n there must be a \n--scoring_kwargs\n JSON in the exact same order. If the degault values should be used or no arguments are required then an empty dictionary (\n{}\n) can be used. For example: \n--scoring diff my_scr --scoring_kwargs '{}' '{my_arg:2}'\n will use \ndiff\n with the default parameters and will instantiate \nmy_scr(my_arg=2)\n.\n\n\nThe default scoring functions (\nlogit\n, \nlogit_alt\n, \nlogit_ref\n, \ndiff\n, \ndeepsea_effect\n) offer different options on how the forward and the reverse complement sequences are merged together. They have an \nrc_merging\n argument which can be \n\"min\"\n, \n\"max\"\n, \n\"mean\"\n, \n\"median\"\n or \n\"absmax\"\n. So if the maximum between forward and reverse complement sequences for the alt-ref prediction differences should be returned, then the command would be: \n--scoring diff --scoring_kwargs '{rc_merging:\"max\"}'\n. By default \nrc_merging\n is set to \n\"mean\"\n.",
            "title": "Variant effect predictions"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#using-variant-effect-prediction",
            "text": "This chapter describes how to run variant prediction using a model in the zoo either using the python functionality or using the command line. A prerequesite is that the model is compatible with variant effect prediction (see: Variant effect prediction prerequesites for the model.yaml and dataloader.yaml)",
            "title": "Using variant effect prediction"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#variant-effect-prediction-in-python",
            "text": "Using variant effect prediction within python allows more flexibility in the finegrain details compared to using the command line interface. The core function of variant effect prediction is  predict_snvs , which on the one hand requires a model with its dataloader as well as a valid VCF.  The easiest way to run variant effect prediction is the following:  import kipoi.postprocessing.variant_effects.snv_predict as sp\nfrom kipoi.postprocessing.variant_effects import VcfWriter\nfrom kipoi.postprocessing.variant_effects import Diff\n\nmodel = kipoi.get_model(\"my_model_name\")\n\nDataloader = kipoi.get_dataloader_factory(model_dir, source=\"dir\")\n\ndataloader_arguments = {...}\n\nvcf_path = \"path/to/my_vcf.vcf\"\nout_vcf_fpath = \"path/to/my_annotated_vcf.vcf\"\n\nwriter = VcfWriter(model, vcf_path, out_vcf_fpath)\n\nsp.predict_snvs(model, Dataloader, vcf_path, batch_size = 32,\n                      dataloader_args=dataloader_arguments,\n                      evaluation_function_kwargs={'diff_types': {'diff': Diff()}},\n                      sync_pred_writer=writer)  Where  model  is a kipoi model - replace  my_model_name  by a valid model name and  Dataloader  is a dataloader factory that fits to the model.  dataloader_arguments  contains all the kwargs that are necessary to run the dataloader. In  vcf_path  the path to a valid VCF has to be set. Coordinates have to match the genome / assembly etc. of the raw input files used by the dataloader. In order to generate an output VCF the  VcfWriter  class is used. For it work it needs to be initialised with the model object, the inpput VCF path and the path where the annotated VCF should be stored. The last missing element that has to be set is the scoring function. This is the function that defines how to compare reference with alternative predictions of the model. For details on the different scoring methods please take a look at the detailed explanation of variant effect prediction or the API defintion.  The above code will run the dataloader based with  dataloader_arguments  and try to overlap the input VCF with the sequences generated by the dataloader. Then the predicitons are compared using whichever scoring functions ae defined in  evaluation_function_kwargs . The results are then stored in the output VCF using the  writer  object.",
            "title": "Variant effect prediction in python"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#model-info-extractor",
            "text": "The model info extractor class which will be used for further fine-grain control of variant effect prediction. It encapsulates functionality to extract model configuration and setup, such as input sequence length of the model, ability to handle reverse complement sequences and much more. It is instaciated as follows and its objects are then used as arguments to other functions and classes:  from kipoi.postprocessing.variant_effects import ModelInfoExtractor\nmodel_info = ModelInfoExtractor(model, Dataloader)",
            "title": "Model info extractor"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#variant-centered-effect-prediction",
            "text": "In the above example the regions were defined by the dataloader arguments, but if the dataloader supports bed file input (see dataloader.yaml definition for variant effect prediction) then the  SnvCenteredRg  class can generate a temporary bed file using a VCF and information on the required input sequence length from the model.yaml which is extracted by the  ModelInfoExtractor  instance  model_info :  from kipoi.postprocessing.variant_effects import SnvCenteredRg\nvcf_to_region = SnvCenteredRg(model_info)  The resulting  vcf_to_region  object can then be used as the  vcf_to_region  argument when calling  predict_snvs .",
            "title": "Variant-centered effect prediction"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#restricted-variant-centered-effect-prediction",
            "text": "This funcionality is similar to variant-centered effect prediction - the only difference is that this function is designed for models that can't predict on arbitrary regions of the genome, but only in certain regions of the genome. If those regions can be defined in a bed file (further on called 'restriction-bed' file) then this approach can be used. Variant effect prediction will then intersect the VCF with the restriction-bed and generate another bed file that is then passed on to the dataloader. Regions in the restriction-bed file may be larger than the input sequence lenght, in that case the generated seuqence will be centered on the variant position as much as possible - restricted by what is defined in the restrictions-bed file. The  SnvPosRestrictedRg  class can generate a temporary bed file using a VCF, the restrictions-bed file ( restricted_regions_fpath  in the example below) and information on the required input sequence length from the model.yaml which is extracted by the  ModelInfoExtractor  instance  model_info :  from kipoi.postprocessing.variant_effects import SnvPosRestrictedRg\nimport pybedtools as pb\n\npbd = pb.BedTool(restricted_regions_fpath)\nvcf_to_region = SnvPosRestrictedRg(model_info, pbd)  The resulting  vcf_to_region  object can then be used as the  vcf_to_region  argument when calling  predict_snvs .",
            "title": "Restricted variant-centered effect prediction"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#scoring-functions",
            "text": "Scoring functions perform calculations on the model predictions for the reference and alternative sequences. Default scoring functions are:  Logit ,  LogitAlt ,  LogitRef ,  Diff ,  DeepSEA_effect . These functions are described in more detail in the variant effect prediction pages.  These and custom scoring functions can be used by setting the  evaluation_function_kwargs  accoringly, for example: {'diff_types': {'diff': Diff()}}  which was used before defines that  diff_types , a.k.a. scoring function contains a dictionary with scoring function labels as keys and callable scoring functions as values. In the example above only the scoring function  Diff  will be used, which is initialised with its default parameters. It is assigned the label  diff . This label is then used in the annotated VCF and in the  predict_snvs  return dictionary, details see below. If a custom scoring function should be defined it has to be a subclass of  Rc_merging_pred_analysis .",
            "title": "Scoring functions"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#fine-tuning-scoring-functions",
            "text": "The default scoring functions ( Logit ,  LogitAlt ,  LogitRef ,  Diff ,  DeepSEA_effect ) offer different options on how the forward and the reverse complement sequences are merged together. They have an  rc_merging  argument which can be  \"min\" ,  \"max\" ,  \"mean\" ,  \"median\"  or  \"absmax\" . So if the maximum between forward and reverse complement sequences for the alt-ref prediction differences should be returned, then the  evaluation_function_kwargs  argument would be:  {'diff_types': {'diff': Diff('max')}} . By default  \"mean\"  is used.",
            "title": "Fine-tuning scoring functions"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#saving-mutated-sequence-sets-to-a-file",
            "text": "A feature that is only available in using the python functions is to save the mutated sequence sets in a file. This can be useful for quality control or if a non-deeplearning model outside the model zoo should be run using the same data. For those cases instances of the  SyncHdf5SeqWriter  can be used. If they are passed to  predict_snvs  as the argument  generated_seq_writer  then the respective sequences are written to a file. Keep in mind that when defining a  generated_seq_writer  then no actual effect prediction is performed, but only the reference/alternative sequence sets are generated and saved.",
            "title": "Saving mutated sequence sets to a file"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#return-predictions",
            "text": "By default effect predicions are not kept in memory, but only written to the output VCF to ensure a low memory profile. By setting the parameter  return_predictions = True  in  predict_snvs  the effect predictions are accumulated in memory and the results are returned as a dictionary of DataFrames, where the keys are the labels of the used scoring functions and the DataFrames have the shape (number effect predictions, number of output tasks of the model).",
            "title": "Return predictions"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#using-the-command-line-interface",
            "text": "Similar to  kipoi predict  variant effect prediction can be run by executing:  kipoi postproc score_variants my_model_name \\\n    --dataloader_args '{...}' \\\n    --vcf_path path/to/my_vcf.vcf \\\n    --out_vcf_fpath path/to/my_annotated_vcf.vcf  Exceptions are that if the dataloader of the model allows the definition of a bed input file, then the respective field in the  --dataloader_args  JSON will be replaced by a bad file that consists in regions that are centered on the variant position. That is, if in the dataloader.yaml file of the respective model the  bed_input  flag is set then the respective argument in the  --dataloader_args  will be overwritten.  When using variant effect prediction from the command line and using  --source dir , keep in mind that whatever the path is that you put where  my_model_name  stands in the above command is treated as your model name. Since the annotated VCF INFO tags contain the model name as an identifier, executing  kipoi postproc score_variants ./ --source dir ...  will result in an annotated VCF with the model name \".\", which is most probably not desired. For those cases  kipoi postproc score_variants ...  should be executed in at least one directory level higher than the one where the model.yaml file lies. Then the command will look similar to this  kipoi postproc score_variants ./my_model --source dir ...  and the annotated VCF INFO tags will contain './my_model'.",
            "title": "Using the command line interface"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#scoring-functions_1",
            "text": "Scoring functions perform calculations on the model predictions for the reference and alternative sequences. Default scoring functions are:  logit ,  logit_alt ,  logit_ref ,  diff ,  deepsea_effect . These functions are described in more detail in the variant effect prediction pages.  Given a model is compatible with said scoring functions one or more of those can be selected by using the  --scoring  argument, e.g.:  --scoring diff logit . The model.yaml file defines which scoring functions are available for a model, with the exception that the  diff  scoring function is available for all models. In the model.yaml also additional custom scoring functions can be defined, for details on please see the variant effect prediction pages. The labels by which the different scoring functions are made available can also be defined in the model.yaml file using the  name  tag.",
            "title": "Scoring functions"
        },
        {
            "location": "/using/02_Variant_effect_prediction/#fine-tuning-scoring-functions_1",
            "text": "Scoring functions may have or may even require arguments at instantiation. Those arguments can be passed as JSON dictionaries to scoring functions by using the  --scoring_kwargs  argument. If  --scoring_kwargs  is used then for every label set in  --scoring  there must be a  --scoring_kwargs  JSON in the exact same order. If the degault values should be used or no arguments are required then an empty dictionary ( {} ) can be used. For example:  --scoring diff my_scr --scoring_kwargs '{}' '{my_arg:2}'  will use  diff  with the default parameters and will instantiate  my_scr(my_arg=2) .  The default scoring functions ( logit ,  logit_alt ,  logit_ref ,  diff ,  deepsea_effect ) offer different options on how the forward and the reverse complement sequences are merged together. They have an  rc_merging  argument which can be  \"min\" ,  \"max\" ,  \"mean\" ,  \"median\"  or  \"absmax\" . So if the maximum between forward and reverse complement sequences for the alt-ref prediction differences should be returned, then the command would be:  --scoring diff --scoring_kwargs '{rc_merging:\"max\"}' . By default  rc_merging  is set to  \"mean\" .",
            "title": "Fine-tuning scoring functions"
        },
        {
            "location": "/using/03_Model_sources/",
            "text": "~/.kipoi/config.yaml\n\n\nkipoi\n package has a config file located at \n~/.kipoi/config.yaml\n. By default, it will look like this (without comments):\n\n\nmodel_sources:\n  kipoi: # source name \n    type: git-lfs  # git repository with large file storage (lfs)\n    remote_url: git@github.com:kipoi/models.git  # git remote\n    local_path: /home/avsec/.kipoi/models/ # local storage path\n\n\n\n\nmodel_sources\n defines all the places where kipoi will search for models and pull them to a local directory.\nBy default, it contains the model-zoo from \ngithub.com/kipoi/models\n. It is not a normal git repository,\nsince bigger files are stored with \ngit large file storage (git-lfs)\n.\nThis repository will be stored locally under \nlocal_path\n. Advantage of using \ngit-lfs\n is that only the files tracked by \ngit\n will be downloaded first. Larger files stored in \ngit-lfs\n will be downloaded individually for each model upon request (say when a user invokes a \nkipoi predict\n command).\n\n\nAll possible model source types\n\n\nIn addition to the default \nkipoi\n source, you can modify \n~/.kipoi/config.yaml\n and add additional (private or public) model sources. Available model source types are:\n\n\n\n\ngit-lfs\n - As for \nkipoi\n model source. Model weights will get downloaded upon request (say when running \nkipoi predict\n).\n\n\ngit\n - Normal git repository, all the files will be downloaded on checkout.\n\n\nlocal\n - Local directory.\n\n\n\n\nExample:\n\n\nmodel_sources:\n  kipoi:\n    type: git-lfs\n    remote_url: git@github.com:kipoi/models.git\n    local_path: /home/avsec/.kipoi/models/\n\n  my_git_models:\n    type: git\n    remote_url: git@github.com:asd/other_models.git\n    local_path: ~/.kipoi/other_models/\n\n  my_local_models:\n    type: local\n    local_path: /data/mymodels/\n\n\n\n\nAbout model definition\n\n\nA particular model is defined by its source (key under \nmodel_sources\n, say \nkipoi\n) and the relative path of the desired model directory from the model source root (say \nrbp_eclip/UPF1\n).\n\n\nA directory is considered a model if it contains a \nmodel.yaml\n file.",
            "title": "Private and public model sources"
        },
        {
            "location": "/using/03_Model_sources/#kipoiconfigyaml",
            "text": "kipoi  package has a config file located at  ~/.kipoi/config.yaml . By default, it will look like this (without comments):  model_sources:\n  kipoi: # source name \n    type: git-lfs  # git repository with large file storage (lfs)\n    remote_url: git@github.com:kipoi/models.git  # git remote\n    local_path: /home/avsec/.kipoi/models/ # local storage path  model_sources  defines all the places where kipoi will search for models and pull them to a local directory.\nBy default, it contains the model-zoo from  github.com/kipoi/models . It is not a normal git repository,\nsince bigger files are stored with  git large file storage (git-lfs) .\nThis repository will be stored locally under  local_path . Advantage of using  git-lfs  is that only the files tracked by  git  will be downloaded first. Larger files stored in  git-lfs  will be downloaded individually for each model upon request (say when a user invokes a  kipoi predict  command).",
            "title": "~/.kipoi/config.yaml"
        },
        {
            "location": "/using/03_Model_sources/#all-possible-model-source-types",
            "text": "In addition to the default  kipoi  source, you can modify  ~/.kipoi/config.yaml  and add additional (private or public) model sources. Available model source types are:   git-lfs  - As for  kipoi  model source. Model weights will get downloaded upon request (say when running  kipoi predict ).  git  - Normal git repository, all the files will be downloaded on checkout.  local  - Local directory.   Example:  model_sources:\n  kipoi:\n    type: git-lfs\n    remote_url: git@github.com:kipoi/models.git\n    local_path: /home/avsec/.kipoi/models/\n\n  my_git_models:\n    type: git\n    remote_url: git@github.com:asd/other_models.git\n    local_path: ~/.kipoi/other_models/\n\n  my_local_models:\n    type: local\n    local_path: /data/mymodels/",
            "title": "All possible model source types"
        },
        {
            "location": "/using/03_Model_sources/#about-model-definition",
            "text": "A particular model is defined by its source (key under  model_sources , say  kipoi ) and the relative path of the desired model directory from the model source root (say  rbp_eclip/UPF1 ).  A directory is considered a model if it contains a  model.yaml  file.",
            "title": "About model definition"
        },
        {
            "location": "/using/04_Installing_on_OSX/",
            "text": "Using Kipoi - Installing on OSX\n\n\nDepending on the versino of OSX you are using there is python pre-installed or not. On OSX Sierra it is not, but on OSX High Sierra it is.\nFor Kipoi to work fully you will need a version of python (2.7, 3.5 or 3.6) installed, preferably you will also have an installation of conda.\nWe have seen problems when conda environments were re-used so we strongly recommend that you create a new environment e.g. \nkipoi\n where you\ninstall Kipoi.\n\n\nSteps\n\n\nMake sure you have python installed:\n\n\nYou can try by just execting \npython\n in your Terminal, if nothing is found you will want to install python (not \npythonw\n).\nThere are some good explanations on how \npython 2 can be installed on OSX Sierra\n\nand if you are using High Sierra and you prefer python 3 you can \nfollow this\n.\n\n\nAfter completing the steps and installing \nconda or miniconda\n please procede as described in \ngetting started\n.",
            "title": "Installing on OSX"
        },
        {
            "location": "/using/04_Installing_on_OSX/#using-kipoi-installing-on-osx",
            "text": "Depending on the versino of OSX you are using there is python pre-installed or not. On OSX Sierra it is not, but on OSX High Sierra it is.\nFor Kipoi to work fully you will need a version of python (2.7, 3.5 or 3.6) installed, preferably you will also have an installation of conda.\nWe have seen problems when conda environments were re-used so we strongly recommend that you create a new environment e.g.  kipoi  where you\ninstall Kipoi.",
            "title": "Using Kipoi - Installing on OSX"
        },
        {
            "location": "/using/04_Installing_on_OSX/#steps",
            "text": "",
            "title": "Steps"
        },
        {
            "location": "/using/04_Installing_on_OSX/#make-sure-you-have-python-installed",
            "text": "You can try by just execting  python  in your Terminal, if nothing is found you will want to install python (not  pythonw ).\nThere are some good explanations on how  python 2 can be installed on OSX Sierra \nand if you are using High Sierra and you prefer python 3 you can  follow this .  After completing the steps and installing  conda or miniconda  please procede as described in  getting started .",
            "title": "Make sure you have python installed:"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/",
            "text": "Variant effect prediction\n\n\nVariant effect prediction offers a simple way predict effects of SNVs using any model that uses DNA sequence as an input. Many different scoring methods can be chosen but the principle relies on in-silico mutagenesis (see below). The default input is a VCF and the default output again is a VCF annotated with predictions of variant effects.\n\n\nHow it works\n\n\nThis sketch highlights the overall functionality of variant effect prediction. More details are given in the chapters below.\n\n\n\n\nDataloader output and a VCF are overlapped and the input DNA sequence is mutated as defined in the VCF. The reference and the alternative set of model inputs is predicted using the model and the differences are evaluated using a scoring function. The results are then stored in an annotated VCF.\n\n\nIn-silico mutagenesis\n\n\nThe principle relies on generating model predictions twice, once with DNA sequence that contains the reference and once with the alternative allele of a variant. Those predictions can then be compared in different ways to generate an effect prediction.\n\n\nScoring methods\n\n\nScoring methods that come with Kipoi are \nDiff\n which simply calculates the difference between the two predictions, \nLogit\n which calculates the difference of \nlogit(prediction)\n of the two predictions and a few more. Those scoring methods can also be user-defined in which case they can be submitted with the model. Not all scoring functions are compatible with all model possible model outputs - for example the logit transformation can only be performed on values [0,1].\n\n\nModel and dataloader requirements\n\n\nThe model has to produce predictions at least partly based on DNA sequence and the DNA sequence either has to be as a string (e.g. \nactgACTG\n) or in a 1-hot encoded way in which A = \n[1,0,0,0]\n, C = \n[0,1,0,0]\n, T= \n[0,0,1,0]\n, G= \n[0,0,0,1]\n. Please note that any letter/base that is not in \nactgACTG\n will be regarded and treated as \nN\n (in one-hot: \n[0,0,0,0]\n)!\n\n\nRequirements for the dataloader are that apart from producing the model input it also has to output information which region of the genome this generated sequence corresponds. On a side note: This region is only used to calculate an overlap with the query VCF, hence as long the dataloader output refers to the same sequence assembly as the VCF file variant scoring will return the desired results.\n\n\nSetting up the model.yaml\n\n\nIn order to indicate that a model is compatible with Kipoi postprocessing the definition of \npostprocessing\n in the model.yaml file is necessary. The postprocessing section can then mention multiple different ways to interpret a model. Here we will discuss variant effect prediction, a sample section of the model.yaml can look like this:\n\n\npostprocessing:\n    variant_effects:\n      seq_input:\n        - seq\n      use_rc: seq_only\n\n\n\n\nThis defines that the current model is capable to be used for variant effect prediction (\nvariant_effects\n) and it defines that \nseq\n is the name of the model input that contains DNA sequence, which can be mutated and used for effect prediction. \nseq_input\n is a mandatory field and variant effect prediction can only be executed if there is at least one model input defined in \nseq_input\n. For some models it is necessary that also reverse-complements of DNA sequences are tested / predicted. To indicate that this is the case for the current model add the optional flag \nuse_rc: seq_only\n. Using \nseq_only\n will reverse-complement \nonly\n the model inputs that are defined in \nseq_input\n. Any other model input will remain untouched and exactly the same input will be fed to the model input as for the \"forward\" version of the model input.\n\n\nAs mentioned above the DNA sequence input may either be a string or 1-hot encoded. To indicate which format is used the \nspecial_type\n flag is used. The model input may then look like this:\n\n\nschema:\n    inputs:\n        seq:\n            shape: (101, 4)\n            special_type: DNASeq\n            doc: One-hot encoded RNA sequence\n\n\n\n\nHere a one-hot encoded sequence (\nDNASeq\n) is expected to be the model input. Note that the model input label (here: \nseq\n) was used before in the \npostprocessing\n section and the same label is expected to be exist in the dataloader output.\n\n\nThe \nspecial_type\n flag for using string input sequences is: \nDNAStringSeq\n. So the following snippet of a model.yaml file\n\n\nschema:\n    inputs:\n        seq:\n            shape: ()\n            special_type: DNAStringSeq\n            doc: RNA sequence as a string\n\n\n\n\nindicates that a single sample of \nseq\n is \nnp.array(string)\n where \nstring\n is a python string.\n\n\nIf \nspecial_type\n is not defined for a model input, but it is used in \nseq_input\n in the \npostprocessing\n section, then by default Kipoi expects one-hot encoded DNA sequences.\n\n\nSetting up the dataloader.yaml\n\n\nSimilar to the model.yaml also \ndataloader.yaml\nhas to have a \npostprocessing\n section defined to indicate that it is compatible with variant effect prediction. As a bare minimum the following has to be defined:\n\n\npostprocessing:\n    variant_effects:\n\n\n\n\nAnd equally important every DNA sequence input of a model (here \nseq\n) has to have an associated metadata tag, which could like follows:\n\n\noutput_schema:\n    inputs:\n        seq:\n            shape: (101, 4)\n            special_type: DNASeq\n            doc: One-hot encoded RNA sequence\n            associated_metadata: ranges\n        some_other_input:\n            shape: (1, 10)\n            doc: Some description\n    metadata:\n        ranges:\n            type: GenomicRanges\n            doc: Ranges describing inputs.seq\n\n\n\n\nHere the \nassociated_metadata\n flag in the input field \nseq\n is set to \nranges\n, which means that for every sample in the \nmodel_input['inputs']['seq']\n one entry in \nmodel_input['metadata']['ranges']\n is expected with its type either being \nGenomicRanges\n or a dictionary of numpy arrays with the keys \nchr\n, \nstart\n, \nend\n, \nid\n. The information in the metadata object gives variant effect prediction the possibilty to find the relative position of a variant within a given input sequence. Hence the \nassociated_metadata\n is mandatory for every  entry in \nseq_input\n in the model.yaml file. Please note that the coordinates in the metadata are expected to be 0-based, hence comply with .bed file format!\n\n\nThe following sketch gives an overview how the different tags play together and how they are used with variant effect prediction.\n\n\n\n\nUse-cases\n\n\nThis section describes a set of functions which cover most of the common queries for variant effect. All of the functions described below require that the model.yaml and dataloader.yaml files are set up in the way defined above.\n\n\nIn literature in-silico mutagenesis-based variant effect predcition is performed in a variant centric way: Starting from a VCF for every variant a sequence centered on said variant is generated. That sequence is then mutated by modifying the central base and setting it to what is defined as reference or alternative allele, generating two sets of sequences. For both the set with the reference allele in the center and the alternative allele in the center the model prediction is run and model outputs are compared.\n\n\nNot all models can predict on aribrary DNA sequences from any region of the genome. Splicing models may for example only be trained on regions surrounding a splice site, hence the variant-centered approach from before will not work. Therefore two more options to run variant effect predicion are offered: restricted variant centered effect prediction and overlap-based effect prediction.\n\n\nVariant effect prediction will try to use variant-centered approaches whenever the \nbed_input\n flag is defined in dataloader.yaml (see below). Otherwise the overlap-based effect prediction is used. This is because the variant centered approach is generally faster and for every variant in the VCF one single prediction can be made (assuming the position of variant is in a valid genomic region).\n\n\nFor all the methods described below it is essential that genomic coordinates in the VCF and the coordinates used by the dataloader are for the same genome / assembly /etc.\n\n\nVariant centered effect prediction\n\n\nIn order to use variant centered effect prediction the dataloader must accept an input bed file based on which it will produce model input. Furthermore the dataloader is required to return the name values (fourth column) of the input bed file in the \nid\n field of \nmodel_input['metadata']['ranges']\n. Additionally the order of samples has to be identical with the order of regions in the input bed file, but regions may be skipped.\n\n\nIn order for the variant effect prediction to know which input argument of the dataloader is accepts a bed file three additional lines in dataloader.yaml are necessary, e.g:\n\n\npostprocessing:\n    variant_effects:\n      bed_input:\n        - intervals_file\n\n\n\n\nThis section indicates that the dataloader function has an argument \nintervals_file\n which accepts a bed file path as input which may be used.\n\n\n\n\nRestricted-variant centered effect prediction\n\n\nRequirements for the dataloader and dataloader.yaml here are identical to the variant centered effect prediction. The only difference is that this function is designed for models that can't predict on arbitrary regions of the genome, but only in certain regions of the genome. If those regions can be defined in a bed file (further on called 'restriction-bed' file) then this approach can be used. Variant effect prediction will then intersect the VCF with the restriction-bed and generate another bed file that is then passed on to the dataloader.\n\n\nRegions in the restriction-bed file may be larger than the input sequence lenght, in that case the generated seuqence will be centered on the variant position as much as possible - restricted by what is defined in the restrictions-bed file.\n\n\n\nOverlap-based effect prediction\n\n\nIf the dataloader does not support bed input files then variant effect predictions can be run by the overlap of a VCF with the regions defined in the metdata output of the dataloader.\n\n\nIf multiple variants overlap with a region then the effect will be predicted inpendently for those variants. If multiple (e.g.: two) model input samples overlap with one variant then the output will contain as many predictions as there were independent overlaps of metadata ranges and variants (e.g.: two).\n\n\n\nScoring functions\n\n\nAfter mutating the model input DNA sequences predictions are created using the models and those predictions then have to compared by scoring methods. Not all scoring methods are compatible with all models depending on the output data range of the model (see below). The compatibility of a scoring function with a given model can be indicated by setting \nscoring_functions\n in model.yaml:\n\n\npostprocessing:\n   variant_effects\n      seq_input:\n        - seq\n      scoring_functions:\n        - name: diff\n          type: diff\n        - type: logit\n          default: true\n\n\n\n\nThe scoring function is identified by the \ntype\n field in \nscoring_functions\n which is the only mandatory field. Allowed values for the \ntype\n field are: \ndiff\n, \nlogit\n, \ndeepsea_effect\n and \ncustom\n.\n\n\nSetting \ndefault:true\n for a scoring function indicates that that respective scoring function is executed by variant effect prediction if none is selected by the used on execution time. If multiple scoring functions have set \ndefault:true\n then all of those will be run by default. If \ndefault:true\n is not set for any scoring function defined in \nscoring_functions\n then all entries in \nscoring_functions\n will be run by default.\n\n\nScoring functions can be assigned a different name with the \nname\n flag by which they are then selected using the command line interface. In general it is not advisable to rename the scoring functions that come with Kipoi.\n\n\nDiff\n\n\nThe simplest scoring method is to calculate the difference between predictions for the reference and the alternative allele: \nprediction(alt) - prediction(ref)\n. This scoring method is available for all models no matter if it is defined in \nscoring_functions\n or not.\n\n\nLogit\n\n\nCalculates the difference of logit-transformed values of the predictions:\n\nlogit(prediction(alt)) - logit(prediction(ref))\n. This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1].\n\n\nLogitAlt\n\n\nReturns the logits transformed predictions for the sequences carrying the alternative allele:\n\nlogit(prediction(alt))\n. This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1].\n\n\nLogitRef\n\n\nReturns the logits transformed predictions for the sequences carrying the reference allele:\n\nlogit(prediction(ref))\n. This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1].\n\n\nDeepsea_effect\n\n\nCalculates the variant scores as defined in the publication of the DeepSEA model (Troyanskaya et al., 2015) by using the absolute value of the logit difference and diff values multiplied together: \nabs(Logit * Diff)\n with \nLogit\n and \nDiff\n defined as above.\n\n\nCustom\n\n\nCustom scoring methods can be defined and shipped with the models. In that case the model.yaml will look similar to this:\n\n\npostprocessing:\n  variant_effects:\n    seq_input:\n      - seq\n    scoring_functions:\n      - name: my_scores\n        type: custom\n        defined_as: postproc.py::myfun\n        args:\n          first_arg:\n            doc: Description of the first argument\n            default: 1\n\n\n\n\nNotice that the selection of \ntype: custom\n requires that \ndefined_as\n is set. The value \npostproc.py::myfun\n indicates that the callable python object \nmyfun\n is stored in a file called \npostproc.py\n. When executing variant effect prediction in the command line the scoring function can be chosen by it's name - which in this case is: \nmy_scores\n.\n\n\nAll scoring functions are subclasses of \nRc_merging_pred_analysis\n this means that also a custom scoring function must inherit from it.\n\n\nOutput\n\n\nThe output of variant effect prediction is by default stored in a VCF that is derived from the input VCF. The output VCF only contains variants for which a effect prediction could be generated (e.g. if no model input sample overlapped a variant no prediction could be made for it). The predictions themselves are stored in the INFO field of the VCF, with the ID starting with \nKPVEP\n and containing the name of the model. Additional to the predictions themselves a also a region ID will be stored in a second INFO field. The region IDs are the values stored in \nmodel_input['metadata']['ranges']['id']\n given to a sequence sample generated by the dataloader. This way it is possible to trace back which sequence was mutated by which variant in order to produce a certain effect prediction\n\n\nSince multiple seqeunces generated by the dataloader may overlap one variant - especially when using the overlap-based effect prediction - it is possible that the generated VCF output will contain a variant multiple times, but the different predictions will be destinguishable by their region ID.\n\n\nIf variant effect prediction is run programmatically in python then the results are returned as a dictionary of pandas DataFrames.\n\n\nMore complex models\n\n\nMore complex models may have more than only one DNA sequence input, it may even be that models have DNA sequence inputs taken from different regions of the genome within one sample in a batch. See this sketch for an illustration of the scenario:\n\n\n\n\nThe dataloader has three sequence outputs which are linked to two metadata ranges. for both ranges objects the beginning of the ranges is displayed. In order to overlap the metadata ranges with variants the input batch is processed one sample at a time. The samples in a batch are displayed in green rectangular boxes: For every sample all the ranges are assembled and overlapped with variants in the VCF. Then the effect is predicted for every single variant in the VCF that overlaps at least one of the region defined in that sample. This means that for the first sample in the batch two variants are investigated: rs1 and rs2. rs1 can only affect seq1a and seq1b, hence those two sequences are mutated, seq2 is not. rs2 overlaps with both ranges in the first sample and hence two sequences are mutated with rs2 to predict its effect. This means that the first sample will be evaluated twice using variants rs1 and rs2, and the second sample only once using rs3.",
            "title": "Variant effect prediction"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#variant-effect-prediction",
            "text": "Variant effect prediction offers a simple way predict effects of SNVs using any model that uses DNA sequence as an input. Many different scoring methods can be chosen but the principle relies on in-silico mutagenesis (see below). The default input is a VCF and the default output again is a VCF annotated with predictions of variant effects.",
            "title": "Variant effect prediction"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#how-it-works",
            "text": "This sketch highlights the overall functionality of variant effect prediction. More details are given in the chapters below.   Dataloader output and a VCF are overlapped and the input DNA sequence is mutated as defined in the VCF. The reference and the alternative set of model inputs is predicted using the model and the differences are evaluated using a scoring function. The results are then stored in an annotated VCF.",
            "title": "How it works"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#in-silico-mutagenesis",
            "text": "The principle relies on generating model predictions twice, once with DNA sequence that contains the reference and once with the alternative allele of a variant. Those predictions can then be compared in different ways to generate an effect prediction.",
            "title": "In-silico mutagenesis"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#scoring-methods",
            "text": "Scoring methods that come with Kipoi are  Diff  which simply calculates the difference between the two predictions,  Logit  which calculates the difference of  logit(prediction)  of the two predictions and a few more. Those scoring methods can also be user-defined in which case they can be submitted with the model. Not all scoring functions are compatible with all model possible model outputs - for example the logit transformation can only be performed on values [0,1].",
            "title": "Scoring methods"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#model-and-dataloader-requirements",
            "text": "The model has to produce predictions at least partly based on DNA sequence and the DNA sequence either has to be as a string (e.g.  actgACTG ) or in a 1-hot encoded way in which A =  [1,0,0,0] , C =  [0,1,0,0] , T=  [0,0,1,0] , G=  [0,0,0,1] . Please note that any letter/base that is not in  actgACTG  will be regarded and treated as  N  (in one-hot:  [0,0,0,0] )!  Requirements for the dataloader are that apart from producing the model input it also has to output information which region of the genome this generated sequence corresponds. On a side note: This region is only used to calculate an overlap with the query VCF, hence as long the dataloader output refers to the same sequence assembly as the VCF file variant scoring will return the desired results.",
            "title": "Model and dataloader requirements"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#setting-up-the-modelyaml",
            "text": "In order to indicate that a model is compatible with Kipoi postprocessing the definition of  postprocessing  in the model.yaml file is necessary. The postprocessing section can then mention multiple different ways to interpret a model. Here we will discuss variant effect prediction, a sample section of the model.yaml can look like this:  postprocessing:\n    variant_effects:\n      seq_input:\n        - seq\n      use_rc: seq_only  This defines that the current model is capable to be used for variant effect prediction ( variant_effects ) and it defines that  seq  is the name of the model input that contains DNA sequence, which can be mutated and used for effect prediction.  seq_input  is a mandatory field and variant effect prediction can only be executed if there is at least one model input defined in  seq_input . For some models it is necessary that also reverse-complements of DNA sequences are tested / predicted. To indicate that this is the case for the current model add the optional flag  use_rc: seq_only . Using  seq_only  will reverse-complement  only  the model inputs that are defined in  seq_input . Any other model input will remain untouched and exactly the same input will be fed to the model input as for the \"forward\" version of the model input.  As mentioned above the DNA sequence input may either be a string or 1-hot encoded. To indicate which format is used the  special_type  flag is used. The model input may then look like this:  schema:\n    inputs:\n        seq:\n            shape: (101, 4)\n            special_type: DNASeq\n            doc: One-hot encoded RNA sequence  Here a one-hot encoded sequence ( DNASeq ) is expected to be the model input. Note that the model input label (here:  seq ) was used before in the  postprocessing  section and the same label is expected to be exist in the dataloader output.  The  special_type  flag for using string input sequences is:  DNAStringSeq . So the following snippet of a model.yaml file  schema:\n    inputs:\n        seq:\n            shape: ()\n            special_type: DNAStringSeq\n            doc: RNA sequence as a string  indicates that a single sample of  seq  is  np.array(string)  where  string  is a python string.  If  special_type  is not defined for a model input, but it is used in  seq_input  in the  postprocessing  section, then by default Kipoi expects one-hot encoded DNA sequences.",
            "title": "Setting up the model.yaml"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#setting-up-the-dataloaderyaml",
            "text": "Similar to the model.yaml also  dataloader.yaml has to have a  postprocessing  section defined to indicate that it is compatible with variant effect prediction. As a bare minimum the following has to be defined:  postprocessing:\n    variant_effects:  And equally important every DNA sequence input of a model (here  seq ) has to have an associated metadata tag, which could like follows:  output_schema:\n    inputs:\n        seq:\n            shape: (101, 4)\n            special_type: DNASeq\n            doc: One-hot encoded RNA sequence\n            associated_metadata: ranges\n        some_other_input:\n            shape: (1, 10)\n            doc: Some description\n    metadata:\n        ranges:\n            type: GenomicRanges\n            doc: Ranges describing inputs.seq  Here the  associated_metadata  flag in the input field  seq  is set to  ranges , which means that for every sample in the  model_input['inputs']['seq']  one entry in  model_input['metadata']['ranges']  is expected with its type either being  GenomicRanges  or a dictionary of numpy arrays with the keys  chr ,  start ,  end ,  id . The information in the metadata object gives variant effect prediction the possibilty to find the relative position of a variant within a given input sequence. Hence the  associated_metadata  is mandatory for every  entry in  seq_input  in the model.yaml file. Please note that the coordinates in the metadata are expected to be 0-based, hence comply with .bed file format!  The following sketch gives an overview how the different tags play together and how they are used with variant effect prediction.",
            "title": "Setting up the dataloader.yaml"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#use-cases",
            "text": "This section describes a set of functions which cover most of the common queries for variant effect. All of the functions described below require that the model.yaml and dataloader.yaml files are set up in the way defined above.  In literature in-silico mutagenesis-based variant effect predcition is performed in a variant centric way: Starting from a VCF for every variant a sequence centered on said variant is generated. That sequence is then mutated by modifying the central base and setting it to what is defined as reference or alternative allele, generating two sets of sequences. For both the set with the reference allele in the center and the alternative allele in the center the model prediction is run and model outputs are compared.  Not all models can predict on aribrary DNA sequences from any region of the genome. Splicing models may for example only be trained on regions surrounding a splice site, hence the variant-centered approach from before will not work. Therefore two more options to run variant effect predicion are offered: restricted variant centered effect prediction and overlap-based effect prediction.  Variant effect prediction will try to use variant-centered approaches whenever the  bed_input  flag is defined in dataloader.yaml (see below). Otherwise the overlap-based effect prediction is used. This is because the variant centered approach is generally faster and for every variant in the VCF one single prediction can be made (assuming the position of variant is in a valid genomic region).  For all the methods described below it is essential that genomic coordinates in the VCF and the coordinates used by the dataloader are for the same genome / assembly /etc.",
            "title": "Use-cases"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#variant-centered-effect-prediction",
            "text": "In order to use variant centered effect prediction the dataloader must accept an input bed file based on which it will produce model input. Furthermore the dataloader is required to return the name values (fourth column) of the input bed file in the  id  field of  model_input['metadata']['ranges'] . Additionally the order of samples has to be identical with the order of regions in the input bed file, but regions may be skipped.  In order for the variant effect prediction to know which input argument of the dataloader is accepts a bed file three additional lines in dataloader.yaml are necessary, e.g:  postprocessing:\n    variant_effects:\n      bed_input:\n        - intervals_file  This section indicates that the dataloader function has an argument  intervals_file  which accepts a bed file path as input which may be used.",
            "title": "Variant centered effect prediction"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#restricted-variant-centered-effect-prediction",
            "text": "Requirements for the dataloader and dataloader.yaml here are identical to the variant centered effect prediction. The only difference is that this function is designed for models that can't predict on arbitrary regions of the genome, but only in certain regions of the genome. If those regions can be defined in a bed file (further on called 'restriction-bed' file) then this approach can be used. Variant effect prediction will then intersect the VCF with the restriction-bed and generate another bed file that is then passed on to the dataloader.  Regions in the restriction-bed file may be larger than the input sequence lenght, in that case the generated seuqence will be centered on the variant position as much as possible - restricted by what is defined in the restrictions-bed file.",
            "title": "Restricted-variant centered effect prediction"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#overlap-based-effect-prediction",
            "text": "If the dataloader does not support bed input files then variant effect predictions can be run by the overlap of a VCF with the regions defined in the metdata output of the dataloader.  If multiple variants overlap with a region then the effect will be predicted inpendently for those variants. If multiple (e.g.: two) model input samples overlap with one variant then the output will contain as many predictions as there were independent overlaps of metadata ranges and variants (e.g.: two).",
            "title": "Overlap-based effect prediction"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#scoring-functions",
            "text": "After mutating the model input DNA sequences predictions are created using the models and those predictions then have to compared by scoring methods. Not all scoring methods are compatible with all models depending on the output data range of the model (see below). The compatibility of a scoring function with a given model can be indicated by setting  scoring_functions  in model.yaml:  postprocessing:\n   variant_effects\n      seq_input:\n        - seq\n      scoring_functions:\n        - name: diff\n          type: diff\n        - type: logit\n          default: true  The scoring function is identified by the  type  field in  scoring_functions  which is the only mandatory field. Allowed values for the  type  field are:  diff ,  logit ,  deepsea_effect  and  custom .  Setting  default:true  for a scoring function indicates that that respective scoring function is executed by variant effect prediction if none is selected by the used on execution time. If multiple scoring functions have set  default:true  then all of those will be run by default. If  default:true  is not set for any scoring function defined in  scoring_functions  then all entries in  scoring_functions  will be run by default.  Scoring functions can be assigned a different name with the  name  flag by which they are then selected using the command line interface. In general it is not advisable to rename the scoring functions that come with Kipoi.",
            "title": "Scoring functions"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#diff",
            "text": "The simplest scoring method is to calculate the difference between predictions for the reference and the alternative allele:  prediction(alt) - prediction(ref) . This scoring method is available for all models no matter if it is defined in  scoring_functions  or not.",
            "title": "Diff"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#logit",
            "text": "Calculates the difference of logit-transformed values of the predictions: logit(prediction(alt)) - logit(prediction(ref)) . This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1].",
            "title": "Logit"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#logitalt",
            "text": "Returns the logits transformed predictions for the sequences carrying the alternative allele: logit(prediction(alt)) . This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1].",
            "title": "LogitAlt"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#logitref",
            "text": "Returns the logits transformed predictions for the sequences carrying the reference allele: logit(prediction(ref)) . This scoring method only makes sense if the model output can be interpreted as probabilities. In a wider sense, it will only produce valid values if the predictions are in the range [0,1].",
            "title": "LogitRef"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#deepsea_effect",
            "text": "Calculates the variant scores as defined in the publication of the DeepSEA model (Troyanskaya et al., 2015) by using the absolute value of the logit difference and diff values multiplied together:  abs(Logit * Diff)  with  Logit  and  Diff  defined as above.",
            "title": "Deepsea_effect"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#custom",
            "text": "Custom scoring methods can be defined and shipped with the models. In that case the model.yaml will look similar to this:  postprocessing:\n  variant_effects:\n    seq_input:\n      - seq\n    scoring_functions:\n      - name: my_scores\n        type: custom\n        defined_as: postproc.py::myfun\n        args:\n          first_arg:\n            doc: Description of the first argument\n            default: 1  Notice that the selection of  type: custom  requires that  defined_as  is set. The value  postproc.py::myfun  indicates that the callable python object  myfun  is stored in a file called  postproc.py . When executing variant effect prediction in the command line the scoring function can be chosen by it's name - which in this case is:  my_scores .  All scoring functions are subclasses of  Rc_merging_pred_analysis  this means that also a custom scoring function must inherit from it.",
            "title": "Custom"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#output",
            "text": "The output of variant effect prediction is by default stored in a VCF that is derived from the input VCF. The output VCF only contains variants for which a effect prediction could be generated (e.g. if no model input sample overlapped a variant no prediction could be made for it). The predictions themselves are stored in the INFO field of the VCF, with the ID starting with  KPVEP  and containing the name of the model. Additional to the predictions themselves a also a region ID will be stored in a second INFO field. The region IDs are the values stored in  model_input['metadata']['ranges']['id']  given to a sequence sample generated by the dataloader. This way it is possible to trace back which sequence was mutated by which variant in order to produce a certain effect prediction  Since multiple seqeunces generated by the dataloader may overlap one variant - especially when using the overlap-based effect prediction - it is possible that the generated VCF output will contain a variant multiple times, but the different predictions will be destinguishable by their region ID.  If variant effect prediction is run programmatically in python then the results are returned as a dictionary of pandas DataFrames.",
            "title": "Output"
        },
        {
            "location": "/postprocessing/variant_effect_prediction/#more-complex-models",
            "text": "More complex models may have more than only one DNA sequence input, it may even be that models have DNA sequence inputs taken from different regions of the genome within one sample in a batch. See this sketch for an illustration of the scenario:   The dataloader has three sequence outputs which are linked to two metadata ranges. for both ranges objects the beginning of the ranges is displayed. In order to overlap the metadata ranges with variants the input batch is processed one sample at a time. The samples in a batch are displayed in green rectangular boxes: For every sample all the ranges are assembled and overlapped with variants in the VCF. Then the effect is predicted for every single variant in the VCF that overlaps at least one of the region defined in that sample. This means that for the first sample in the batch two variants are investigated: rs1 and rs2. rs1 can only affect seq1a and seq1b, hence those two sequences are mutated, seq2 is not. rs2 overlaps with both ranges in the first sample and hence two sequences are mutated with rs2 to predict its effect. This means that the first sample will be evaluated twice using variants rs1 and rs2, and the second sample only once using rs3.",
            "title": "More complex models"
        },
        {
            "location": "/contributing/01_Getting_started/",
            "text": "Contributing models - Getting started\n\n\nKipoi stores models (descriptions, parameter files, dataloader code, ...) as folders in the \nkipoi/models\n github repository. Files residing in folders with a suffix of \n_files\n are tracked via Git Large File Storage (LFS). New models are added by simply submitting a pull-request to \nhttps://github.com/kipoi/models\n.\n\n\nRequired steps\n\n\nHere is a list of steps required to contribute a model to \nkipoi/models\n:\n\n\n1. Install Kipoi\n\n\n\n\nInstall git-lfs\n\n\nconda install -c conda-forge git-lfs && git lfs install\n\n\nFor alternative installation options  see \nhttps://git-lfs.github.com/\n.\n\n\n\n\n\n\nInstall kipoi\n\n\npip install kipoi\n\n\n\n\n\n\nRun \nkipoi ls\n (this will checkout the \nkipoi/models\n repo to \n~/.kipoi/models\n)\n\n\n\n\n2. Add the model\n\n\n\n\ncd ~/.kipoi/models\n\n\nWrite the model\n: Create a new folder \n<my new model>\n containing all the required files. The required files can be created by doing one of the following three options:\n\n\nOption 1: Copy the existing model: \ncp -R <existing model> <my new model>\n, edit/replace/add the copied files until they fit your new model.\n\n\nOption 2: Run \nkipoi init\n, answer the questions, edit/replace the created files until they fit your new model.\n\n\nOption 3: \nmkdir <my new model>\n & write all the files from scratch\n\n\n\n\n\n\nTest the model\n\n\nStep 1: \nkipoi test ~/.kipoi/models/my_new_model\n\n\nStep 2: \nkipoi test-source kipoi --all -k my_new_model\n\n\n\n\n\n\n\n\n3. Submit the pull-request\n\n\nOption 1: Fork the repository\n\n\n\n\nMake sure you have all the recent changes locally\n\n\ncd ~/.kipoi/models\n\n\nexport GIT_LFS_SKIP_SMUDGE=1 && git pull\n - pulls all the changes but doesn't download the files tracked by git-lfs.\n\n\n\n\n\n\nCommit your changes\n\n\ngit add my_new_model/\n\n\ngit commit -m \"Added <my new model>\"\n\n\n\n\n\n\nFork\n the \nhttps://github.com/kipoi/models\n repo on github (click on the Fork button)\n\n\nAdd your fork as a git remote to \n~/.kipoi/models\n\n\ngit remote add fork https://github.com/<username>/models.git\n\n\n\n\n\n\nPush to your fork\n\n\ngit push fork master\n\n\n\n\n\n\nSubmit a pull-request\n\n\nclick the \nNew pull request\n button on your github fork - \nhttps://github.com/<username>/models>\n\n\n\n\n\n\n\n\nOption 2: Create a new branch on kipoi/models\n\n\nIf you wish to contribute models more frequently, please \njoin the team\n. You will be added to the Kipoi organization. This will allow you to push to branches of the \nkipoi/models\n github repo directly.\n\n\n\n\nMake sure you have all the recent changes locally\n\n\ncd ~/.kipoi/models\n\n\nexport GIT_LFS_SKIP_SMUDGE=1 && git pull\n - pulls all the changes but doesn't download the files tracked by git-lfs.\n\n\n\n\n\n\nCreate a new branch in \n~/.kipoi/models\n\n\ngit stash\n - this will store/stash all local changes in \ngit stash\n\n\ngit checkout -b my_new_model\n - create a new branch\n\n\ngit stash pop\n - get the stashed files back\n\n\n\n\n\n\nCommit changes\n\n\ngit add my_new_model/\n\n\ngit commit -m \"Added <my new model>\"\n\n\n\n\n\n\nPush changes to \nmy_new_model\n branch\n\n\ngit push -u origin my_new_model\n\n\n\n\n\n\nSubmit a pull-request\n\n\nclick the \nNew pull request\n button on \nmy_new_model\n branch of repo \nhttps://github.com/kipoi/models\n.\n\n\n\n\n\n\n\n\nRest of this document will go more into the details about steps writing the model and testing the model.\n\n\nHow to write the model\n\n\nBest place to start figuring out which files you need to contribute is to look at some of the existing models. Explore the \nhttps://github.com/kipoi/models\n repository and see if there are any models similar to yours (in terms of the dependencies, framework, input-output data modalities). See \ntutorials/contributing_models\n for a step-by-step procedure for contributing models.\n\n\nOption #1: Copy existing model\n\n\nOnce you have found the closest match, simply copy the directory and start editing/replacing the files. Edit the files in this order:\n\n\n\n\nmodel.yaml\n\n\ndataloader.yaml\n\n\ndataloader.py\n\n\noverwrite files in \nmodel_files/\n\n\nexample_files/\n\n\nLICENSE\n\n\n\n\nOption #2: Use \nkipoi init\n\n\nAlternatively, you can use \nkipoi init\n instead of copying the existing model:\n\n\ncd ~/.kipoi/models && kipoi init\n\n\n\n\nThis will ask you a few questions and create a new model folder.\n\n\n$ kipoi init\nINFO [kipoi.cli.main] Initializing a new Kipoi model\n\nPlease answer the questions below. Defaults are shown in square brackets.\n\nYou might find the following links useful: \n- (model_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_models.md\n- (dataloader_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_dataloaders.md\n--------------------------------------------\n\nmodel_name [my_model]: my_new_model\nauthor_name [Your name]: Ziga Avsec\nauthor_github [Your github username]: avsecz\nauthor_email [Your email(optional)]: \nmodel_doc [Model description]: Model predicting iris species\nSelect model_license:\n1 - MIT\n2 - BSD\n3 - ISCL\n4 - Apache Software License 2.0\n5 - Not open source\nChoose from 1, 2, 3, 4, 5 [1]:  \nSelect model_type:\n1 - keras\n2 - custom\n3 - sklearn\nChoose from 1, 2, 3 [1]: 1\nSelect model_input_type:\n1 - np.array\n2 - list of np.arrays\n3 - dict of np.arrays\nChoose from 1, 2, 3 [1]: 2\nSelect model_output_type:\n1 - np.array\n2 - list of np.arrays\n3 - dict of np.arrays\nChoose from 1, 2, 3 [1]: 3\nSelect dataloader_type:\n1 - Dataset\n2 - PreloadedDataset\n3 - BatchDataset\n4 - SampleIterator\n5 - SampleGenerator\n6 - BatchIterator\n7 - BatchGenerator\nChoose from 1, 2, 3, 4, 5, 6, 7 [1]: 1\n--------------------------------------------\nINFO [kipoi.cli.main] Done!\nCreated the following folder into the current working directory: my_new_model\n\n\n\n\nThe created folder contains a model and a dataloader for predicting the Iris species. You will now have to \nedit the model.yaml\n and to \nedit the dataloader.yaml\n files according to your model. Also you will have to copy you rmodel files into the model_files directory. You can check whether you have succeeded and your model is setup correctly with the commands below.\n\n\nHow to test the model\n\n\nBe aware that the test functions will only check whether the definition side of things (model.yaml, dataloader.yaml, syntax errors, etc.) is setup correctly, you will have to validate yourself whether the outputs created by using the predict function produce the desired model output!\n\n\nStep 1: Run \nkipoi test ~/.kipoi/models/my_new_model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis checks the yaml files and runs \nkipoi predict\n for the example files (specified in \ndataloader.yaml > args > my_arg > example\n). Once this command returns no errors or warnings proceed to the next step.\n\n\nStep 2: Run \nkipoi test-source kipoi --all -k my_new_model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis will run \nkipoi test\n in a new conda environment with dependencies specified in \nmodel.yaml\n and \ndataloader.yaml\n.\n\n\nRemoving or updating models\n\n\nTo remove, rename or update an existing model, send a pull-request (as when contributing models, see \n3. Submit the pull-request\n).",
            "title": "Getting started"
        },
        {
            "location": "/contributing/01_Getting_started/#contributing-models-getting-started",
            "text": "Kipoi stores models (descriptions, parameter files, dataloader code, ...) as folders in the  kipoi/models  github repository. Files residing in folders with a suffix of  _files  are tracked via Git Large File Storage (LFS). New models are added by simply submitting a pull-request to  https://github.com/kipoi/models .",
            "title": "Contributing models - Getting started"
        },
        {
            "location": "/contributing/01_Getting_started/#required-steps",
            "text": "Here is a list of steps required to contribute a model to  kipoi/models :",
            "title": "Required steps"
        },
        {
            "location": "/contributing/01_Getting_started/#1-install-kipoi",
            "text": "Install git-lfs  conda install -c conda-forge git-lfs && git lfs install  For alternative installation options  see  https://git-lfs.github.com/ .    Install kipoi  pip install kipoi    Run  kipoi ls  (this will checkout the  kipoi/models  repo to  ~/.kipoi/models )",
            "title": "1. Install Kipoi"
        },
        {
            "location": "/contributing/01_Getting_started/#2-add-the-model",
            "text": "cd ~/.kipoi/models  Write the model : Create a new folder  <my new model>  containing all the required files. The required files can be created by doing one of the following three options:  Option 1: Copy the existing model:  cp -R <existing model> <my new model> , edit/replace/add the copied files until they fit your new model.  Option 2: Run  kipoi init , answer the questions, edit/replace the created files until they fit your new model.  Option 3:  mkdir <my new model>  & write all the files from scratch    Test the model  Step 1:  kipoi test ~/.kipoi/models/my_new_model  Step 2:  kipoi test-source kipoi --all -k my_new_model",
            "title": "2. Add the model"
        },
        {
            "location": "/contributing/01_Getting_started/#3-submit-the-pull-request",
            "text": "",
            "title": "3. Submit the pull-request"
        },
        {
            "location": "/contributing/01_Getting_started/#option-1-fork-the-repository",
            "text": "Make sure you have all the recent changes locally  cd ~/.kipoi/models  export GIT_LFS_SKIP_SMUDGE=1 && git pull  - pulls all the changes but doesn't download the files tracked by git-lfs.    Commit your changes  git add my_new_model/  git commit -m \"Added <my new model>\"    Fork  the  https://github.com/kipoi/models  repo on github (click on the Fork button)  Add your fork as a git remote to  ~/.kipoi/models  git remote add fork https://github.com/<username>/models.git    Push to your fork  git push fork master    Submit a pull-request  click the  New pull request  button on your github fork -  https://github.com/<username>/models>",
            "title": "Option 1: Fork the repository"
        },
        {
            "location": "/contributing/01_Getting_started/#option-2-create-a-new-branch-on-kipoimodels",
            "text": "If you wish to contribute models more frequently, please  join the team . You will be added to the Kipoi organization. This will allow you to push to branches of the  kipoi/models  github repo directly.   Make sure you have all the recent changes locally  cd ~/.kipoi/models  export GIT_LFS_SKIP_SMUDGE=1 && git pull  - pulls all the changes but doesn't download the files tracked by git-lfs.    Create a new branch in  ~/.kipoi/models  git stash  - this will store/stash all local changes in  git stash  git checkout -b my_new_model  - create a new branch  git stash pop  - get the stashed files back    Commit changes  git add my_new_model/  git commit -m \"Added <my new model>\"    Push changes to  my_new_model  branch  git push -u origin my_new_model    Submit a pull-request  click the  New pull request  button on  my_new_model  branch of repo  https://github.com/kipoi/models .     Rest of this document will go more into the details about steps writing the model and testing the model.",
            "title": "Option 2: Create a new branch on kipoi/models"
        },
        {
            "location": "/contributing/01_Getting_started/#how-to-write-the-model",
            "text": "Best place to start figuring out which files you need to contribute is to look at some of the existing models. Explore the  https://github.com/kipoi/models  repository and see if there are any models similar to yours (in terms of the dependencies, framework, input-output data modalities). See  tutorials/contributing_models  for a step-by-step procedure for contributing models.",
            "title": "How to write the model"
        },
        {
            "location": "/contributing/01_Getting_started/#option-1-copy-existing-model",
            "text": "Once you have found the closest match, simply copy the directory and start editing/replacing the files. Edit the files in this order:   model.yaml  dataloader.yaml  dataloader.py  overwrite files in  model_files/  example_files/  LICENSE",
            "title": "Option #1: Copy existing model"
        },
        {
            "location": "/contributing/01_Getting_started/#option-2-use-kipoi-init",
            "text": "Alternatively, you can use  kipoi init  instead of copying the existing model:  cd ~/.kipoi/models && kipoi init  This will ask you a few questions and create a new model folder.  $ kipoi init\nINFO [kipoi.cli.main] Initializing a new Kipoi model\n\nPlease answer the questions below. Defaults are shown in square brackets.\n\nYou might find the following links useful: \n- (model_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_models.md\n- (dataloader_type) https://github.com/kipoi/kipoi/blob/master/docs/writing_dataloaders.md\n--------------------------------------------\n\nmodel_name [my_model]: my_new_model\nauthor_name [Your name]: Ziga Avsec\nauthor_github [Your github username]: avsecz\nauthor_email [Your email(optional)]: \nmodel_doc [Model description]: Model predicting iris species\nSelect model_license:\n1 - MIT\n2 - BSD\n3 - ISCL\n4 - Apache Software License 2.0\n5 - Not open source\nChoose from 1, 2, 3, 4, 5 [1]:  \nSelect model_type:\n1 - keras\n2 - custom\n3 - sklearn\nChoose from 1, 2, 3 [1]: 1\nSelect model_input_type:\n1 - np.array\n2 - list of np.arrays\n3 - dict of np.arrays\nChoose from 1, 2, 3 [1]: 2\nSelect model_output_type:\n1 - np.array\n2 - list of np.arrays\n3 - dict of np.arrays\nChoose from 1, 2, 3 [1]: 3\nSelect dataloader_type:\n1 - Dataset\n2 - PreloadedDataset\n3 - BatchDataset\n4 - SampleIterator\n5 - SampleGenerator\n6 - BatchIterator\n7 - BatchGenerator\nChoose from 1, 2, 3, 4, 5, 6, 7 [1]: 1\n--------------------------------------------\nINFO [kipoi.cli.main] Done!\nCreated the following folder into the current working directory: my_new_model  The created folder contains a model and a dataloader for predicting the Iris species. You will now have to  edit the model.yaml  and to  edit the dataloader.yaml  files according to your model. Also you will have to copy you rmodel files into the model_files directory. You can check whether you have succeeded and your model is setup correctly with the commands below.",
            "title": "Option #2: Use kipoi init"
        },
        {
            "location": "/contributing/01_Getting_started/#how-to-test-the-model",
            "text": "Be aware that the test functions will only check whether the definition side of things (model.yaml, dataloader.yaml, syntax errors, etc.) is setup correctly, you will have to validate yourself whether the outputs created by using the predict function produce the desired model output!",
            "title": "How to test the model"
        },
        {
            "location": "/contributing/01_Getting_started/#step-1-run-kipoi-test-kipoimodelsmy_new_model",
            "text": "This checks the yaml files and runs  kipoi predict  for the example files (specified in  dataloader.yaml > args > my_arg > example ). Once this command returns no errors or warnings proceed to the next step.",
            "title": "Step 1: Run kipoi test ~/.kipoi/models/my_new_model"
        },
        {
            "location": "/contributing/01_Getting_started/#step-2-run-kipoi-test-source-kipoi-all-k-my_new_model",
            "text": "This will run  kipoi test  in a new conda environment with dependencies specified in  model.yaml  and  dataloader.yaml .",
            "title": "Step 2: Run kipoi test-source kipoi --all -k my_new_model"
        },
        {
            "location": "/contributing/01_Getting_started/#removing-or-updating-models",
            "text": "To remove, rename or update an existing model, send a pull-request (as when contributing models, see  3. Submit the pull-request ).",
            "title": "Removing or updating models"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/",
            "text": "model.yaml\n\n\nThe model.yaml file describes the individual model in the model zoo. It defines its dependencies, framework, architecture, input / output schema, general information and more. Correct defintions in the model.yaml enable to make full use of Kipoi features and make sure that a model can be executed at any point in future.\n\n\nTo help understand the synthax of YAML please take a look at: \nYAML Synthax Basics\n\n\nHere is an example \nmodel.yaml\n:\n\n\ntype: keras  # use `kipoi.model.KerasModel`\nargs:  # arguments of `kipoi.model.KerasModel`\n    arch: model_files/model.json\n    weights: model_files/weights.h5\ndefault_dataloader: . # path to the dataloader directory. Here it's defined in the same directory\ninfo: # General information about the model\n    authors: \n        - name: Your Name\n          github: your_github_username\n          email: your_email@host.org\n    doc: Model predicting the Iris species\n    version: 0.1  # optional \n    cite_as: https://doi.org:/... # preferably a doi url to the paper\n    trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description\n    license: MIT # Software License - defaults to MIT\ndependencies:\n    conda: # install via conda\n      - python=3.5\n      - h5py\n      # - soumith::pytorch  # specify packages from other channels via <channel>::<package>      \n    pip:   # install via pip\n      - keras>=2.0.4\n      - tensorflow>=1.0\nschema:  # Model schema\n    inputs:\n        features:\n            shape: (4,)  # array shape of a single sample (omitting the batch dimension)\n            doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\"\n    targets:\n        shape: (3,)\n        doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\"\n\n\n\n\nThe model.yaml file has the following mandatory fields:\n\n\ntype\n\n\nThe model type refers to base framework which the model was defined in. Kipoi comes with a support for Keras, PyTorch, SciKit-learn and tensorflow models. To indicate which kind of model will be used the following values for \ntype\n are allowed: \nkeras\n, \npytorch\n, \nsklearn\n, \ntensorflow\n, and \ncustom\n.\n\n\nThe model type is required to find the right internal prepresentation of a model within Kipoi, which enables loading weights and architecture correctly and offers to have a unified API across frameworks.\n\n\nIn the model.yaml file the definition of a Keras model would like this:\n\n\ntype: keras\n\n\n\n\nargs\n\n\nModel arguments define where the files are files and functions are located to instantiate the model. Most entries of \nargs\n will contain paths to files, those paths are relative to the location of the model.yaml file. The correct definition of \nargs\n depends on the \ntype\n that was selected:\n\n\nkeras\n models\n\n\nFor Keras models the following args are available: \nweights\n, \narch\n, \ncustom_objects\n. The Keras framework offers different ways to store model architecture and weights:\n\n\n\n\nArchitecture and weights can be stored separately:\n\n\n\n\ntype: keras\nargs:\n    arch: model_files/model.json\n    weights: model_files/weights.h5\n\n\n\n\n\n\nThe architecture can be stored together with the weights:\n\n\n\n\ntype: keras\nargs:\n    weights: model_files/model.h5\n\n\n\n\nIn Keras models can have custom layers, which then have to be available at the instantiation of the Keras model, those should be stored in one python file that comes with the model architecture and weights. This file defines a dictionary containing custom Keras components called \nOBJECTS\n. These objects will be added to \ncustom_objects\n when loading the model with \nkeras.models.load_model\n.\n\n\nExample of a \ncustom_keras_objects.py\n:\n\n\nfrom concise.layers import SplineT\n\nOBJECTS = {\"SplineT\": SplineT}\n\n\n\n\nExample of the corresponding model.yaml entry:\n\n\ntype: keras\nargs:\n    ...\n    custom_objects: model_files/custom_keras_objects.py\n\n\n\n\nHere all the objects present in \nmodel_files/custom_keras_objects.py\n will be made available to Keras when loading the model.\n\n\npytorch\n models\n\n\nPytorch offers much freedom as to how the model is stored. In Kipoi a pytorch model has the following \nargs\n: \nfile\n, \nbuild_fn\n, \nweights\n. If \ncuda\n is available the model will automatically be switched to cuda mode, so the user does not have to take care of that and the \nbuild_fn\n should not attempt to do this conversion. The following ways of instantiating a model are supported:\n\n\n\n\nBuild function: In the example below Kipoi expects that when calling \nget_full_model()\n (which is defined in \nmodel_files/model_def.py\n) A pytorch model is returned that for which the weights have already been loaded.\n\n\n\n\ntype: pytorch\nargs:\n    file: model_files/model_def.py\n    build_fn: get_full_model\n\n\n\n\n\n\nBuild function + weights: In the example below the model is instantiated by calling \nget_model()\n which can be found in \nmodel_files/model_def.py\n. After that the weights will be loaded by executing \nmodel.load_state_dict(torch.load(weights))\n.\n\n\n\n\ntype: pytorch\nargs:\n    file: model_files/model_def.py\n    build_fn: get_model\n    weights: model_files/weights.pth\n\n\n\n\n\n\nArchitecture and weights in one file: In this case Kipoi assumes that \nmodel = torch.load(weights)\n will be a valid pytorch model. Care has to be taken when storing the architecture a model this way as only standard pytorch layers will be loaded correctly, please see the pytorch documentation for details.\n\n\n\n\ntype: pytorch\nargs:\n    weights: model_files/model.pth\n\n\n\n\nsklearn\n models\n\n\nSciKit-learn models can be loaded from a pickle file as defined below. The command used for loading is: \njoblib.load(pkl_file)\n\n\ntype: sklearn\nargs:\n  pkl_file: model_files/model.pkl\n\n\n\n\ntensorflow\n models\n\n\nTensorflow models are expected to be stored by calling \nsaver = tf.train.Saver(); saver.save(checkpoint_path)\n. The \ninput_nodes\n argument is then a string, list of strings or dictionary of strings that define the input node names. The \ntarget_nodes\n argument is a string, list of strings or dictionary of strings that define the model target node names.\n\n\ntype: tensorflow\nargs:\n  input_nodes: \"inputs\"\n  target_nodes: \"preds\"\n  checkpoint_path: \"model_files/model.tf\"\n\n\n\n\nIf a model requires a constant feed of data which is not provided by the dataloader the \nconst_feed_dict_pkl\n argument can be defined additionally to the above. Values given in the pickle file will be added to the batch samples created by the dataloader. If values with identical keys have been created by the dataloader they will be overwritten with what is given in \nconst_feed_dict_pkl\n.\n\n\n\ntype: tensorflow\nargs:\n  ...\n  const_feed_dict_pkl: \"model_files/const_feed_dict.pkl\"\n\n\n\n\ncustom\n models\n\n\nIt is possible to defined a model class independent of the ones which are made available in Kipoi. In that case the contributor-defined \nModel\n class must be a subclass of \nBaseModel\n defined in \nkipoi.model\n. Custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the \npredict_on_batch\n function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo!\n\n\nIf for example a custom model class definition (\nMyModel\n) lies in a file \nmy_model.py\n, then the model.yaml will contain:\n\n\ntype: custom\nargs:\n  file: my_model.py\n  object: MyModel\n\n\n\n\nKipoi will then use an instance of MyModel as a model. Keep in mind that MyModel has to be subclass of \nBaseModel\n, which in other words means that \ndef predict_on_batch(self, x)\n has to be implemented. So if \nbatch\n is for example what the dataloader returns for a batch then \npredict_on_batch(batch['inputs'])\n has to work.\n\n\ninfo\n\n\nThe \ninfo\n field of a model.yaml file contains general information about the model.\n\n\n\n\nauthors\n: a list of authors with the field: \nname\n, and the optional fields: \ngithub\n and \nemail\n. Where the \ngithub\n name is the github user id of the respective author\n\n\ndoc\n: Free text documentation of the model. A short description of what it does and what it is designed for.\n\n\nversion\n: Model version\n\n\nlicense\n: String indicating the license, if not defined it defaults to \nMIT\n\n\ntags\n: A list of key words describing the model and its use cases\n\n\ncite_as\n: Link to the journal, arXiv, ...\n\n\ntrained_on\n: Description of the training dataset\n\n\ntraining_procedure\n: Description of the training procedure\n\n\n\n\nA dummy example could look like this:\n\n\ninfo:\n  authors:\n    - name: My Name\n      github: myGithubName\n      email: my@email.com\n    - name: Second Author\n  doc: My fancy model description\n  version: 1.0\n  license: GNU\n  tags:\n    - TFBS\n    - tag2\n  cite_as: http://www.the_journal.com/mypublication\n  trained_on: The XXXX dataset from YYYY\n  training_procedure: 10-fold cross validation\n\n\n\n\ndefault_dataloader\n\n\nThe \ndefault_dataloader\n points to the location of the dataloader.yaml file. By default this will be in the same folder as the model.yaml file, in which case \ndefault_dataloader\n doesn't have to be defined.\n\n\nIf dataloader.yaml lies in different subfolder then \ndefault_dataloader: path/to/folder\n would be used where dataloader.yaml would lie in \nfolder\n.\n\n\nschema\n\n\nSchema defines what the model inputs and outputs are, what they consist in and what the dimensions are.\n\n\nschema\n contains two categories \ninputs\n and \ntargets\n which each specify the shapes of the model input and model output.\n\n\nIn general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or \nOrderedDict\n) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition:\n\n\n\n\nA single numpy array as input or target:\n\n\n\n\nschema:\n    inputs:\n       name: seq\n       shape: (1000,4)\n\n\n\n\n\n\nA list of numpy arrays as inputs or targets:\n\n\n\n\nschema:\n    targets:\n       - name: seq\n         shape: (1000,4)\n       - name: inp2\n         shape: (10)\n\n\n\n\n\n\nA dictionary of numpy arrays as inputs or targets:\n\n\n\n\nschema:\n    inputs:\n       seq:\n         shape: (1000,4)\n       inp2:\n         shape: (10)\n\n\n\n\ninputs\n\n\nThe \ninputs\n fields of \nschema\n may be lists, dictionaries or single occurences of the following entries:\n\n\n\n\nshape\n: Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of \n(1000, 4)\n inputs \nshape: (1000, 4)\n should be set. If a dimension is of variable size then the numerical should be replaced by \nNone\n.\n\n\ndoc\n: A free text description of the model input\n\n\nname\n: Name of model input , not required if input is a dictionary.\n\n\nspecial_type\n: Possibility to flag that respective input is a 1-hot encoded DNA sequence (\nspecial_type: DNASeq\n) or a string DNA sequence (\nspecial_type: DNAStringSeq\n), which is important for variant effect prediction.\n\n\n\n\ntargets\n\n\nThe \ntargets\n fields of \nschema\n may be lists, dictionaries or single occurences of the following entries:\n\n\n\n\nshape\n: Required: Details see in \ninput\n\n\ndoc\n: A free text description of the model input\n\n\nname\n: Name of model  target, not required if target is a dictionary.\n\n\ncolumn_labels\n: Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line).\n\n\n\n\nHow model types handle schemas\n\n\nThe different model types handle those three different encapsulations of numpy arrays differently:\n\n\nkeras\n type models\n\n\nInput\n\n\nIn case a Keras model is used the batch produced by the dataloader is passed on as it is to the \nmodel.predict_on_batch()\n function. So if for example a dictionary is defined in the model.yaml and that is produced by the dataloader then this dicationary is passed on to \nmodel.predict_on_batch()\n.\n\n\nOutput\n\n\nThe model is expected to return the schema that is defined in model.yaml. If for example a model returns a list of numpy arrays then that has to be defined correctly in the model.yaml schema.\n\n\npytorch\n type models\n\n\nPytorch needs \ntorch.autograd.Variable\n instances to work. Hence all inputs are automatically converted into \nVariable\n objects and results are converted back into numpy arrays transparently. If \ncuda\n is available the model will automatically be used in cuda mode and also the input variables will be switched to \ncuda\n.\n\n\nInput\n\n\nFor prediction the following will happen to the tree different encapsulations of input arrays:\n\n\n\n\nA single array: Will be passed directly as the only argument to model call: \nmodel(Variable(from_numpy(x)))\n\n\nA list of arrays: The model will be called with the list of converted array as args (e.g.: \nmodel(*list_of_variables)\n)\n\n\nA dictionary of arrays: The model will be called with the dictionary of converted array as kwargs (e.g.: \nmodel(**dict_of_variables)\n)\n\n\n\n\nOutput\n\n\nThe model return values will be converted back into encapsulations of numpy arrays, where:\n\n\n\n\na single \nVariable\n object will be converted into a numpy arrays\n\n\nlists of \nVariable\n objects will be converted into a list of numpy arrays in the same order and\n\n\n\n\nsklearn\n type models\n\n\nThe batch generated by the dataloader will be passed on directly to the SciKit-learn model using \nmodel.predict(x)\n\n\ntensorflow\n type models\n\n\nInput\n\n\nThe \nfeed_dict\n for running a tensorflow session is generated by converting the batch samples into the \nfeed_dict\n using \ninput_nodes\n defined in the \nargs\n section of the model.yaml. For prediction the following will happen to the tree different encapsulations of input arrays:\n\n\n\n\nIf \ninput_nodes\n is a single string the model will be fed with a dictionary \n{input_ops: x}\n\n\nIf \ninput_nodes\n is a list then the batch is also exptected to be a list in the corresponding order and the feed dict will be created from that.\n\n\nIf \ninput_nodes\n is a dictionary then the batch is also exptected to be a dictionary with the same keys and the feed dict will be created from that.\n\n\n\n\nOutput\n\n\nThe return value of the tensorflow model is returned without further transformations and the model outpu schema defined in the \nschema\n field of model.yaml has to match that.\n\n\ndependencies\n\n\nOne of the core elements of ensuring functionality of a model is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the \nconda\n and \npip\n sections respectively.\n\n\nBoth can either be defined as a list of packages or as a text file (ending in \n.txt\n) which lists the dependencies.\n\n\nConda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: \npackage>=1.0\n is very likely to break at some point in future.\n\n\nconda\n\n\nConda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in \n.txt\n).\n\n\nIf conda packages need to be loaded from a channel then the nomenclature \nchannel_name::package_name\n can be used.\n\n\npip\n\n\nPip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in \n.txt\n).\n\n\npostprocessing\n\n\nThe postprocessing section of a model.yaml is necessary to indicate that a model is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your model up for variant effect prediction, please take a look at the documentation of variant effect prediction.",
            "title": "model.yaml"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#modelyaml",
            "text": "The model.yaml file describes the individual model in the model zoo. It defines its dependencies, framework, architecture, input / output schema, general information and more. Correct defintions in the model.yaml enable to make full use of Kipoi features and make sure that a model can be executed at any point in future.  To help understand the synthax of YAML please take a look at:  YAML Synthax Basics  Here is an example  model.yaml :  type: keras  # use `kipoi.model.KerasModel`\nargs:  # arguments of `kipoi.model.KerasModel`\n    arch: model_files/model.json\n    weights: model_files/weights.h5\ndefault_dataloader: . # path to the dataloader directory. Here it's defined in the same directory\ninfo: # General information about the model\n    authors: \n        - name: Your Name\n          github: your_github_username\n          email: your_email@host.org\n    doc: Model predicting the Iris species\n    version: 0.1  # optional \n    cite_as: https://doi.org:/... # preferably a doi url to the paper\n    trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description\n    license: MIT # Software License - defaults to MIT\ndependencies:\n    conda: # install via conda\n      - python=3.5\n      - h5py\n      # - soumith::pytorch  # specify packages from other channels via <channel>::<package>      \n    pip:   # install via pip\n      - keras>=2.0.4\n      - tensorflow>=1.0\nschema:  # Model schema\n    inputs:\n        features:\n            shape: (4,)  # array shape of a single sample (omitting the batch dimension)\n            doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\"\n    targets:\n        shape: (3,)\n        doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\"  The model.yaml file has the following mandatory fields:",
            "title": "model.yaml"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#type",
            "text": "The model type refers to base framework which the model was defined in. Kipoi comes with a support for Keras, PyTorch, SciKit-learn and tensorflow models. To indicate which kind of model will be used the following values for  type  are allowed:  keras ,  pytorch ,  sklearn ,  tensorflow , and  custom .  The model type is required to find the right internal prepresentation of a model within Kipoi, which enables loading weights and architecture correctly and offers to have a unified API across frameworks.  In the model.yaml file the definition of a Keras model would like this:  type: keras",
            "title": "type"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#args",
            "text": "Model arguments define where the files are files and functions are located to instantiate the model. Most entries of  args  will contain paths to files, those paths are relative to the location of the model.yaml file. The correct definition of  args  depends on the  type  that was selected:",
            "title": "args"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#keras-models",
            "text": "For Keras models the following args are available:  weights ,  arch ,  custom_objects . The Keras framework offers different ways to store model architecture and weights:   Architecture and weights can be stored separately:   type: keras\nargs:\n    arch: model_files/model.json\n    weights: model_files/weights.h5   The architecture can be stored together with the weights:   type: keras\nargs:\n    weights: model_files/model.h5  In Keras models can have custom layers, which then have to be available at the instantiation of the Keras model, those should be stored in one python file that comes with the model architecture and weights. This file defines a dictionary containing custom Keras components called  OBJECTS . These objects will be added to  custom_objects  when loading the model with  keras.models.load_model .  Example of a  custom_keras_objects.py :  from concise.layers import SplineT\n\nOBJECTS = {\"SplineT\": SplineT}  Example of the corresponding model.yaml entry:  type: keras\nargs:\n    ...\n    custom_objects: model_files/custom_keras_objects.py  Here all the objects present in  model_files/custom_keras_objects.py  will be made available to Keras when loading the model.",
            "title": "keras models"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#pytorch-models",
            "text": "Pytorch offers much freedom as to how the model is stored. In Kipoi a pytorch model has the following  args :  file ,  build_fn ,  weights . If  cuda  is available the model will automatically be switched to cuda mode, so the user does not have to take care of that and the  build_fn  should not attempt to do this conversion. The following ways of instantiating a model are supported:   Build function: In the example below Kipoi expects that when calling  get_full_model()  (which is defined in  model_files/model_def.py ) A pytorch model is returned that for which the weights have already been loaded.   type: pytorch\nargs:\n    file: model_files/model_def.py\n    build_fn: get_full_model   Build function + weights: In the example below the model is instantiated by calling  get_model()  which can be found in  model_files/model_def.py . After that the weights will be loaded by executing  model.load_state_dict(torch.load(weights)) .   type: pytorch\nargs:\n    file: model_files/model_def.py\n    build_fn: get_model\n    weights: model_files/weights.pth   Architecture and weights in one file: In this case Kipoi assumes that  model = torch.load(weights)  will be a valid pytorch model. Care has to be taken when storing the architecture a model this way as only standard pytorch layers will be loaded correctly, please see the pytorch documentation for details.   type: pytorch\nargs:\n    weights: model_files/model.pth",
            "title": "pytorch models"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#sklearn-models",
            "text": "SciKit-learn models can be loaded from a pickle file as defined below. The command used for loading is:  joblib.load(pkl_file)  type: sklearn\nargs:\n  pkl_file: model_files/model.pkl",
            "title": "sklearn models"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#tensorflow-models",
            "text": "Tensorflow models are expected to be stored by calling  saver = tf.train.Saver(); saver.save(checkpoint_path) . The  input_nodes  argument is then a string, list of strings or dictionary of strings that define the input node names. The  target_nodes  argument is a string, list of strings or dictionary of strings that define the model target node names.  type: tensorflow\nargs:\n  input_nodes: \"inputs\"\n  target_nodes: \"preds\"\n  checkpoint_path: \"model_files/model.tf\"  If a model requires a constant feed of data which is not provided by the dataloader the  const_feed_dict_pkl  argument can be defined additionally to the above. Values given in the pickle file will be added to the batch samples created by the dataloader. If values with identical keys have been created by the dataloader they will be overwritten with what is given in  const_feed_dict_pkl .  \ntype: tensorflow\nargs:\n  ...\n  const_feed_dict_pkl: \"model_files/const_feed_dict.pkl\"",
            "title": "tensorflow models"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#custom-models",
            "text": "It is possible to defined a model class independent of the ones which are made available in Kipoi. In that case the contributor-defined  Model  class must be a subclass of  BaseModel  defined in  kipoi.model . Custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the  predict_on_batch  function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo!  If for example a custom model class definition ( MyModel ) lies in a file  my_model.py , then the model.yaml will contain:  type: custom\nargs:\n  file: my_model.py\n  object: MyModel  Kipoi will then use an instance of MyModel as a model. Keep in mind that MyModel has to be subclass of  BaseModel , which in other words means that  def predict_on_batch(self, x)  has to be implemented. So if  batch  is for example what the dataloader returns for a batch then  predict_on_batch(batch['inputs'])  has to work.",
            "title": "custom models"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#info",
            "text": "The  info  field of a model.yaml file contains general information about the model.   authors : a list of authors with the field:  name , and the optional fields:  github  and  email . Where the  github  name is the github user id of the respective author  doc : Free text documentation of the model. A short description of what it does and what it is designed for.  version : Model version  license : String indicating the license, if not defined it defaults to  MIT  tags : A list of key words describing the model and its use cases  cite_as : Link to the journal, arXiv, ...  trained_on : Description of the training dataset  training_procedure : Description of the training procedure   A dummy example could look like this:  info:\n  authors:\n    - name: My Name\n      github: myGithubName\n      email: my@email.com\n    - name: Second Author\n  doc: My fancy model description\n  version: 1.0\n  license: GNU\n  tags:\n    - TFBS\n    - tag2\n  cite_as: http://www.the_journal.com/mypublication\n  trained_on: The XXXX dataset from YYYY\n  training_procedure: 10-fold cross validation",
            "title": "info"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#default_dataloader",
            "text": "The  default_dataloader  points to the location of the dataloader.yaml file. By default this will be in the same folder as the model.yaml file, in which case  default_dataloader  doesn't have to be defined.  If dataloader.yaml lies in different subfolder then  default_dataloader: path/to/folder  would be used where dataloader.yaml would lie in  folder .",
            "title": "default_dataloader"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#schema",
            "text": "Schema defines what the model inputs and outputs are, what they consist in and what the dimensions are.  schema  contains two categories  inputs  and  targets  which each specify the shapes of the model input and model output.  In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or  OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition:   A single numpy array as input or target:   schema:\n    inputs:\n       name: seq\n       shape: (1000,4)   A list of numpy arrays as inputs or targets:   schema:\n    targets:\n       - name: seq\n         shape: (1000,4)\n       - name: inp2\n         shape: (10)   A dictionary of numpy arrays as inputs or targets:   schema:\n    inputs:\n       seq:\n         shape: (1000,4)\n       inp2:\n         shape: (10)",
            "title": "schema"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#inputs",
            "text": "The  inputs  fields of  schema  may be lists, dictionaries or single occurences of the following entries:   shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of  (1000, 4)  inputs  shape: (1000, 4)  should be set. If a dimension is of variable size then the numerical should be replaced by  None .  doc : A free text description of the model input  name : Name of model input , not required if input is a dictionary.  special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ), which is important for variant effect prediction.",
            "title": "inputs"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#targets",
            "text": "The  targets  fields of  schema  may be lists, dictionaries or single occurences of the following entries:   shape : Required: Details see in  input  doc : A free text description of the model input  name : Name of model  target, not required if target is a dictionary.  column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line).",
            "title": "targets"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#how-model-types-handle-schemas",
            "text": "The different model types handle those three different encapsulations of numpy arrays differently:",
            "title": "How model types handle schemas"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#keras-type-models",
            "text": "",
            "title": "keras type models"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#input",
            "text": "In case a Keras model is used the batch produced by the dataloader is passed on as it is to the  model.predict_on_batch()  function. So if for example a dictionary is defined in the model.yaml and that is produced by the dataloader then this dicationary is passed on to  model.predict_on_batch() .",
            "title": "Input"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#output",
            "text": "The model is expected to return the schema that is defined in model.yaml. If for example a model returns a list of numpy arrays then that has to be defined correctly in the model.yaml schema.",
            "title": "Output"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#pytorch-type-models",
            "text": "Pytorch needs  torch.autograd.Variable  instances to work. Hence all inputs are automatically converted into  Variable  objects and results are converted back into numpy arrays transparently. If  cuda  is available the model will automatically be used in cuda mode and also the input variables will be switched to  cuda .",
            "title": "pytorch type models"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#input_1",
            "text": "For prediction the following will happen to the tree different encapsulations of input arrays:   A single array: Will be passed directly as the only argument to model call:  model(Variable(from_numpy(x)))  A list of arrays: The model will be called with the list of converted array as args (e.g.:  model(*list_of_variables) )  A dictionary of arrays: The model will be called with the dictionary of converted array as kwargs (e.g.:  model(**dict_of_variables) )",
            "title": "Input"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#output_1",
            "text": "The model return values will be converted back into encapsulations of numpy arrays, where:   a single  Variable  object will be converted into a numpy arrays  lists of  Variable  objects will be converted into a list of numpy arrays in the same order and",
            "title": "Output"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#sklearn-type-models",
            "text": "The batch generated by the dataloader will be passed on directly to the SciKit-learn model using  model.predict(x)",
            "title": "sklearn type models"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#tensorflow-type-models",
            "text": "",
            "title": "tensorflow type models"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#input_2",
            "text": "The  feed_dict  for running a tensorflow session is generated by converting the batch samples into the  feed_dict  using  input_nodes  defined in the  args  section of the model.yaml. For prediction the following will happen to the tree different encapsulations of input arrays:   If  input_nodes  is a single string the model will be fed with a dictionary  {input_ops: x}  If  input_nodes  is a list then the batch is also exptected to be a list in the corresponding order and the feed dict will be created from that.  If  input_nodes  is a dictionary then the batch is also exptected to be a dictionary with the same keys and the feed dict will be created from that.",
            "title": "Input"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#output_2",
            "text": "The return value of the tensorflow model is returned without further transformations and the model outpu schema defined in the  schema  field of model.yaml has to match that.",
            "title": "Output"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#dependencies",
            "text": "One of the core elements of ensuring functionality of a model is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the  conda  and  pip  sections respectively.  Both can either be defined as a list of packages or as a text file (ending in  .txt ) which lists the dependencies.  Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.:  package>=1.0  is very likely to break at some point in future.",
            "title": "dependencies"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#conda",
            "text": "Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in  .txt ).  If conda packages need to be loaded from a channel then the nomenclature  channel_name::package_name  can be used.",
            "title": "conda"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#pip",
            "text": "Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in  .txt ).",
            "title": "pip"
        },
        {
            "location": "/contributing/02_Writing_model.yaml/#postprocessing",
            "text": "The postprocessing section of a model.yaml is necessary to indicate that a model is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your model up for variant effect prediction, please take a look at the documentation of variant effect prediction.",
            "title": "postprocessing"
        },
        {
            "location": "/contributing/04_Writing_dataloader.py/",
            "text": "Dataloader\n\n\nThe main aim of a dataloader is to generate batches of data with which a model can be run. It therefore has to return a dictionary with three keys:\n\n\n\n\ninputs\n\n\ntargets\n (optional)\n\n\nmetadata\n (optional).\n\n\n\n\nAs the names suggest, the \ninputs\n will get feeded to the model to make the predictions and \ntargets\n could be used to train the model. The \nmetadata\n field is used to give additional information about the samples (like sample ID, or genomic ranges for DNA-sequence based models).\n\n\nIn a batch of data returned by the dataloader, all three fields can be further nested - i.e. \ninputs\n can be a list of numpy arrays or a dictionary of numpy arrays. The only restriction is that the leaf objects are numpy arrays and that the first axis (batch dimension) is the same for all arrays.\n\n\nNote that the \ninputs\n and \ntargets\n have to be compatible with the model you are using. Keras, for instance, can accept as inputs and targets all three options: single numpy array, list of numpy arrays, dictionary of numpy arrays (note: to use as input a dictionary of numpy arrays you have to use the functional API and specify the \nname\n fields in the \nkeras.layers.Input\n layer). On the other hand, the Scikit-learn models only allow the inputs and targets to be a single 2-dimensional numpy array.\n\n\nConceptionally, there are three ways how you can write a dataloader. The dataloader can either yield:\n\n\n\n\nindividual samples\n\n\nbatches of data\n\n\nwhole dataset\n\n\n\n\nNote that when a dataloader returns individual samples, the returned numpy arrays shouldn't contain the batch axis. The batch axis will get generated by Kipoi when batching the samples. Also, the samples may contain non-numpy array scalar types like \nbool\n, \nfloat\n, \nint\n, \nstr\n. These will later get stacked into a one-dimensional numpy array.\n\n\nDataloader types\n\n\nSpecifically, a dataloader has to inherit from one of the following classes defined in \nkipoi.data\n:\n\n\n\n\n\n\nPreloadedDataset\n \n\n\n\n\nFunction that returns the whole dataset as a nested dictionary/list of numpy arrays\n\n\nuseful when:\n the dataset is expected to load quickly and fit into the memory\n\n\n\n\n\n\n\n\nDataset\n \n\n\n\n\nClass that inherits from \nkipoi.data.Dataset\n and implements \n__len__\n and \n__getitem__\n methods. \n__getitem__\n returns a single sample from the dataset.\n\n\nuseful when:\n dataset length is easy to infer, there are no significant performance gain when reading data of the disk in batches\n\n\n\n\n\n\n\n\nBatchDataset\n \n\n\n\n\nClass that inherits from \nkipoi.data.BatchDataset\n and implements \n__len__\n and \n__getitem__\n methods. \n__getitem__\n returns a single batch of samples from the dataset.\n\n\nuseful when:\n dataset length is easy to infer, and there is a significant performance gain when reading data of the disk in batches\n\n\n\n\n\n\n\n\nSampleIterator\n \n\n\n\n\nClass that inherits from \nkipoi.data.SampleIterator\n and implements \n__iter__\n and \n__next__\n (\nnext\n in python 2). \n__next__\n returns a single sample from the dataset or raises \nStopIteration\n if all the samples were already returned.\n\n\nuseful when:\n the dataset length is not know in advance or is difficult to infer, and there are no significant performance gain when reading data of the disk in batches\n\n\n\n\n\n\n\n\nBatchIterator\n \n\n\n\n\nClass that inherits from \nkipoi.data.BatchIterator\n and implements \n__iter__\n and \n__next__\n (\nnext\n in python 2). \n__next__\n returns a single batch of samples sample from the dataset or raises \nStopIteration\n if all the samples were already returned.\n\n\nuseful when:\n the dataset length is not know in advance or is difficult to infer, and there is a significant performance gain when reading data of the disk in batches\n\n\n\n\n\n\n\n\nSampleGenerator\n \n\n\n\n\nA generator function that yields a single sample from the dataset and returns when all the samples were yielded.\n\n\nuseful when:\n same as for \nSampleIterator\n, but can be typically implemented in fewer lines of code\n\n\n\n\n\n\n\n\nBatchGenerator\n \n\n\n\n\nA generator function that yields a single batch of samples from the dataset and returns when all the samples were yielded.\n\n\nuseful when:\n same as for \nBatchIterator\n, but can be typically implemented in fewer lines of code\n\n\n\n\n\n\n\n\nHere is a table showing the (recommended) requirements for each dataloader type:\n\n\n\n\n\n\n\n\nDataloader type\n\n\nLength known?\n\n\nSignificant benefit from loading data in batches?\n\n\nFits into memory and loads quickly?\n\n\n\n\n\n\n\n\n\n\nPreloadedDataset\n\n\nyes\n\n\nyes\n\n\nyes\n\n\n\n\n\n\nDataset\n\n\nyes\n\n\nno\n\n\nno\n\n\n\n\n\n\nBatchDataset\n\n\nyes\n\n\nyes\n\n\nno\n\n\n\n\n\n\nSampleIterator\n\n\nno\n\n\nno\n\n\nno\n\n\n\n\n\n\nBatchIterator\n\n\nno\n\n\nyes\n\n\nno\n\n\n\n\n\n\nSampleGenerator\n\n\nno\n\n\nno\n\n\nno\n\n\n\n\n\n\nBatchGenerator\n\n\nno\n\n\nyes\n\n\nno\n\n\n\n\n\n\n\n\nDataset example\n\n\nHere is an example dataloader that gets as input a \nfasta\n file and a \nbed\n file and returns a one-hot encoded sequence (under 'inputs') along with the used genomic interval (under 'metadata/ranges').\n\n\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np\nfrom pybedtools import BedTool\nfrom genomelake.extractors import FastaExtractor\nfrom kipoi.data import Dataset\nfrom kipoi.metadata import GenomicRanges\n\nclass SeqDataset(Dataset):\n    \"\"\"\n    Args:\n        intervals_file: bed3 file containing intervals\n        fasta_file: file path; Genome sequence\n    \"\"\"\n\n    def __init__(self, intervals_file, fasta_file):\n\n        self.bt = BedTool(intervals_file)\n        self.fasta_extractor = FastaExtractor(fasta_file)\n\n    def __len__(self):\n        return len(self.bt)\n\n    def __getitem__(self, idx):\n        interval = self.bt[idx]\n\n        seq = np.squeeze(self.fasta_extractor([interval]), axis=0)\n        return {\n            \"inputs\": seq,\n            # lacks targets\n            \"metadata\": {\n                \"ranges\": GenomicRanges.from_interval(interval)\n            }\n        }\n\n\n\n\nFurther examples\n\n\nTo see examples of other dataloaders, run \nkipoi init\n from the command-line and choose each time a different dataloader_type.\n\n\n$ kipoi init\nINFO [kipoi.cli.main] Initializing a new Kipoi model\n\n...\n\nSelect dataloader_type:\n1 - Dataset\n2 - PreloadedDataset\n3 - BatchDataset\n4 - SampleIterator\n5 - SampleGenerator\n6 - BatchIterator\n7 - BatchGenerator\nChoose from 1, 2, 3, 4, 5, 6, 7 [1]:\n\n\n\n\nThe generated model directory will contain a working implementation of a dataloader.",
            "title": "dataloader.py"
        },
        {
            "location": "/contributing/04_Writing_dataloader.py/#dataloader",
            "text": "The main aim of a dataloader is to generate batches of data with which a model can be run. It therefore has to return a dictionary with three keys:   inputs  targets  (optional)  metadata  (optional).   As the names suggest, the  inputs  will get feeded to the model to make the predictions and  targets  could be used to train the model. The  metadata  field is used to give additional information about the samples (like sample ID, or genomic ranges for DNA-sequence based models).  In a batch of data returned by the dataloader, all three fields can be further nested - i.e.  inputs  can be a list of numpy arrays or a dictionary of numpy arrays. The only restriction is that the leaf objects are numpy arrays and that the first axis (batch dimension) is the same for all arrays.  Note that the  inputs  and  targets  have to be compatible with the model you are using. Keras, for instance, can accept as inputs and targets all three options: single numpy array, list of numpy arrays, dictionary of numpy arrays (note: to use as input a dictionary of numpy arrays you have to use the functional API and specify the  name  fields in the  keras.layers.Input  layer). On the other hand, the Scikit-learn models only allow the inputs and targets to be a single 2-dimensional numpy array.  Conceptionally, there are three ways how you can write a dataloader. The dataloader can either yield:   individual samples  batches of data  whole dataset   Note that when a dataloader returns individual samples, the returned numpy arrays shouldn't contain the batch axis. The batch axis will get generated by Kipoi when batching the samples. Also, the samples may contain non-numpy array scalar types like  bool ,  float ,  int ,  str . These will later get stacked into a one-dimensional numpy array.",
            "title": "Dataloader"
        },
        {
            "location": "/contributing/04_Writing_dataloader.py/#dataloader-types",
            "text": "Specifically, a dataloader has to inherit from one of the following classes defined in  kipoi.data :    PreloadedDataset     Function that returns the whole dataset as a nested dictionary/list of numpy arrays  useful when:  the dataset is expected to load quickly and fit into the memory     Dataset     Class that inherits from  kipoi.data.Dataset  and implements  __len__  and  __getitem__  methods.  __getitem__  returns a single sample from the dataset.  useful when:  dataset length is easy to infer, there are no significant performance gain when reading data of the disk in batches     BatchDataset     Class that inherits from  kipoi.data.BatchDataset  and implements  __len__  and  __getitem__  methods.  __getitem__  returns a single batch of samples from the dataset.  useful when:  dataset length is easy to infer, and there is a significant performance gain when reading data of the disk in batches     SampleIterator     Class that inherits from  kipoi.data.SampleIterator  and implements  __iter__  and  __next__  ( next  in python 2).  __next__  returns a single sample from the dataset or raises  StopIteration  if all the samples were already returned.  useful when:  the dataset length is not know in advance or is difficult to infer, and there are no significant performance gain when reading data of the disk in batches     BatchIterator     Class that inherits from  kipoi.data.BatchIterator  and implements  __iter__  and  __next__  ( next  in python 2).  __next__  returns a single batch of samples sample from the dataset or raises  StopIteration  if all the samples were already returned.  useful when:  the dataset length is not know in advance or is difficult to infer, and there is a significant performance gain when reading data of the disk in batches     SampleGenerator     A generator function that yields a single sample from the dataset and returns when all the samples were yielded.  useful when:  same as for  SampleIterator , but can be typically implemented in fewer lines of code     BatchGenerator     A generator function that yields a single batch of samples from the dataset and returns when all the samples were yielded.  useful when:  same as for  BatchIterator , but can be typically implemented in fewer lines of code     Here is a table showing the (recommended) requirements for each dataloader type:     Dataloader type  Length known?  Significant benefit from loading data in batches?  Fits into memory and loads quickly?      PreloadedDataset  yes  yes  yes    Dataset  yes  no  no    BatchDataset  yes  yes  no    SampleIterator  no  no  no    BatchIterator  no  yes  no    SampleGenerator  no  no  no    BatchGenerator  no  yes  no",
            "title": "Dataloader types"
        },
        {
            "location": "/contributing/04_Writing_dataloader.py/#dataset-example",
            "text": "Here is an example dataloader that gets as input a  fasta  file and a  bed  file and returns a one-hot encoded sequence (under 'inputs') along with the used genomic interval (under 'metadata/ranges').  from __future__ import absolute_import, division, print_function\nimport numpy as np\nfrom pybedtools import BedTool\nfrom genomelake.extractors import FastaExtractor\nfrom kipoi.data import Dataset\nfrom kipoi.metadata import GenomicRanges\n\nclass SeqDataset(Dataset):\n    \"\"\"\n    Args:\n        intervals_file: bed3 file containing intervals\n        fasta_file: file path; Genome sequence\n    \"\"\"\n\n    def __init__(self, intervals_file, fasta_file):\n\n        self.bt = BedTool(intervals_file)\n        self.fasta_extractor = FastaExtractor(fasta_file)\n\n    def __len__(self):\n        return len(self.bt)\n\n    def __getitem__(self, idx):\n        interval = self.bt[idx]\n\n        seq = np.squeeze(self.fasta_extractor([interval]), axis=0)\n        return {\n            \"inputs\": seq,\n            # lacks targets\n            \"metadata\": {\n                \"ranges\": GenomicRanges.from_interval(interval)\n            }\n        }",
            "title": "Dataset example"
        },
        {
            "location": "/contributing/04_Writing_dataloader.py/#further-examples",
            "text": "To see examples of other dataloaders, run  kipoi init  from the command-line and choose each time a different dataloader_type.  $ kipoi init\nINFO [kipoi.cli.main] Initializing a new Kipoi model\n\n...\n\nSelect dataloader_type:\n1 - Dataset\n2 - PreloadedDataset\n3 - BatchDataset\n4 - SampleIterator\n5 - SampleGenerator\n6 - BatchIterator\n7 - BatchGenerator\nChoose from 1, 2, 3, 4, 5, 6, 7 [1]:  The generated model directory will contain a working implementation of a dataloader.",
            "title": "Further examples"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/",
            "text": "dataloader.yaml\n\n\nThe dataloader.yaml file describes how a dataloader for a certain model can be created and how it has to be set up. A model without functional dataloader is as bad as a model that doesn't work, so the correct setup of the dataloader.yaml is essential for the use of a model in the zoo. Make sure you have read \nWriting dataloader.py\n.\n\n\nTo help understand the synthax of YAML please take a look at: \nYAML Synthax Basics\n\n\nHere is an example \ndataloader.yaml\n:\n\n\ntype: Dataset\ndefined_as: dataloader.py::MyDataset  # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py\nargs:\n    features_file:\n        # descr: > allows multi-line fields\n        doc: >\n          Csv file of the Iris Plants Database from\n          http://archive.ics.uci.edu/ml/datasets/Iris features.\n        type: str\n        example: example_files/features.csv  # example files\n    targets_file:\n        doc: >\n          Csv file of the Iris Plants Database targets.\n          Not required for making the prediction.\n        type: str\n        example: example_files/targets.csv\n        optional: True  # if not present, the `targets` field will not be present in the dataloader output\ninfo:\n    authors: \n        - name: Your Name\n          github: your_github_account\n          email: your_email@host.org\n    version: 0.1\n    doc: Model predicting the Iris species\ndependencies:\n    conda:\n      - python=3.5\n      - pandas\n      - numpy\n      - sklearn\noutput_schema:\n    inputs:\n        features:\n            shape: (4,)\n            doc: Features in cm: sepal length, sepal width, petal length, petal width.\n    targets:\n        shape: (3, )\n        doc: One-hot encoded array of classes: setosa, versicolor, virginica.\n    metadata:  # field providing additional information to the samples (not directly required by the model)\n        example_row_number:\n            shape: int\n            doc: Just an example metadata column\n\n\n\n\ntype\n\n\nThe type of the dataloader indicates from which class the dataloader is inherits. It has to be one of the following values:\n\n\n\n\nPreloadedDataset\n\n\nDataset\n\n\nBatchDataset\n\n\nSampleIterator\n\n\nSampleGenerator\n\n\nBatchIterator\n\n\nBatchGenerator\n\n\n\n\ndefined_as\n\n\ndefined_as\n indicates where the dataloader class can be found. It is a string value of \npath/to/file.py::class_name\n with a the relative path from where the dataloader.yaml lies. E.g.: \nmodel_files/dataloader.py::MyDataLoader\n.\n\n\nThis class will then be instantiated by Kipoi with keyword arguments that have to be mentioned explicitely in \nargs\n (see below).\n\n\nargs\n\n\nA dataloader will always require arguments, they might for example be a path to the reference genome fasta file, a bed file that defines which regions should be investigated, etc. Dataloader arguments are given defined as a yaml dictionary with argument names as keys, e.g.:\n\n\nargs:\n   reference_fasta:\n       example: example_files/chr22.fa\n   argument_2:\n       example: example_files/example_input.txt\n\n\n\n\nAn argument has the following fields:\n\n\n\n\ndoc\n: A free text field describing the argument\n\n\nexample\n: A value that can be used to demonstrate the functionality of the dataloader and of the entire model. Those example files are very useful for users and for automatic testing procedures. For example the command line call \nkipoi test\n uses the exmaple values given for dataloader arguments to assess that a model can be used and is functional. It is therefore important to submit all necessary example files with the model.\n\n\ntype\n: Optional: datatype of the argument (\nstr\n, \nbool\n, \nint\n, \nfloat\n)\n\n\noptional\n: Optional: Boolean flag (\ntrue\n / \nfalse\n) for an argument if it is optional.\n\n\n\n\ninfo\n\n\nThe \ninfo\n field of a dataloader.yaml file contains general information about the model.\n\n\n\n\nauthors\n: a list of authors with the field: \nname\n, and the optional fields: \ngithub\n and \nemail\n. Where the \ngithub\n name is the github user id of the respective author\n\n\ndoc\n: Free text documentation of the dataloader. A short description of what it does.\n\n\nversion\n: Version of the dataloader\n\n\nlicense\n: String indicating the license, if not defined it defaults to \nMIT\n\n\ntags\n: A list of key words describing the dataloader and its use cases\n\n\n\n\nA dummy example could look like this:\n\n\ninfo:\n  authors:\n    - name: My Name\n      github: myGithubName\n      email: my@email.com\n  doc: Datalaoder for my fancy model description\n  version: 1.0\n  license: GNU\n  tags:\n    - TFBS\n    - tag2\n\n\n\n\noutput_schema\n\n\noutput_schema\n defines what the dataloader outputs are, what they consist in, what the dimensions are and some additional meta data.\n\n\noutput_schema\n contains three categories \ninputs\n, \ntargets\n and \nmetadata\n. \ninputs\n and \ntargets\n each specify the shapes of data generated for the model input and model. Offering the \ntargets\n option enables the opportunity to possibly train models with the same dataloader.\n\n\nIn general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or \nOrderedDict\n) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition:\n\n\n\n\nA single numpy array as input or target:\n\n\n\n\noutput_schema:\n    inputs:\n       name: seq\n       shape: (1000,4)\n\n\n\n\n\n\nA list of numpy arrays as inputs or targets:\n\n\n\n\noutput_schema:\n    targets:\n       - name: seq\n         shape: (1000,4)\n       - name: inp2\n         shape: (10)\n\n\n\n\n\n\nA list of numpy arrays as inputs or targets:\n\n\n\n\noutput_schema:\n    inputs:\n       seq:\n         shape: (1000,4)\n       inp2:\n         shape: (10)\n\n\n\n\ninputs\n\n\nThe \ninputs\n fields of \noutput_schema\n may be lists, dictionaries or single occurences of the following entries:\n\n\n\n\nshape\n: Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of \n(1000, 4)\n inputs \nshape: (1000, 4)\n should be set. If a dimension is of variable size then the numerical should be replaced by \nNone\n.\n\n\ndoc\n: A free text description of the model input\n\n\nname\n: Name of model input, not required if input is a dictionary.\n\n\nspecial_type\n: Possibility to flag that respective input is a 1-hot encoded DNA sequence (\nspecial_type: DNASeq\n) or a string DNA sequence (\nspecial_type: DNAStringSeq\n), which is important for variant effect prediction.\n\n\nassociated_metadata\n: Link the respective model input to metadata, such as a genomic region. E.g: If model input is a DNA sequence, then metadata may contain the genomic region from where it was extracted. If the associated \nmetadata\n field is called \nranges\n then \nassociated_metadata: ranges\n has to be set.\n\n\n\n\ntargets\n\n\nThe \ntargets\n fields of \nschema\n may be lists, dictionaries or single occurences of the following entries:\n\n\n\n\nshape\n: Required: Details see in \ninput\n\n\ndoc\n: A free text description of the model target\n\n\nname\n: Name of model  target, not required if target is a dictionary.\n\n\ncolumn_labels\n: Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line).\n\n\n\n\nmetadata\n\n\nMetadata fields capture additional information on the data generated by the dataloader. So for example a model input can be linked to a metadata field using its \nassociated_metadata\n flag (see above). The metadata fields themselves are yaml dictionaries where the name of the metadata field is the key of dictionary and possible attributes are:\n\n\n\n\ndoc\n: A free text description of the metadata element\n\n\ntype\n: The datatype of the metadata field: \nstr\n, \nint\n, \nfloat\n, \narray\n, \nGenomicRanges\n. Where the convenience class \nGenomicRanges\n is defined in \nkipoi.metadata\n, which is essentially an in-memory representation of a bed file.\n\n\n\n\nDefinition of \nmetadata\n is essential for postprocessing algorihms as variant effect prediction. Please refer to their detailed description for their requirements.\n\n\nAn example of the defintion of dataloader.yaml with \nmetadata\n can be seen here:\n\n\noutput_schema:\n    inputs:\n       - name: seq\n         shape: (1000,4)\n         associated_metadata: my_ranges\n       - name: inp2\n         shape: (10)\n    ...\n    metadata:\n         my_ranges:\n            type: GenomicRanges\n            doc: Region from where inputs.seq was extracted\n\n\n\n\ndependencies\n\n\nOne of the core elements of ensuring functionality of a dataloader is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the \nconda\n and \npip\n sections respectively.\n\n\nBoth can either be defined as a list of packages or as a text file (ending in \n.txt\n) which lists the dependencies.\n\n\nConda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.: \npackage>=1.0\n is very likely to break at some point in future.\n\n\nconda\n\n\nConda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in \n.txt\n).\n\n\nIf conda packages need to be loaded from a channel then the nomenclature \nchannel_name::package_name\n can be used.\n\n\npip\n\n\nPip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in \n.txt\n).\n\n\npostprocessing\n\n\nThe postprocessing section of a dataloader.yaml is necessary to indicate that a dataloader is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your dataloader up for variant effect prediction, please take a look at the documentation of variant effect prediction.",
            "title": "dataloader.yaml"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#dataloaderyaml",
            "text": "The dataloader.yaml file describes how a dataloader for a certain model can be created and how it has to be set up. A model without functional dataloader is as bad as a model that doesn't work, so the correct setup of the dataloader.yaml is essential for the use of a model in the zoo. Make sure you have read  Writing dataloader.py .  To help understand the synthax of YAML please take a look at:  YAML Synthax Basics  Here is an example  dataloader.yaml :  type: Dataset\ndefined_as: dataloader.py::MyDataset  # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py\nargs:\n    features_file:\n        # descr: > allows multi-line fields\n        doc: >\n          Csv file of the Iris Plants Database from\n          http://archive.ics.uci.edu/ml/datasets/Iris features.\n        type: str\n        example: example_files/features.csv  # example files\n    targets_file:\n        doc: >\n          Csv file of the Iris Plants Database targets.\n          Not required for making the prediction.\n        type: str\n        example: example_files/targets.csv\n        optional: True  # if not present, the `targets` field will not be present in the dataloader output\ninfo:\n    authors: \n        - name: Your Name\n          github: your_github_account\n          email: your_email@host.org\n    version: 0.1\n    doc: Model predicting the Iris species\ndependencies:\n    conda:\n      - python=3.5\n      - pandas\n      - numpy\n      - sklearn\noutput_schema:\n    inputs:\n        features:\n            shape: (4,)\n            doc: Features in cm: sepal length, sepal width, petal length, petal width.\n    targets:\n        shape: (3, )\n        doc: One-hot encoded array of classes: setosa, versicolor, virginica.\n    metadata:  # field providing additional information to the samples (not directly required by the model)\n        example_row_number:\n            shape: int\n            doc: Just an example metadata column",
            "title": "dataloader.yaml"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#type",
            "text": "The type of the dataloader indicates from which class the dataloader is inherits. It has to be one of the following values:   PreloadedDataset  Dataset  BatchDataset  SampleIterator  SampleGenerator  BatchIterator  BatchGenerator",
            "title": "type"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#defined_as",
            "text": "defined_as  indicates where the dataloader class can be found. It is a string value of  path/to/file.py::class_name  with a the relative path from where the dataloader.yaml lies. E.g.:  model_files/dataloader.py::MyDataLoader .  This class will then be instantiated by Kipoi with keyword arguments that have to be mentioned explicitely in  args  (see below).",
            "title": "defined_as"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#args",
            "text": "A dataloader will always require arguments, they might for example be a path to the reference genome fasta file, a bed file that defines which regions should be investigated, etc. Dataloader arguments are given defined as a yaml dictionary with argument names as keys, e.g.:  args:\n   reference_fasta:\n       example: example_files/chr22.fa\n   argument_2:\n       example: example_files/example_input.txt  An argument has the following fields:   doc : A free text field describing the argument  example : A value that can be used to demonstrate the functionality of the dataloader and of the entire model. Those example files are very useful for users and for automatic testing procedures. For example the command line call  kipoi test  uses the exmaple values given for dataloader arguments to assess that a model can be used and is functional. It is therefore important to submit all necessary example files with the model.  type : Optional: datatype of the argument ( str ,  bool ,  int ,  float )  optional : Optional: Boolean flag ( true  /  false ) for an argument if it is optional.",
            "title": "args"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#info",
            "text": "The  info  field of a dataloader.yaml file contains general information about the model.   authors : a list of authors with the field:  name , and the optional fields:  github  and  email . Where the  github  name is the github user id of the respective author  doc : Free text documentation of the dataloader. A short description of what it does.  version : Version of the dataloader  license : String indicating the license, if not defined it defaults to  MIT  tags : A list of key words describing the dataloader and its use cases   A dummy example could look like this:  info:\n  authors:\n    - name: My Name\n      github: myGithubName\n      email: my@email.com\n  doc: Datalaoder for my fancy model description\n  version: 1.0\n  license: GNU\n  tags:\n    - TFBS\n    - tag2",
            "title": "info"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#output_schema",
            "text": "output_schema  defines what the dataloader outputs are, what they consist in, what the dimensions are and some additional meta data.  output_schema  contains three categories  inputs ,  targets  and  metadata .  inputs  and  targets  each specify the shapes of data generated for the model input and model. Offering the  targets  option enables the opportunity to possibly train models with the same dataloader.  In general model inputs and outputs can either be a numpy array, a list of numpy arrays or a dictionary (or  OrderedDict ) of numpy arrays. Whatever format is defined in the schema is expected to be produced by the dataloader and is expected to be accepted as input by the model. The three different kinds are represented by the single entries, lists or dictionaries in the yaml definition:   A single numpy array as input or target:   output_schema:\n    inputs:\n       name: seq\n       shape: (1000,4)   A list of numpy arrays as inputs or targets:   output_schema:\n    targets:\n       - name: seq\n         shape: (1000,4)\n       - name: inp2\n         shape: (10)   A list of numpy arrays as inputs or targets:   output_schema:\n    inputs:\n       seq:\n         shape: (1000,4)\n       inp2:\n         shape: (10)",
            "title": "output_schema"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#inputs",
            "text": "The  inputs  fields of  output_schema  may be lists, dictionaries or single occurences of the following entries:   shape : Required: A tuple defining the shape of a single input sample. E.g. for a model that predicts a batch of  (1000, 4)  inputs  shape: (1000, 4)  should be set. If a dimension is of variable size then the numerical should be replaced by  None .  doc : A free text description of the model input  name : Name of model input, not required if input is a dictionary.  special_type : Possibility to flag that respective input is a 1-hot encoded DNA sequence ( special_type: DNASeq ) or a string DNA sequence ( special_type: DNAStringSeq ), which is important for variant effect prediction.  associated_metadata : Link the respective model input to metadata, such as a genomic region. E.g: If model input is a DNA sequence, then metadata may contain the genomic region from where it was extracted. If the associated  metadata  field is called  ranges  then  associated_metadata: ranges  has to be set.",
            "title": "inputs"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#targets",
            "text": "The  targets  fields of  schema  may be lists, dictionaries or single occurences of the following entries:   shape : Required: Details see in  input  doc : A free text description of the model target  name : Name of model  target, not required if target is a dictionary.  column_labels : Labels for the tasks of a multitask matrix output. Can be the file name of a text file containing the task labels (one label per line).",
            "title": "targets"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#metadata",
            "text": "Metadata fields capture additional information on the data generated by the dataloader. So for example a model input can be linked to a metadata field using its  associated_metadata  flag (see above). The metadata fields themselves are yaml dictionaries where the name of the metadata field is the key of dictionary and possible attributes are:   doc : A free text description of the metadata element  type : The datatype of the metadata field:  str ,  int ,  float ,  array ,  GenomicRanges . Where the convenience class  GenomicRanges  is defined in  kipoi.metadata , which is essentially an in-memory representation of a bed file.   Definition of  metadata  is essential for postprocessing algorihms as variant effect prediction. Please refer to their detailed description for their requirements.  An example of the defintion of dataloader.yaml with  metadata  can be seen here:  output_schema:\n    inputs:\n       - name: seq\n         shape: (1000,4)\n         associated_metadata: my_ranges\n       - name: inp2\n         shape: (10)\n    ...\n    metadata:\n         my_ranges:\n            type: GenomicRanges\n            doc: Region from where inputs.seq was extracted",
            "title": "metadata"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#dependencies",
            "text": "One of the core elements of ensuring functionality of a dataloader is to define software dependencies correctly and strictly. Dependencies can be defined for conda and for pip using the  conda  and  pip  sections respectively.  Both can either be defined as a list of packages or as a text file (ending in  .txt ) which lists the dependencies.  Conda as well as pip dependencies can and should be defined with exact versions of the required packages, as defining a package version using e.g.:  package>=1.0  is very likely to break at some point in future.",
            "title": "dependencies"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#conda",
            "text": "Conda dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in  .txt ).  If conda packages need to be loaded from a channel then the nomenclature  channel_name::package_name  can be used.",
            "title": "conda"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#pip",
            "text": "Pip dependencies can be defined as lists or if the dependencies are defined in a text file then the path of the text must be given (ending in  .txt ).",
            "title": "pip"
        },
        {
            "location": "/contributing/03_Writing_dataloader.yaml/#postprocessing",
            "text": "The postprocessing section of a dataloader.yaml is necessary to indicate that a dataloader is compatible with a certain kind of postprocessing feature available in Kipoi. At the moment only variant effect prediction is available for postprocessing. To understand how to set your dataloader up for variant effect prediction, please take a look at the documentation of variant effect prediction.",
            "title": "postprocessing"
        },
        {
            "location": "/contributing/05_Writing_model.py/",
            "text": "model.py\n\n\nCustom models enable using any other framework or non-deep learning predictive model to be integrated within Kipoi. In general it is highly advisable not to use custom models if there is an implementation for the model that should be integrated, in other words: If your model is a pytorch model, please use the pytorch model type in Kipoi rather than defining your own custom model type.\n\n\nAlso, custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the \npredict_on_batch\n function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo!\n\n\nThe use of a custom model requires definition of a Kipoi-compliant model object, which can then be referred to by the model.yaml file. The model class has to be a subclass of \nBaseModel\n defined in \nkipoi.model\n, which in other words means that \ndef predict_on_batch(self, x)\n has to be implemented. So for example if \nbatch\n is  what the dataloader returns for a batch then \npredict_on_batch(batch['inputs'])\n has to run the model prediction on the given input.\n\n\nA very simple version of such a model definition that can be stored in for example \nmodel_files/model.py\n may be:\n\n\nfrom kipoi.model import BaseModel\n\ndef load_my_model():\n    # Loading code here\n    return model\n\nclass MyModel(BaseModel):\n    def __init__(self):\n        self.model = load_my_model()\n\n    # Execute model prediction for input data\n    def predict_on_batch(self, x):\n        return self.model.predict(x)\n\n\n\n\nThis can then be integrated in the model.yaml in the following way:\n\n\ntype: custom\nargs:\n  file: model_files/model.py\n  object: MyModel\n...",
            "title": "model.py"
        },
        {
            "location": "/contributing/05_Writing_model.py/#modelpy",
            "text": "Custom models enable using any other framework or non-deep learning predictive model to be integrated within Kipoi. In general it is highly advisable not to use custom models if there is an implementation for the model that should be integrated, in other words: If your model is a pytorch model, please use the pytorch model type in Kipoi rather than defining your own custom model type.  Also, custom models should never deviate from using only numpy arrays, lists thereof, or dictionaries thereof as input for the  predict_on_batch  function. This is essential to maintain a homogeneous and clear interface between dataloaders and models in the Kipoi zoo!  The use of a custom model requires definition of a Kipoi-compliant model object, which can then be referred to by the model.yaml file. The model class has to be a subclass of  BaseModel  defined in  kipoi.model , which in other words means that  def predict_on_batch(self, x)  has to be implemented. So for example if  batch  is  what the dataloader returns for a batch then  predict_on_batch(batch['inputs'])  has to run the model prediction on the given input.  A very simple version of such a model definition that can be stored in for example  model_files/model.py  may be:  from kipoi.model import BaseModel\n\ndef load_my_model():\n    # Loading code here\n    return model\n\nclass MyModel(BaseModel):\n    def __init__(self):\n        self.model = load_my_model()\n\n    # Execute model prediction for input data\n    def predict_on_batch(self, x):\n        return self.model.predict(x)  This can then be integrated in the model.yaml in the following way:  type: custom\nargs:\n  file: model_files/model.py\n  object: MyModel\n...",
            "title": "model.py"
        },
        {
            "location": "/contributing/06_dumping_models_programatically/",
            "text": "Contributing multiple very similar models\n\n\nConsider an example where multiple models were trained, each for a different cell-lines (case for CpGenie). Here is the final folder structure of the contributed model \ngroup\n (simplifed from te \nCpGenie\n model)\n\n\ncell_line_1\n\u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py\n\u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml\n\u251c\u2500\u2500 example_files -> ../template/example_files\n\u251c\u2500\u2500 model_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 model.h5\n\u2514\u2500\u2500 model.yaml -> ../template/model.yaml\n\ncell_line_2\n\u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py\n\u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml\n\u251c\u2500\u2500 example_files -> ../template/example_files\n\u251c\u2500\u2500 model_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 model.h5\n\u2514\u2500\u2500 model.yaml -> ../template/model.yaml\n\ntemplate\n\u251c\u2500\u2500 dataloader.py\n\u251c\u2500\u2500 dataloader.yaml\n\u251c\u2500\u2500 example_files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hg38_chr22.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hg38_chr22.fa.fai\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 intervals.bed\n\u2514\u2500\u2500 model.yaml\nMakefile\ntest_subset.txt\n\n\n\n\ntemplate/\n folder\n\n\nThe \ntemplate/\n folder should contain all the common files or templates. This directory is ignored when listing models.\n\n\nSoftlinks\n\n\nOne option to prevent code duplication is to use soft-links. In the simplest case (as shown above), all files except model weights can be shared accross models. When selectively downloading files from git-lfs, Kipoi also considers soft-links and downloads the original files (e.g. when running \nkipoi predict my_model/cell_line_1 ...\n, the git-lfs files in \nmy_model/template\n will also get downloaded).\n\n\nNote\n Make sure you are using \nrelative\n soft-links (as shown above). \n\n\n# example code-snippet to dump of multiple Keras models\n# and to softlink the remaining files\n\ndef get_model(cell_line):\n    \"\"\"Returns the Keras model\"\"\"\n    pass\n\n\ndef write_model(root_path, cell_line):\n    \"\"\"For a particular cell_line:\n    - write out the model\n    - softlink the other files from `template/`\n    \"\"\"\n    model_dir = os.path.join(root_path, cell_line)\n    os.makedirs(os.path.join(model_dir, \"model_files\"), exist_ok=True)\n\n    model = get_model(cell_line)\n\n    model.save(os.path.join(model_dir, \"model_files/model.h5\"))\n\n    symlink_files = [\"model.yaml\", \n                     \"example_files\", \n                     \"dataloader.yaml\", \n                     \"dataloader.py\"]\n    for f in symlink_files:\n        os.symlink(os.path.join(root_path, \"template\", f),\n                   os.path.join(model_dir, f))\n\n\nfor cell_line in all_cell_lines:\n    write_model(\"my_model_path\", cell_line)\n\n\n\n\nJinja templating\n\n\nAnother option is to use template engines. Template engines are heavily used in web-development to dynamically generate html files. One of the most popular template engines is \njinja\n. Template engines offer more flexibility over softlinks. With softlinks you can only re-use the whole file, while with templating you can choose which pieces of the file are shared and which ones are specific to each model.\n\n\n# template_model.yaml\ntype: keras\nargs:\n    weights: model_files/model.h5\n...\ninfo:\n  trained_on: DNase-seq of {{cell_line}} cell line\n  ...\nschema:\n    ...\n    targets:\n      name: output\n      doc: DNA accessibility in {{cell_line}} cell line\n\n\n\n\n# Script to generate <cell line>/model.yaml from template_model.yaml\nimport os\nfrom jinja2 import Template\n\ndef render_template(template_path, output_path, context, mkdir=False):\n    \"\"\"Render template with jinja\n\n    Args:\n      template_path: path to the jinja template\n      output_path: path where to write the rendered template\n      context: Dictionary containing context variable\n    \"\"\"\n    if mkdir:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    with open(template_path, \"r\") as f:\n        template = Template(f.read())\n    out = template.render(**context)\n    with open(output_path, \"w\") as f:\n        f.write(out)\n\n\ndef write_model_yaml(root_path, cell_line):\n    \"\"\"For a particular cell_line:\n    - Generate `{cell_line}/model.yaml`\n    \"\"\"\n\n    render_template(os.path.join(root_path, \"template\", \"template_model.yaml\"),\n                    os.path.join(root_dir, cell_line, \"model.yaml\"),\n                    context={\"cell_line\": cell_line},\n                    mkdir=True)\n\n\n\n\nImporting common functions, classes\n\n\nIn case the dataloaders or custom models vary between models and we want to re-use python code, we can import objects from modules in the \ntemplate/\n directory:\n\n\nimport os\nimport inspect\n\n# Get the directory of this python file\nfilename = inspect.getframeinfo(inspect.currentframe()).filename\nthis_path = os.path.dirname(os.path.abspath(filename))\n\n# attach template to pythonpath\nimport sys\nsys.path.append(os.path.join(this_path, \"../template\"))\n\nfrom model_template import TemplateModel\n\nclass SpecificModel(TemplateModel):\n    def __init__(self):\n        super(SpecificModel, self).__init__(arg1=\"value\")\n\n\n\n\ntest_subset.txt\n - Testing only some models\n\n\nSince many models are essentially the same, the automatic tests should only test one or few models. To specify which models to test,\nwrite the \ntest_subset.txt\n file in the same directory level as the \ntemplate/\n folder and list the models you want to test.\n\n\nExamples:\n\n\nCpGenie/test_subset.txt\n: \n\n\nGM19239_ENCSR000DGH\nmerged\n\n\n\n\n\nrbp_eclip/test_subset.txt\n: \n\n\nAARS\n\n\n\n\nReproducible script\n\n\nRegardless of which approch you choose to take, consider writing a single script/Makefile in the model-group root (at the same directory level as \ntemplate/\n). The script/Makefile should generate or softlink all the files given the template folder, making it easier to update the files later.",
            "title": "Multiple very similar models"
        },
        {
            "location": "/contributing/06_dumping_models_programatically/#contributing-multiple-very-similar-models",
            "text": "Consider an example where multiple models were trained, each for a different cell-lines (case for CpGenie). Here is the final folder structure of the contributed model  group  (simplifed from te  CpGenie  model)  cell_line_1\n\u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py\n\u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml\n\u251c\u2500\u2500 example_files -> ../template/example_files\n\u251c\u2500\u2500 model_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 model.h5\n\u2514\u2500\u2500 model.yaml -> ../template/model.yaml\n\ncell_line_2\n\u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py\n\u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml\n\u251c\u2500\u2500 example_files -> ../template/example_files\n\u251c\u2500\u2500 model_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 model.h5\n\u2514\u2500\u2500 model.yaml -> ../template/model.yaml\n\ntemplate\n\u251c\u2500\u2500 dataloader.py\n\u251c\u2500\u2500 dataloader.yaml\n\u251c\u2500\u2500 example_files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hg38_chr22.fa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hg38_chr22.fa.fai\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 intervals.bed\n\u2514\u2500\u2500 model.yaml\nMakefile\ntest_subset.txt",
            "title": "Contributing multiple very similar models"
        },
        {
            "location": "/contributing/06_dumping_models_programatically/#template-folder",
            "text": "The  template/  folder should contain all the common files or templates. This directory is ignored when listing models.",
            "title": "template/ folder"
        },
        {
            "location": "/contributing/06_dumping_models_programatically/#softlinks",
            "text": "One option to prevent code duplication is to use soft-links. In the simplest case (as shown above), all files except model weights can be shared accross models. When selectively downloading files from git-lfs, Kipoi also considers soft-links and downloads the original files (e.g. when running  kipoi predict my_model/cell_line_1 ... , the git-lfs files in  my_model/template  will also get downloaded).  Note  Make sure you are using  relative  soft-links (as shown above).   # example code-snippet to dump of multiple Keras models\n# and to softlink the remaining files\n\ndef get_model(cell_line):\n    \"\"\"Returns the Keras model\"\"\"\n    pass\n\n\ndef write_model(root_path, cell_line):\n    \"\"\"For a particular cell_line:\n    - write out the model\n    - softlink the other files from `template/`\n    \"\"\"\n    model_dir = os.path.join(root_path, cell_line)\n    os.makedirs(os.path.join(model_dir, \"model_files\"), exist_ok=True)\n\n    model = get_model(cell_line)\n\n    model.save(os.path.join(model_dir, \"model_files/model.h5\"))\n\n    symlink_files = [\"model.yaml\", \n                     \"example_files\", \n                     \"dataloader.yaml\", \n                     \"dataloader.py\"]\n    for f in symlink_files:\n        os.symlink(os.path.join(root_path, \"template\", f),\n                   os.path.join(model_dir, f))\n\n\nfor cell_line in all_cell_lines:\n    write_model(\"my_model_path\", cell_line)",
            "title": "Softlinks"
        },
        {
            "location": "/contributing/06_dumping_models_programatically/#jinja-templating",
            "text": "Another option is to use template engines. Template engines are heavily used in web-development to dynamically generate html files. One of the most popular template engines is  jinja . Template engines offer more flexibility over softlinks. With softlinks you can only re-use the whole file, while with templating you can choose which pieces of the file are shared and which ones are specific to each model.  # template_model.yaml\ntype: keras\nargs:\n    weights: model_files/model.h5\n...\ninfo:\n  trained_on: DNase-seq of {{cell_line}} cell line\n  ...\nschema:\n    ...\n    targets:\n      name: output\n      doc: DNA accessibility in {{cell_line}} cell line  # Script to generate <cell line>/model.yaml from template_model.yaml\nimport os\nfrom jinja2 import Template\n\ndef render_template(template_path, output_path, context, mkdir=False):\n    \"\"\"Render template with jinja\n\n    Args:\n      template_path: path to the jinja template\n      output_path: path where to write the rendered template\n      context: Dictionary containing context variable\n    \"\"\"\n    if mkdir:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    with open(template_path, \"r\") as f:\n        template = Template(f.read())\n    out = template.render(**context)\n    with open(output_path, \"w\") as f:\n        f.write(out)\n\n\ndef write_model_yaml(root_path, cell_line):\n    \"\"\"For a particular cell_line:\n    - Generate `{cell_line}/model.yaml`\n    \"\"\"\n\n    render_template(os.path.join(root_path, \"template\", \"template_model.yaml\"),\n                    os.path.join(root_dir, cell_line, \"model.yaml\"),\n                    context={\"cell_line\": cell_line},\n                    mkdir=True)",
            "title": "Jinja templating"
        },
        {
            "location": "/contributing/06_dumping_models_programatically/#importing-common-functions-classes",
            "text": "In case the dataloaders or custom models vary between models and we want to re-use python code, we can import objects from modules in the  template/  directory:  import os\nimport inspect\n\n# Get the directory of this python file\nfilename = inspect.getframeinfo(inspect.currentframe()).filename\nthis_path = os.path.dirname(os.path.abspath(filename))\n\n# attach template to pythonpath\nimport sys\nsys.path.append(os.path.join(this_path, \"../template\"))\n\nfrom model_template import TemplateModel\n\nclass SpecificModel(TemplateModel):\n    def __init__(self):\n        super(SpecificModel, self).__init__(arg1=\"value\")",
            "title": "Importing common functions, classes"
        },
        {
            "location": "/contributing/06_dumping_models_programatically/#test_subsettxt-testing-only-some-models",
            "text": "Since many models are essentially the same, the automatic tests should only test one or few models. To specify which models to test,\nwrite the  test_subset.txt  file in the same directory level as the  template/  folder and list the models you want to test.  Examples:  CpGenie/test_subset.txt :   GM19239_ENCSR000DGH\nmerged  rbp_eclip/test_subset.txt :   AARS",
            "title": "test_subset.txt - Testing only some models"
        },
        {
            "location": "/contributing/06_dumping_models_programatically/#reproducible-script",
            "text": "Regardless of which approch you choose to take, consider writing a single script/Makefile in the model-group root (at the same directory level as  template/ ). The script/Makefile should generate or softlink all the files given the template folder, making it easier to update the files later.",
            "title": "Reproducible script"
        },
        {
            "location": "/tutorials/contributing_models/",
            "text": "Generated from \nnbs/contributing_models.ipynb\n\n\nContributing a model to the Kipoi model repository\n\n\nThis notebook will show you how to contribute a model to the \nKipoi model repository\n. For a simple 'model contribution checklist' see also \nhttp://kipoi.org/docs/contributing/01_Getting_started/\n.\n\n\nKipoi basics\n\n\nContributing a model to Kipoi means writing a sub-folder with all the required files to the \nKipoi model repository\n via pull request.\n\n\nTwo main components of the model repository are \nmodel\n and \ndataloader\n.\n\n\n\n\nModel\n\n\nModel takes as input numpy arrays and outputs numpy arrays. In practice, a model needs to implement the \npredict_on_batch(x)\n method, where \nx\n is dictionary/list of numpy arrays. The model contributor needs to provide one of the following:\n\n\n\n\nSerialized Keras model\n\n\nSerialized Sklearn model\n\n\nCustom model inheriting from \nkeras.model.BaseModel\n.\n\n\nall the required files, i.e. weights need to be loaded in the \n__init__\n\n\n\n\nSee \nhttp://kipoi.org/docs/contributing/02_Writing_model.yaml/\n and \nhttp://kipoi.org/docs/contributing/05_Writing_model.py/\n for more info.\n\n\nDataloader\n\n\nDataloader takes raw file paths or other parameters as argument and outputs modelling-ready numpy arrays. The dataloading can be done through a generator---batch-by-batch, sample-by-sample---or by just returning the whole dataset. The goal is to work really with raw files (say fasta, bed, vcf, etc in bioinformatics), as this allows to make model predictions on new datasets without going through the burden of running custom pre-processing scripts. The model contributor needs to implement one of the following:\n\n\n\n\nPreloadedDataset\n\n\nDataset\n\n\nBatchDataset\n\n\nSampleIterator\n\n\nBatchIterator\n\n\nSampleGenerator\n\n\nBatchGenerator\n\n\n\n\nSee \nhttp://kipoi.org/docs/contributing/04_Writing_dataloader.py/\n for more info.\n\n\nFolder layout\n\n\nHere is an example folder structure of a Kipoi model:\n\n\n\u251c\u2500\u2500 dataloader.py     # implements the dataloader\n\u251c\u2500\u2500 dataloader.yaml   # describes the dataloader\n\u251c\u2500\u2500 dataloader_files/      #/ files required by the dataloader\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 x_transfomer.pkl\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 y_transfomer.pkl\n\u251c\u2500\u2500 model.yaml        # describes the model\n\u251c\u2500\u2500 model_files/           #/ files required by the model\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 model.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 weights.h5\n\u2514\u2500\u2500 example_files/         #/ small example files\n    \u251c\u2500\u2500 features.csv\n    \u2514\u2500\u2500 targets.csv\n\n\n\n\nTwo most important files are \nmodel.yaml\n and \ndataloader.yaml\n. They provide a complete description about the model, the dataloader and the files they depend on.\n\n\nContributing a simple Iris-classifier\n\n\nDetails about the individual files will be revealed throught the tutorial below. A simple Keras model will be trained to predict the Iris plant class from the well-known \nIris\n dataset.\n\n\nOutline\n\n\n\n\nTrain the model\n\n\nGenerate \ndataloader_files/\n\n\nGenerate \nmodel_files/\n\n\nGenerate \nexample_files/\n\n\nWrite \nmodel.yaml\n\n\nWrite \ndataloader.yaml\n\n\nWrite \ndataloader.py\n\n\nTest with the model with \n$ kipoi test .\n\n\n\n\n1. Train the model\n\n\nLoad and pre-process the data\n\n\nimport pandas as pd\nimport os\nfrom sklearn.preprocessing import LabelBinarizer, StandardScaler\n\nfrom sklearn import datasets\niris = datasets.load_iris()\n\n\n\n\n# view more info about the dataset\n# print(iris[\"DESCR\"])\n\n\n\n\n# Data pre-processing\ny_transformer = LabelBinarizer().fit(iris[\"target\"])\nx_transformer = StandardScaler().fit(iris[\"data\"])\n\n\n\n\nx = x_transformer.transform(iris[\"data\"])\ny = y_transformer.transform(iris[\"target\"])\n\n\n\n\nx[:3]\n\n\n\n\narray([[-0.9007,  1.0321, -1.3413, -1.313 ],\n       [-1.143 , -0.125 , -1.3413, -1.313 ],\n       [-1.3854,  0.3378, -1.3981, -1.313 ]])\n\n\n\ny[:3]\n\n\n\n\narray([[1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0]])\n\n\n\nTrain an example model\n\n\nLet's train a simple linear-regression model using Keras.\n\n\nfrom keras.models import Model\nimport keras.layers as kl\n\ninp = kl.Input(shape=(4, ), name=\"features\")\nout = kl.Dense(units=3)(inp)\nmodel = Model(inp, out)\nmodel.compile(\"adam\", \"categorical_crossentropy\")\n\nmodel.fit(x, y, verbose=0)\n\n\n\n\nUsing TensorFlow backend.\n\n\n\n\n\n<keras.callbacks.History at 0x7f5c537f2d68>\n\n\n\n2. Generate \ndataloader_files/\n\n\nNow that we have everything we need, let's start writing the files to model's directory (here \nmodel_template/\n). \n\n\nIn reality, you would need to \n\n\n\n\nFork the \nkipoi/models repository\n\n\nClone your repository fork, ignoring all the git-lfs files\n\n\n$ git lfs clone git@github.com:<your_username>/models.git '-I /'\n\n\n\n\n\n\nCreate a new folder \n<mynewmodel>\n containing all the model files in the repostiory root\n\n\nput all the non-code files (serialized models, test data) into a \n*files/\n directory, where \n*\n can be anything. These will namely be tracked by \ngit-lfs\n instead of \ngit\n.\n\n\nExamples: \nmodel_files/\n, \ndataloader_files/\n\n\n\n\n\n\nTest your repository locally:\n\n\n$ kipoi test <mynewmodel_folder>\n\n\n\n\n\n\nCommit, push to your forked remote and submit a pull request to \ngithub.com/kipoi/models\n\n\n\n\nDataloader can use some trained transformer (here the \nLabelBinarizer\n and \nStandardScaler\n transformers form sklearn). These should be written to \ndataloader_files/\n.\n\n\ncd ../examples/iris_model_template\n\n\n\n\n/data/nasif12/home_if12/avsec/projects-work/kipoi/examples/iris_model_template\n\n\n\nos.makedirs(\"dataloader_files\", exist_ok=True)\n\n\n\n\nls\n\n\n\n\n\u001b[0m\u001b[38;5;27mdataloader_files\u001b[0m/  dataloader.yaml  \u001b[38;5;27mmodel_files\u001b[0m/  \u001b[38;5;27m__pycache__\u001b[0m/\ndataloader.py      \u001b[38;5;27mexample_files\u001b[0m/   model.yaml\n\n\n\nimport pickle\n\n\n\n\nwith open(\"dataloader_files/y_transformer.pkl\", \"wb\") as f:\n    pickle.dump(y_transformer, f)\n\nwith open(\"dataloader_files/x_transformer.pkl\", \"wb\") as f:\n    pickle.dump(x_transformer, f)\n\n\n\n\nls dataloader_files\n\n\n\n\nx_transformer.pkl  y_transformer.pkl\n\n\n\n3. Generate \nmodel_files/\n\n\nThe serialized model weights and architecture go to \nmodel_files/\n.\n\n\nos.makedirs(\"model_files\", exist_ok=True)\n\n\n\n\n# Architecture\nwith open(\"model_files/model.json\", \"w\") as f:\n    f.write(model.to_json())\n\n\n\n\n# Weights\nmodel.save_weights(\"model_files/weights.h5\")\n\n\n\n\n# Alternatively, for the scikit-learn model we would save the pickle file\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nlr = OneVsRestClassifier(LogisticRegression())\nlr.fit(x, y)\n\nwith open(\"model_files/sklearn_model.pkl\", \"wb\") as f:\n    pickle.dump(lr, f)\n\n\n\n\n4. Generate \nexample_files/\n\n\nexample_files/\n should contain a small subset of the raw files the dataloader will read.\n\n\nNumpy arrays -> pd.DataFrame\n\n\niris.keys()\n\n\n\n\ndict_keys(['target_names', 'feature_names', 'data', 'DESCR', 'target'])\n\n\n\nX = pd.DataFrame(iris[\"data\"][:20], columns=iris[\"feature_names\"])\n\n\n\n\nX.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nsepal length (cm)\n\n      \nsepal width (cm)\n\n      \npetal length (cm)\n\n      \npetal width (cm)\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n5.1\n\n      \n3.5\n\n      \n1.4\n\n      \n0.2\n\n    \n\n    \n\n      \n1\n\n      \n4.9\n\n      \n3.0\n\n      \n1.4\n\n      \n0.2\n\n    \n\n    \n\n      \n2\n\n      \n4.7\n\n      \n3.2\n\n      \n1.3\n\n      \n0.2\n\n    \n\n    \n\n      \n3\n\n      \n4.6\n\n      \n3.1\n\n      \n1.5\n\n      \n0.2\n\n    \n\n    \n\n      \n4\n\n      \n5.0\n\n      \n3.6\n\n      \n1.4\n\n      \n0.2\n\n    \n\n  \n\n\n\n\n\n\n\ny = pd.DataFrame({\"class\": iris[\"target\"][:20]})\n\n\n\n\ny.head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nclass\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n0\n\n    \n\n    \n\n      \n1\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \n0\n\n    \n\n    \n\n      \n3\n\n      \n0\n\n    \n\n    \n\n      \n4\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\nSave example files\n\n\nos.makedirs(\"example_files\", exist_ok=True)\n\n\n\n\nX.to_csv(\"example_files/features.csv\", index=False)\n\n\n\n\ny.to_csv(\"example_files/targets.csv\", index=False)\n\n\n\n\n!head -n 2 example_files/targets.csv\n\n\n\n\nclass\n0\n\n\n\n!head -n 2 example_files/features.csv\n\n\n\n\nsepal length (cm),sepal width (cm),petal length (cm),petal width (cm)\n5.1,3.5,1.4,0.2\n\n\n\n5. Write \nmodel.yaml\n\n\nThe \nmodel.yaml\n for this model should look like this:\n\n\ntype: keras  # use `kipoi.model.KerasModel`\nargs:  # arguments of `kipoi.model.KerasModel`\n    arch: model_files/model.json\n    weights: model_files/weights.h5\ndefault_dataloader: . # path to the dataloader directory. Here it's defined in the same directory\ninfo: # General information about the model\n    authors: \n        - name: Your Name\n          github: your_github_username\n          email: your_email@host.org\n    doc: Model predicting the Iris species\n    version: 0.1  # optional \n    cite_as: https://doi.org:/... # preferably a doi url to the paper\n    trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description\n    license: MIT # Software License - defaults to MIT\ndependencies:\n    conda: # install via conda\n      - python=3.5\n      - h5py\n      # - soumith::pytorch  # specify packages from other channels via <channel>::<package>      \n    pip:   # install via pip\n      - keras>=2.0.4\n      - tensorflow>=1.0\nschema:  # Model schema\n    inputs:\n        features:\n            shape: (4,)  # array shape of a single sample (omitting the batch dimension)\n            doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\"\n    targets:\n        shape: (3,)\n        doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\"\n\n\n\n\nAll file paths are relative relative to \nmodel.yaml\n.\n\n\n6. Write \ndataloader.yaml\n\n\ntype: Dataset\ndefined_as: dataloader.py::MyDataset  # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py\nargs:\n    features_file:\n        # descr: > allows multi-line fields\n        doc: >\n          Csv file of the Iris Plants Database from\n          http://archive.ics.uci.edu/ml/datasets/Iris features.\n        type: str\n        example: example_files/features.csv  # example files\n    targets_file:\n        doc: >\n          Csv file of the Iris Plants Database targets.\n          Not required for making the prediction.\n        type: str\n        example: example_files/targets.csv\n        optional: True  # if not present, the `targets` field will not be present in the dataloader output\ninfo:\n    authors: \n        - name: Your Name\n          github: your_github_account\n          email: your_email@host.org\n    version: 0.1\n    doc: Model predicting the Iris species\ndependencies:\n    conda:\n      - python=3.5\n      - pandas\n      - numpy\n      - sklearn\noutput_schema:\n    inputs:\n        features:\n            shape: (4,)\n            doc: Features in cm: sepal length, sepal width, petal length, petal width.\n    targets:\n        shape: (3, )\n        doc: One-hot encoded array of classes: setosa, versicolor, virginica.\n    metadata:  # field providing additional information to the samples (not directly required by the model)\n        example_row_number:\n            shape: int\n            doc: Just an example metadata column\n\n\n\n\n7. Write \ndataloader.py\n\n\nFinally, let's implement MyDataset. We need to implement two methods: \n__len__\n and \n__getitem__\n. \n\n\n__getitem__\n will return one item of the dataset. In our case, this is a dictionary with \noutput_schema\n described in \ndataloader.yaml\n.\n\n\nFor more information about writing such dataloaders, see the \nData Loading and Processing Tutorial from pytorch\n.\n\n\nimport pickle\nfrom kipoi.data import Dataset\nimport pandas as pd\nimport numpy as np\n\ndef read_pickle(f):\n    with open(f, \"rb\") as f:\n        return pickle.load(f)\n\nclass MyDataset(Dataset):\n\n    def __init__(self, features_file, targets_file=None):\n        self.features_file = features_file\n        self.targets_file = targets_file\n\n        self.y_transformer = read_pickle(\"dataloader_files/y_transformer.pkl\")\n        self.x_transformer = read_pickle(\"dataloader_files/x_transformer.pkl\")\n\n        self.features = pd.read_csv(features_file)\n        if targets_file is not None:\n            self.targets = pd.read_csv(targets_file)\n            assert len(self.targets) == len(self.features)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis]))\n        if self.targets_file is None:\n            y_class = {}\n        else:\n            y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis]))\n        return {\n            \"inputs\": {\n                \"features\": x_features\n            },\n            \"targets\": y_class,\n            \"metadata\": {\n                \"example_row_number\": idx\n            }\n        }\n\n\n\n\nExample usage of the dataset\n\n\nds = MyDataset(\"example_files/features.csv\", \"example_files/targets.csv\")\n\n\n\n\n# call __getitem__\nds[5]\n\n\n\n\n{'inputs': {'features': array([-0.5372,  1.9577, -1.1707, -1.05  ])},\n 'metadata': {'example_row_number': 5},\n 'targets': array([1, 0, 0])}\n\n\n\nSince MyDatset inherits from \nkipoi.data.Dataset\n, it has some additional nice feature. See \npython-sdk.ipynb\n for more information.\n\n\n# batch-iterator\nit = ds.batch_iter(batch_size=3, shuffle=False, num_workers=2)\nnext(it)\n\n\n\n\n{'inputs': {'features': array([[-0.9007,  1.0321, -1.3413, -1.313 ],\n         [-1.143 , -0.125 , -1.3413, -1.313 ],\n         [-1.3854,  0.3378, -1.3981, -1.313 ]])},\n 'metadata': {'example_row_number': array([0, 1, 2])},\n 'targets': array([[1, 0, 0],\n        [1, 0, 0],\n        [1, 0, 0]])}\n\n\n\n# ds.load_all()  # load the whole dataset into memory\n\n\n\n\n8. Test with the model with \n$ kipoi test .\n\n\nBefore we contribute the model to the repository, let's run the test:\n\n\n!kipoi test .\n\n\n\n\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from dataloader.py::MyDataset\u001b[0m\nUsing TensorFlow backend.\n2017-11-29 17:26:21.755321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-11-29 17:26:21.755368: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-11-29 17:26:21.755385: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-11-29 17:26:21.755399: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-11-29 17:26:21.755414: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model architecture from <_io.TextIOWrapper name='model_files/model.json' mode='r' encoding='UTF-8'>\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model weights from model_files/weights.h5\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Initialized data generator. Running batches...\u001b[0m\n/opt/modules/i12g/anaconda/3-4.1.1/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n/opt/modules/i12g/anaconda/3-4.1.1/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Returned data schema correct\u001b[0m\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 89.45it/s]\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m predict_example done!\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Successfully ran test_predict\u001b[0m\n\u001b[0m\n\n\n\nThis command did the following:\n\n\n\n\nvalidated if \noutput_schema\n defined in \ndataloader.yaml\n matches the shapes of the returned arrays\n\n\nvalidated that model and dataloader are compatible in \ninputs\n and \ntargets\n\n\nexecuted the model pipeline for the example \n\n\n\n\nAccessing the model through kipoi\n\n\nimport kipoi\n\n\n\n\nreload(kipoi)\n\n\n\n\n<module 'kipoi' from '/data/nasif12/home_if12/avsec/projects-work/kipoi/kipoi/__init__.py'>\n\n\n\nm = kipoi.get_model(\".\", source=\"dir\")  # See also python-sdk.ipynb\n\n\n\n\nUsing TensorFlow backend.\n\n\n\nm.pipeline.predict({\"features_file\": \"example_files/features.csv\", \"targets_file\": \"example_files/targets.csv\" })[:5]\n\n\n\n\narray([[ 1.5356, -0.8118, -0.2712],\n       [ 0.4649, -0.22  , -1.1491],\n       [ 0.6735, -0.1923, -0.8083],\n       [ 0.3958,  0.0178, -0.9159],\n       [ 1.6362, -0.79  , -0.0849]], dtype=float32)\n\n\n\nm.info\n\n\n\n\nInfo(authors=[Author(name='Your Name', github='your_github_username', email=None)], doc='Model predicting the Iris species', name=None, version='0.1', tags=[])\n\n\n\nm.default_dataloader\n\n\n\n\ndataloader.MyDataset\n\n\n\nm.model\n\n\n\n\n<keras.engine.training.Model at 0x7f0dbe68f8d0>\n\n\n\nm.predict_on_batch\n\n\n\n\n<bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f0dc19cf400>>\n\n\n\nRecap\n\n\nCongrats! You made it through the tutorial! Feel free to use this model for your model template. Alternatively, you can use \nkipoi init\n to setup a model directory. Make sure you have read the \ngetting started guide\n for contributing models.",
            "title": "Contributing models"
        },
        {
            "location": "/tutorials/contributing_models/#contributing-a-model-to-the-kipoi-model-repository",
            "text": "This notebook will show you how to contribute a model to the  Kipoi model repository . For a simple 'model contribution checklist' see also  http://kipoi.org/docs/contributing/01_Getting_started/ .",
            "title": "Contributing a model to the Kipoi model repository"
        },
        {
            "location": "/tutorials/contributing_models/#kipoi-basics",
            "text": "Contributing a model to Kipoi means writing a sub-folder with all the required files to the  Kipoi model repository  via pull request.  Two main components of the model repository are  model  and  dataloader .",
            "title": "Kipoi basics"
        },
        {
            "location": "/tutorials/contributing_models/#model",
            "text": "Model takes as input numpy arrays and outputs numpy arrays. In practice, a model needs to implement the  predict_on_batch(x)  method, where  x  is dictionary/list of numpy arrays. The model contributor needs to provide one of the following:   Serialized Keras model  Serialized Sklearn model  Custom model inheriting from  keras.model.BaseModel .  all the required files, i.e. weights need to be loaded in the  __init__   See  http://kipoi.org/docs/contributing/02_Writing_model.yaml/  and  http://kipoi.org/docs/contributing/05_Writing_model.py/  for more info.",
            "title": "Model"
        },
        {
            "location": "/tutorials/contributing_models/#dataloader",
            "text": "Dataloader takes raw file paths or other parameters as argument and outputs modelling-ready numpy arrays. The dataloading can be done through a generator---batch-by-batch, sample-by-sample---or by just returning the whole dataset. The goal is to work really with raw files (say fasta, bed, vcf, etc in bioinformatics), as this allows to make model predictions on new datasets without going through the burden of running custom pre-processing scripts. The model contributor needs to implement one of the following:   PreloadedDataset  Dataset  BatchDataset  SampleIterator  BatchIterator  SampleGenerator  BatchGenerator   See  http://kipoi.org/docs/contributing/04_Writing_dataloader.py/  for more info.",
            "title": "Dataloader"
        },
        {
            "location": "/tutorials/contributing_models/#folder-layout",
            "text": "Here is an example folder structure of a Kipoi model:  \u251c\u2500\u2500 dataloader.py     # implements the dataloader\n\u251c\u2500\u2500 dataloader.yaml   # describes the dataloader\n\u251c\u2500\u2500 dataloader_files/      #/ files required by the dataloader\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 x_transfomer.pkl\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 y_transfomer.pkl\n\u251c\u2500\u2500 model.yaml        # describes the model\n\u251c\u2500\u2500 model_files/           #/ files required by the model\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 model.json\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 weights.h5\n\u2514\u2500\u2500 example_files/         #/ small example files\n    \u251c\u2500\u2500 features.csv\n    \u2514\u2500\u2500 targets.csv  Two most important files are  model.yaml  and  dataloader.yaml . They provide a complete description about the model, the dataloader and the files they depend on.",
            "title": "Folder layout"
        },
        {
            "location": "/tutorials/contributing_models/#contributing-a-simple-iris-classifier",
            "text": "Details about the individual files will be revealed throught the tutorial below. A simple Keras model will be trained to predict the Iris plant class from the well-known  Iris  dataset.",
            "title": "Contributing a simple Iris-classifier"
        },
        {
            "location": "/tutorials/contributing_models/#outline",
            "text": "Train the model  Generate  dataloader_files/  Generate  model_files/  Generate  example_files/  Write  model.yaml  Write  dataloader.yaml  Write  dataloader.py  Test with the model with  $ kipoi test .",
            "title": "Outline"
        },
        {
            "location": "/tutorials/contributing_models/#1-train-the-model",
            "text": "",
            "title": "1. Train the model"
        },
        {
            "location": "/tutorials/contributing_models/#load-and-pre-process-the-data",
            "text": "import pandas as pd\nimport os\nfrom sklearn.preprocessing import LabelBinarizer, StandardScaler\n\nfrom sklearn import datasets\niris = datasets.load_iris()  # view more info about the dataset\n# print(iris[\"DESCR\"])  # Data pre-processing\ny_transformer = LabelBinarizer().fit(iris[\"target\"])\nx_transformer = StandardScaler().fit(iris[\"data\"])  x = x_transformer.transform(iris[\"data\"])\ny = y_transformer.transform(iris[\"target\"])  x[:3]  array([[-0.9007,  1.0321, -1.3413, -1.313 ],\n       [-1.143 , -0.125 , -1.3413, -1.313 ],\n       [-1.3854,  0.3378, -1.3981, -1.313 ]])  y[:3]  array([[1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0]])",
            "title": "Load and pre-process the data"
        },
        {
            "location": "/tutorials/contributing_models/#train-an-example-model",
            "text": "Let's train a simple linear-regression model using Keras.  from keras.models import Model\nimport keras.layers as kl\n\ninp = kl.Input(shape=(4, ), name=\"features\")\nout = kl.Dense(units=3)(inp)\nmodel = Model(inp, out)\nmodel.compile(\"adam\", \"categorical_crossentropy\")\n\nmodel.fit(x, y, verbose=0)  Using TensorFlow backend.\n\n\n\n\n\n<keras.callbacks.History at 0x7f5c537f2d68>",
            "title": "Train an example model"
        },
        {
            "location": "/tutorials/contributing_models/#2-generate-dataloader_files",
            "text": "Now that we have everything we need, let's start writing the files to model's directory (here  model_template/ ).   In reality, you would need to    Fork the  kipoi/models repository  Clone your repository fork, ignoring all the git-lfs files  $ git lfs clone git@github.com:<your_username>/models.git '-I /'    Create a new folder  <mynewmodel>  containing all the model files in the repostiory root  put all the non-code files (serialized models, test data) into a  *files/  directory, where  *  can be anything. These will namely be tracked by  git-lfs  instead of  git .  Examples:  model_files/ ,  dataloader_files/    Test your repository locally:  $ kipoi test <mynewmodel_folder>    Commit, push to your forked remote and submit a pull request to  github.com/kipoi/models   Dataloader can use some trained transformer (here the  LabelBinarizer  and  StandardScaler  transformers form sklearn). These should be written to  dataloader_files/ .  cd ../examples/iris_model_template  /data/nasif12/home_if12/avsec/projects-work/kipoi/examples/iris_model_template  os.makedirs(\"dataloader_files\", exist_ok=True)  ls  \u001b[0m\u001b[38;5;27mdataloader_files\u001b[0m/  dataloader.yaml  \u001b[38;5;27mmodel_files\u001b[0m/  \u001b[38;5;27m__pycache__\u001b[0m/\ndataloader.py      \u001b[38;5;27mexample_files\u001b[0m/   model.yaml  import pickle  with open(\"dataloader_files/y_transformer.pkl\", \"wb\") as f:\n    pickle.dump(y_transformer, f)\n\nwith open(\"dataloader_files/x_transformer.pkl\", \"wb\") as f:\n    pickle.dump(x_transformer, f)  ls dataloader_files  x_transformer.pkl  y_transformer.pkl",
            "title": "2. Generate dataloader_files/"
        },
        {
            "location": "/tutorials/contributing_models/#3-generate-model_files",
            "text": "The serialized model weights and architecture go to  model_files/ .  os.makedirs(\"model_files\", exist_ok=True)  # Architecture\nwith open(\"model_files/model.json\", \"w\") as f:\n    f.write(model.to_json())  # Weights\nmodel.save_weights(\"model_files/weights.h5\")  # Alternatively, for the scikit-learn model we would save the pickle file\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nlr = OneVsRestClassifier(LogisticRegression())\nlr.fit(x, y)\n\nwith open(\"model_files/sklearn_model.pkl\", \"wb\") as f:\n    pickle.dump(lr, f)",
            "title": "3. Generate model_files/"
        },
        {
            "location": "/tutorials/contributing_models/#4-generate-example_files",
            "text": "example_files/  should contain a small subset of the raw files the dataloader will read.",
            "title": "4. Generate example_files/"
        },
        {
            "location": "/tutorials/contributing_models/#numpy-arrays-pddataframe",
            "text": "iris.keys()  dict_keys(['target_names', 'feature_names', 'data', 'DESCR', 'target'])  X = pd.DataFrame(iris[\"data\"][:20], columns=iris[\"feature_names\"])  X.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       sepal length (cm) \n       sepal width (cm) \n       petal length (cm) \n       petal width (cm) \n     \n   \n   \n     \n       0 \n       5.1 \n       3.5 \n       1.4 \n       0.2 \n     \n     \n       1 \n       4.9 \n       3.0 \n       1.4 \n       0.2 \n     \n     \n       2 \n       4.7 \n       3.2 \n       1.3 \n       0.2 \n     \n     \n       3 \n       4.6 \n       3.1 \n       1.5 \n       0.2 \n     \n     \n       4 \n       5.0 \n       3.6 \n       1.4 \n       0.2 \n     \n      y = pd.DataFrame({\"class\": iris[\"target\"][:20]})  y.head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       class \n     \n   \n   \n     \n       0 \n       0 \n     \n     \n       1 \n       0 \n     \n     \n       2 \n       0 \n     \n     \n       3 \n       0 \n     \n     \n       4 \n       0",
            "title": "Numpy arrays -&gt; pd.DataFrame"
        },
        {
            "location": "/tutorials/contributing_models/#save-example-files",
            "text": "os.makedirs(\"example_files\", exist_ok=True)  X.to_csv(\"example_files/features.csv\", index=False)  y.to_csv(\"example_files/targets.csv\", index=False)  !head -n 2 example_files/targets.csv  class\n0  !head -n 2 example_files/features.csv  sepal length (cm),sepal width (cm),petal length (cm),petal width (cm)\n5.1,3.5,1.4,0.2",
            "title": "Save example files"
        },
        {
            "location": "/tutorials/contributing_models/#5-write-modelyaml",
            "text": "The  model.yaml  for this model should look like this:  type: keras  # use `kipoi.model.KerasModel`\nargs:  # arguments of `kipoi.model.KerasModel`\n    arch: model_files/model.json\n    weights: model_files/weights.h5\ndefault_dataloader: . # path to the dataloader directory. Here it's defined in the same directory\ninfo: # General information about the model\n    authors: \n        - name: Your Name\n          github: your_github_username\n          email: your_email@host.org\n    doc: Model predicting the Iris species\n    version: 0.1  # optional \n    cite_as: https://doi.org:/... # preferably a doi url to the paper\n    trained_on: Iris species dataset (http://archive.ics.uci.edu/ml/datasets/Iris) # short dataset description\n    license: MIT # Software License - defaults to MIT\ndependencies:\n    conda: # install via conda\n      - python=3.5\n      - h5py\n      # - soumith::pytorch  # specify packages from other channels via <channel>::<package>      \n    pip:   # install via pip\n      - keras>=2.0.4\n      - tensorflow>=1.0\nschema:  # Model schema\n    inputs:\n        features:\n            shape: (4,)  # array shape of a single sample (omitting the batch dimension)\n            doc: \"Features in cm: sepal length, sepal width, petal length, petal width.\"\n    targets:\n        shape: (3,)\n        doc: \"One-hot encoded array of classes: setosa, versicolor, virginica.\"  All file paths are relative relative to  model.yaml .",
            "title": "5. Write model.yaml"
        },
        {
            "location": "/tutorials/contributing_models/#6-write-dataloaderyaml",
            "text": "type: Dataset\ndefined_as: dataloader.py::MyDataset  # We need to implement MyDataset class inheriting from kipoi.data.Dataset in dataloader.py\nargs:\n    features_file:\n        # descr: > allows multi-line fields\n        doc: >\n          Csv file of the Iris Plants Database from\n          http://archive.ics.uci.edu/ml/datasets/Iris features.\n        type: str\n        example: example_files/features.csv  # example files\n    targets_file:\n        doc: >\n          Csv file of the Iris Plants Database targets.\n          Not required for making the prediction.\n        type: str\n        example: example_files/targets.csv\n        optional: True  # if not present, the `targets` field will not be present in the dataloader output\ninfo:\n    authors: \n        - name: Your Name\n          github: your_github_account\n          email: your_email@host.org\n    version: 0.1\n    doc: Model predicting the Iris species\ndependencies:\n    conda:\n      - python=3.5\n      - pandas\n      - numpy\n      - sklearn\noutput_schema:\n    inputs:\n        features:\n            shape: (4,)\n            doc: Features in cm: sepal length, sepal width, petal length, petal width.\n    targets:\n        shape: (3, )\n        doc: One-hot encoded array of classes: setosa, versicolor, virginica.\n    metadata:  # field providing additional information to the samples (not directly required by the model)\n        example_row_number:\n            shape: int\n            doc: Just an example metadata column",
            "title": "6. Write dataloader.yaml"
        },
        {
            "location": "/tutorials/contributing_models/#7-write-dataloaderpy",
            "text": "Finally, let's implement MyDataset. We need to implement two methods:  __len__  and  __getitem__ .   __getitem__  will return one item of the dataset. In our case, this is a dictionary with  output_schema  described in  dataloader.yaml .  For more information about writing such dataloaders, see the  Data Loading and Processing Tutorial from pytorch .  import pickle\nfrom kipoi.data import Dataset\nimport pandas as pd\nimport numpy as np\n\ndef read_pickle(f):\n    with open(f, \"rb\") as f:\n        return pickle.load(f)\n\nclass MyDataset(Dataset):\n\n    def __init__(self, features_file, targets_file=None):\n        self.features_file = features_file\n        self.targets_file = targets_file\n\n        self.y_transformer = read_pickle(\"dataloader_files/y_transformer.pkl\")\n        self.x_transformer = read_pickle(\"dataloader_files/x_transformer.pkl\")\n\n        self.features = pd.read_csv(features_file)\n        if targets_file is not None:\n            self.targets = pd.read_csv(targets_file)\n            assert len(self.targets) == len(self.features)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        x_features = np.ravel(self.x_transformer.transform(self.features.iloc[idx].values[np.newaxis]))\n        if self.targets_file is None:\n            y_class = {}\n        else:\n            y_class = np.ravel(self.y_transformer.transform(self.targets.iloc[idx].values[np.newaxis]))\n        return {\n            \"inputs\": {\n                \"features\": x_features\n            },\n            \"targets\": y_class,\n            \"metadata\": {\n                \"example_row_number\": idx\n            }\n        }",
            "title": "7. Write dataloader.py"
        },
        {
            "location": "/tutorials/contributing_models/#example-usage-of-the-dataset",
            "text": "ds = MyDataset(\"example_files/features.csv\", \"example_files/targets.csv\")  # call __getitem__\nds[5]  {'inputs': {'features': array([-0.5372,  1.9577, -1.1707, -1.05  ])},\n 'metadata': {'example_row_number': 5},\n 'targets': array([1, 0, 0])}  Since MyDatset inherits from  kipoi.data.Dataset , it has some additional nice feature. See  python-sdk.ipynb  for more information.  # batch-iterator\nit = ds.batch_iter(batch_size=3, shuffle=False, num_workers=2)\nnext(it)  {'inputs': {'features': array([[-0.9007,  1.0321, -1.3413, -1.313 ],\n         [-1.143 , -0.125 , -1.3413, -1.313 ],\n         [-1.3854,  0.3378, -1.3981, -1.313 ]])},\n 'metadata': {'example_row_number': array([0, 1, 2])},\n 'targets': array([[1, 0, 0],\n        [1, 0, 0],\n        [1, 0, 0]])}  # ds.load_all()  # load the whole dataset into memory",
            "title": "Example usage of the dataset"
        },
        {
            "location": "/tutorials/contributing_models/#8-test-with-the-model-with-kipoi-test",
            "text": "Before we contribute the model to the repository, let's run the test:  !kipoi test .  \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from dataloader.py::MyDataset\u001b[0m\nUsing TensorFlow backend.\n2017-11-29 17:26:21.755321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-11-29 17:26:21.755368: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-11-29 17:26:21.755385: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-11-29 17:26:21.755399: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-11-29 17:26:21.755414: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model architecture from <_io.TextIOWrapper name='model_files/model.json' mode='r' encoding='UTF-8'>\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.model]\u001b[0m successfully loaded model weights from model_files/weights.h5\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Initialized data generator. Running batches...\u001b[0m\n/opt/modules/i12g/anaconda/3-4.1.1/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n/opt/modules/i12g/anaconda/3-4.1.1/lib/python3.5/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Returned data schema correct\u001b[0m\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 89.45it/s]\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m predict_example done!\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m Successfully ran test_predict\u001b[0m\n\u001b[0m  This command did the following:   validated if  output_schema  defined in  dataloader.yaml  matches the shapes of the returned arrays  validated that model and dataloader are compatible in  inputs  and  targets  executed the model pipeline for the example",
            "title": "8. Test with the model with $ kipoi test ."
        },
        {
            "location": "/tutorials/contributing_models/#accessing-the-model-through-kipoi",
            "text": "import kipoi  reload(kipoi)  <module 'kipoi' from '/data/nasif12/home_if12/avsec/projects-work/kipoi/kipoi/__init__.py'>  m = kipoi.get_model(\".\", source=\"dir\")  # See also python-sdk.ipynb  Using TensorFlow backend.  m.pipeline.predict({\"features_file\": \"example_files/features.csv\", \"targets_file\": \"example_files/targets.csv\" })[:5]  array([[ 1.5356, -0.8118, -0.2712],\n       [ 0.4649, -0.22  , -1.1491],\n       [ 0.6735, -0.1923, -0.8083],\n       [ 0.3958,  0.0178, -0.9159],\n       [ 1.6362, -0.79  , -0.0849]], dtype=float32)  m.info  Info(authors=[Author(name='Your Name', github='your_github_username', email=None)], doc='Model predicting the Iris species', name=None, version='0.1', tags=[])  m.default_dataloader  dataloader.MyDataset  m.model  <keras.engine.training.Model at 0x7f0dbe68f8d0>  m.predict_on_batch  <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f0dc19cf400>>",
            "title": "Accessing the model through kipoi"
        },
        {
            "location": "/tutorials/contributing_models/#recap",
            "text": "Congrats! You made it through the tutorial! Feel free to use this model for your model template. Alternatively, you can use  kipoi init  to setup a model directory. Make sure you have read the  getting started guide  for contributing models.",
            "title": "Recap"
        },
        {
            "location": "/tutorials/python-api/",
            "text": "Generated from \nnbs/python-api.ipynb\n\n\nKipoi python API\n\n\nQuick start\n\n\nThere are three basic building blocks in kipoi:\n\n\n\n\nSource\n - provides Models and DataLoaders.\n\n\nModel\n - makes the prediction given the numpy arrays. \n\n\nDataloader\n - loads the data from raw files and transforms them into a form that is directly consumable by the Model\n\n\n\n\n\n\nList of main commands\n\n\nGet/list sources\n- \nkipoi.list_sources()\n\n- \nkipoi.get_source()\n\n\nList models/dataloaders\n- \nkipoi.list_models()\n\n- \nkipoi.list_dataloaders()\n\n\nGet model/dataloader\n- \nkipoi.get_model()\n\n- \nkipoi.get_dataloader_factory()\n\n\nLoad only model/dataloader description from the yaml file without loading the model\n\n\n\n\nkipoi.get_model_descr()\n  \n\n\nkipoi.get_dataloader_descr()\n\n\n\n\nInstall the dependencies\n- \nkipoi.install_model_dependencies()\n\n- \nkipoi.install_dataloader_dependencies()\n\n\nimport kipoi\n\n\n\n\nSource\n\n\nAvailable sources are specified in the config file located at: \n~/.kipoi/config.yaml\n. Here is an example config file:\n\n\nmodel_sources:\n    kipoi: # default\n        type: git-lfs # git repository with large file storage (git-lfs)\n        remote_url: git@github.com:kipoi/models.git # git remote\n        local_path: ~/.kipoi/models/ # local storage path\n    gl:\n        type: git-lfs  # custom model\n        remote_url: https://i12g-gagneurweb.informatik.tu-muenchen.de/gitlab/gagneurlab/model-zoo.git\n        local_path: /s/project/model-zoo\n\n\n\n\nThere are three different model sources possible: \n\n\n\n\ngit-lfs\n - git repository with source files tracked normally by git and all the binary files like model weights (located in \nfiles*\n directories) are tracked by \ngit-lfs\n. \n\n\nRequires \ngit-lfs\n to be installed.\n\n\ngit\n - all the files including weights (not recommended)\n\n\nlocal\n - local directory containing models defined in subdirectories\n\n\n\n\nFor \ngit-lfs\n source type, larger files tracked by \ngit-lfs\n will be downloaded into the specified directory \nlocal_path\n only after the model has been requested (when invoking \nkipoi.get_model()\n).\n\n\nNote\n\n\nA particular model/dataloader is defined by its source (say \nkipoi\n or \nmy_git_models\n) and the relative path of the desired model directory from the model source root (say \nrbp/\n).\n\n\nA directory is considered a model if it contains a \nmodel.yaml\n file.\n\n\nimport kipoi\n\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport logging\nlogging.disable(1000)\n\n\n\n\nkipoi.list_sources()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nsource\n\n      \ntype\n\n      \nlocation\n\n      \nlocal_size\n\n      \nn_models\n\n      \nn_dataloaders\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nkipoi\n\n      \ngit-lfs\n\n      \n/home/avsec/.kipoi/mo...\n\n      \n1,2G\n\n      \n780\n\n      \n780\n\n    \n\n  \n\n\n\n\n\n\n\ns = kipoi.get_source(\"kipoi\")\n\n\n\n\ns\n\n\n\n\nGitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/')\n\n\n\nkipoi.list_models().head()\n\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nsource\n\n      \nmodel\n\n      \nversion\n\n      \nauthors\n\n      \ncontributors\n\n      \ndoc\n\n      \ntype\n\n      \ninputs\n\n      \ntargets\n\n      \npostproc_score_variants\n\n      \nlicense\n\n      \ncite_as\n\n      \ntrained_on\n\n      \ntraining_procedure\n\n      \ntags\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nkipoi\n\n      \nDeepSEAKeras\n\n      \n0.1\n\n      \n[Author(name='Jian Zh...\n\n      \n[Author(name='Lara Ur...\n\n      \nThis CNN is based on ...\n\n      \nkeras\n\n      \nseq\n\n      \nTFBS_DHS_probs\n\n      \nTrue\n\n      \nMIT\n\n      \nhttps://doi.org/10.10...\n\n      \nENCODE and Roadmap Ep...\n\n      \nhttps://www.nature.co...\n\n      \n[Histone modification...\n\n    \n\n    \n\n      \n1\n\n      \nkipoi\n\n      \nextended_coda\n\n      \n0.1\n\n      \n[Author(name='Pang We...\n\n      \n[Author(name='Johnny ...\n\n      \nSingle bp resolution ...\n\n      \nkeras\n\n      \n[H3K27AC_subsampled]\n\n      \n[H3K27ac]\n\n      \nFalse\n\n      \nMIT\n\n      \nhttps://doi.org/10.10...\n\n      \nDescribed in https://...\n\n      \nDescribed in https://...\n\n      \n[Histone modification]\n\n    \n\n    \n\n      \n2\n\n      \nkipoi\n\n      \nDeepCpG_DNA/Hou2016_m...\n\n      \n1.0.4\n\n      \n[Author(name='Christo...\n\n      \n[Author(name='Roman K...\n\n      \nThis is the extractio...\n\n      \nkeras\n\n      \n[dna]\n\n      \n[cpg/mESC1, cpg/mESC2...\n\n      \nTrue\n\n      \nMIT\n\n      \nhttps://doi.org/10.11...\n\n      \nscBS-seq and scRRBS-s...\n\n      \nDescribed in https://...\n\n      \n[DNA methylation]\n\n    \n\n    \n\n      \n3\n\n      \nkipoi\n\n      \nDeepCpG_DNA/Smallwood...\n\n      \n1.0.4\n\n      \n[Author(name='Christo...\n\n      \n[Author(name='Roman K...\n\n      \nThis is the extractio...\n\n      \nkeras\n\n      \n[dna]\n\n      \n[cpg/BS24_1_2I, cpg/B...\n\n      \nTrue\n\n      \nMIT\n\n      \nhttps://doi.org/10.11...\n\n      \nscBS-seq and scRRBS-s...\n\n      \nDescribed in https://...\n\n      \n[DNA methylation]\n\n    \n\n    \n\n      \n4\n\n      \nkipoi\n\n      \nDeepCpG_DNA/Hou2016_H...\n\n      \n1.0.4\n\n      \n[Author(name='Christo...\n\n      \n[Author(name='Roman K...\n\n      \nThis is the extractio...\n\n      \nkeras\n\n      \n[dna]\n\n      \n[cpg/HepG21, cpg/HepG...\n\n      \nTrue\n\n      \nMIT\n\n      \nhttps://doi.org/10.11...\n\n      \nscBS-seq and scRRBS-s...\n\n      \nDescribed in https://...\n\n      \n[DNA methylation]\n\n    \n\n  \n\n\n\n\n\n\n\nModel\n\n\nLet's choose to use the \nrbp_eclip/UPF1\n model from kipoi\n\n\nMODEL = \"rbp_eclip/UPF1\"\n\n\n\n\nNOTE:\n If you are using python2, use a different model like \nMaxEntScan/3prime\n to following this example.\n\n\n# Note. Install all the dependencies for that model:\n# add --gpu flag to install gpu-compatible dependencies (e.g. installs tensorflow-gpu instead of tensorflow)\n!kipoi env install {MODEL}\n\n\n\n\nmodel = kipoi.get_model(MODEL)\n\n\n\n\nAvailable fields:\n\n\nModel\n\n\n\n\ntype\n\n\nargs\n\n\ninfo\n\n\nauthors\n\n\nname\n\n\nversion\n\n\ntags\n\n\ndoc\n\n\nschema\n\n\ninputs\n\n\ntargets\n\n\n\n\ndefault_dataloader - loaded dataloader class\n\n\n\n\n\n\npredict_on_batch()\n\n\n\n\nsource\n\n\nsource_dir\n\n\npipeline\n\n\npredict()\n\n\npredict_example()\n\n\npredict_generator()\n\n\n\n\nDataloader\n\n\n\n\ntype\n\n\ndefined_as\n\n\nargs\n\n\ninfo (same as for the model)\n\n\noutput_schema\n\n\ninputs\n\n\ntargets\n\n\n\n\nmetadata\n\n\n\n\n\n\nsource\n\n\n\n\nsource_dir\n\n\nexample_kwargs\n\n\ninit_example()\n\n\nbatch_iter()\n\n\nbatch_train_iter()\n\n\nbatch_predict_iter()\n\n\nload_all()\n\n\n\n\nmodel\n\n\n\n\n<kipoi.model.KerasModel at 0x7f95b27af2b0>\n\n\n\nmodel.type\n\n\n\n\n'keras'\n\n\n\nInfo\n\n\nmodel.info\n\n\n\n\nModelInfo(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='\\'RBP binding model from Avsec et al: \"Modeling positional effects of regulatory sequences with spline transformations increases prediction accuracy of deep neural networks\". \\'\n\n\n\n', name=None, version='0.1', license='MIT', tags=['RNA binding'], contributors=[Author(name='Ziga Avsec', github='avsecz', email=None)], cite_as='https://doi.org/10.1093/bioinformatics/btx727', trained_on='RBP occupancy peaks measured by eCLIP-seq (Van Nostrand et al., 2016 - https://doi.org/10.1038/nmeth.3810), https://github.com/gagneurlab/Manuscript_Avsec_Bioinformatics_2017\n', training_procedure='Single task training with ADAM')\n\n\nmodel.info.version\n\n\n\n\n'0.1'\n\n\n\nSchema\n\n\ndict(model.schema.inputs)\n\n\n\n\n{'dist_exon_intron': ArraySchema(shape=(1, 10), doc='Distance the nearest exon_intron (splice donor) site transformed with B-splines', name='dist_exon_intron', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_gene_end': ArraySchema(shape=(1, 10), doc='Distance the nearest gene end transformed with B-splines', name='dist_gene_end', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_gene_start': ArraySchema(shape=(1, 10), doc='Distance the nearest gene start transformed with B-splines', name='dist_gene_start', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_intron_exon': ArraySchema(shape=(1, 10), doc='Distance the nearest intron_exon (splice acceptor) site transformed with B-splines', name='dist_intron_exon', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_polya': ArraySchema(shape=(1, 10), doc='Distance the nearest Poly-A site transformed with B-splines', name='dist_polya', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_start_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest start codon transformed with B-splines', name='dist_start_codon', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_stop_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest stop codon transformed with B-splines', name='dist_stop_codon', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_tss': ArraySchema(shape=(1, 10), doc='Distance the nearest TSS site transformed with B-splines', name='dist_tss', special_type=None, associated_metadata=[], column_labels=None),\n 'seq': ArraySchema(shape=(101, 4), doc='One-hot encoded RNA sequence', name='seq', special_type=<ArraySpecialType.DNASeq: 'DNASeq'>, associated_metadata=[], column_labels=None)}\n\n\n\nmodel.schema.targets\n\n\n\n\nArraySchema(shape=(1,), doc='Predicted binding strength', name=None, special_type=None, associated_metadata=[], column_labels=None)\n\n\n\nDefault dataloader\n\n\nModel already has the default dataloder present. To use it, specify\n\n\nmodel.source_dir\n\n\n\n\n'/home/avsec/.kipoi/models/rbp_eclip/UPF1'\n\n\n\nmodel.default_dataloader\n\n\n\n\ndataloader.SeqDistDataset\n\n\n\nmodel.default_dataloader.info\n\n\n\n\nInfo(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='RBP binding model taking as input 101nt long sequence as well as 8 distances to nearest genomic landmarks -  tss, poly-A, exon-intron boundary, intron-exon boundary, start codon, stop codon, gene start, gene end\n\n\n\n', name=None, version='0.1', license='MIT', tags=[])\n\n\nPredict_on_batch\n\n\nmodel.predict_on_batch\n\n\n\n\n<bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f95b27af2b0>>\n\n\n\nOthers\n\n\n# Model source\nmodel.source\n\n\n\n\nGitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/')\n\n\n\n# model location directory\nmodel.source_dir\n\n\n\n\n'/home/avsec/.kipoi/models/rbp_eclip/UPF1'\n\n\n\nDataLoader\n\n\nDataLoader = kipoi.get_dataloader_factory(MODEL)\n# same as DataLoader = model.default_dataloader\n\n\n\n\nA dataloader will most likely require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. There are several options where the dataloader input keyword arguments are displayed:\n\n\n# Display information about the dataloader\nprint(DataLoader.__doc__)\n\n\n\n\n    Args:\n        intervals_file: file path; tsv file\n            Assumes bed-like `chrom start end id score strand` format.\n        fasta_file: file path; Genome sequence\n        gtf_file: file path; Genome annotation GTF file.\n        filter_protein_coding: Considering genomic landmarks only for protein coding genes\n        preproc_transformer: file path; tranformer used for pre-processing.\n        target_file: file path; path to the targets\n        batch_size: int\n\n\n\n# Alternatively the dataloader keyword arguments can be displayed using the function:\nkipoi.print_dl_kwargs(DataLoader)\n\n\n\n\nKeyword argument: `intervals_file`\n    doc: bed6 file with `chrom start end id score strand` columns\n    type: str\n    optional: False\n    example: example_files/intervals.bed\nKeyword argument: `fasta_file`\n    doc: Reference genome sequence\n    type: str\n    optional: False\n    example: example_files/hg38_chr22.fa\nKeyword argument: `gtf_file`\n    doc: file path; Genome annotation GTF file\n    type: str\n    optional: False\n    example: example_files/gencode.v24.annotation_chr22.gtf\nKeyword argument: `filter_protein_coding`\n    doc: Considering genomic landmarks only for protein coding genes when computing the distances to the nearest genomic landmark.\n    type: str\n    optional: True\n    example: True\nKeyword argument: `target_file`\n    doc: path to the targets (txt) file\n    type: str\n    optional: True\n    example: example_files/targets.tsv\nKeyword argument: `use_linecache`\n    doc: if True, use linecache https://docs.python.org/3/library/linecache.html to access bed file rows\n    type: str\n    optional: True\n--------------------------------------------------------------------------------\nExample keyword arguments are: {'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'}\n\n\n\nRun dataloader on some examples\n\n\n# each dataloader already provides example files which can be used to illustrate its use:\nDataLoader.example_kwargs\n\n\n\n\n{'fasta_file': 'example_files/hg38_chr22.fa',\n 'filter_protein_coding': True,\n 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf',\n 'intervals_file': 'example_files/intervals.bed',\n 'target_file': 'example_files/targets.tsv'}\n\n\n\nimport os\n\n\n\n\n# cd into the source directory \nos.chdir(DataLoader.source_dir)\n\n\n\n\n!tree\n\n\n\n\n.\n\u251c\u2500\u2500 custom_keras_objects.py -> ../template/custom_keras_objects.py\n\u251c\u2500\u2500 dataloader_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 position_transformer.pkl\n\u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py\n\u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml\n\u251c\u2500\u2500 example_files -> ../template/example_files\n\u251c\u2500\u2500 model_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 model.h5\n\u251c\u2500\u2500 model.yaml -> ../template/model.yaml\n\u2514\u2500\u2500 __pycache__\n    \u251c\u2500\u2500 custom_keras_objects.cpython-36.pyc\n    \u2514\u2500\u2500 dataloader.cpython-36.pyc\n\n4 directories, 8 files\n\n\n\ndl = DataLoader(**DataLoader.example_kwargs)\n# could be also done with DataLoader.init_example()\n\n\n\n\n# This particular dataloader is of type Dataset\n# i.e. it implements the __getitem__ method:\ndl[0].keys()\n\n\n\n\ndict_keys(['inputs', 'targets', 'metadata'])\n\n\n\ndl[0][\"inputs\"][\"seq\"][:5]\n\n\n\n\narray([[0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25]], dtype=float32)\n\n\n\ndl[0][\"inputs\"][\"seq\"][:5]\n\n\n\n\narray([[0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25]], dtype=float32)\n\n\n\nlen(dl)\n\n\n\n\n14\n\n\n\nGet the whole dataset\n\n\nwhole_data = dl.load_all()\n\n\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.24it/s]\n\n\n\nwhole_data.keys()\n\n\n\n\ndict_keys(['inputs', 'targets', 'metadata'])\n\n\n\nwhole_data[\"inputs\"][\"seq\"].shape\n\n\n\n\n(14, 101, 4)\n\n\n\nGet the iterator to run predictions\n\n\nit = dl.batch_iter(batch_size=1, shuffle=False, num_workers=0, drop_last=False)\n\n\n\n\nnext(it)[\"inputs\"][\"seq\"].shape\n\n\n\n\n(1, 101, 4)\n\n\n\nmodel.predict_on_batch(next(it)[\"inputs\"])\n\n\n\n\narray([[0.1351]], dtype=float32)\n\n\n\nPipeline\n\n\nPipeline object will take the dataloader arguments and run the whole pipeline directly:\n\n\ndataloader arguments --Dataloader-->  numpy arrays --Model--> prediction\n\n\n\n\nexample_kwargs = model.default_dataloader.example_kwargs\n\n\n\n\npreds = model.pipeline.predict_example()\npreds\n\n\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.78it/s]\n\n\n\n\n\narray([[0.4208],\n       [0.0005],\n       [0.0005],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.0005],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208]], dtype=float32)\n\n\n\nmodel.pipeline.predict(example_kwargs)\n\n\n\n\n1it [00:01,  1.56s/it]\n\n\n\n\n\narray([0.4208, 0.0005, 0.0005, 0.4208, 0.4208, 0.4208, 0.0005, 0.4208, 0.4208, 0.4208, 0.4208,\n       0.4208, 0.4208, 0.4208], dtype=float32)\n\n\n\nnext(model.pipeline.predict_generator(example_kwargs, batch_size=2))\n\n\n\n\narray([[0.4208],\n       [0.0005]], dtype=float32)\n\n\n\nfrom kipoi.data_utils import numpy_collate\nnumpy_collate_concat(list(model.pipeline.predict_generator(example_kwargs)))\n\n\n\n\narray([[0.4208],\n       [0.0005],\n       [0.0005],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.0005],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208]], dtype=float32)\n\n\n\nRe-train the Keras model\n\n\nKeras model is stored under the \n.model\n attribute.\n\n\nmodel.model.compile(\"adam\", \"binary_crossentropy\")\n\n\n\n\ntrain_it = dl.batch_train_iter(batch_size=2)\n\n\n\n\n# model.model.summary()\n\n\n\n\nmodel.model.fit_generator(train_it, steps_per_epoch=3, epochs=1)\n\n\n\n\nEpoch 1/1\n3/3 [==============================] - 1s 291ms/step - loss: 1.3592\n\n\n\n\n\n<keras.callbacks.History at 0x7f95b0095fd0>",
            "title": "Python API"
        },
        {
            "location": "/tutorials/python-api/#kipoi-python-api",
            "text": "",
            "title": "Kipoi python API"
        },
        {
            "location": "/tutorials/python-api/#quick-start",
            "text": "There are three basic building blocks in kipoi:   Source  - provides Models and DataLoaders.  Model  - makes the prediction given the numpy arrays.   Dataloader  - loads the data from raw files and transforms them into a form that is directly consumable by the Model",
            "title": "Quick start"
        },
        {
            "location": "/tutorials/python-api/#list-of-main-commands",
            "text": "Get/list sources\n-  kipoi.list_sources() \n-  kipoi.get_source()  List models/dataloaders\n-  kipoi.list_models() \n-  kipoi.list_dataloaders()  Get model/dataloader\n-  kipoi.get_model() \n-  kipoi.get_dataloader_factory()  Load only model/dataloader description from the yaml file without loading the model   kipoi.get_model_descr()     kipoi.get_dataloader_descr()   Install the dependencies\n-  kipoi.install_model_dependencies() \n-  kipoi.install_dataloader_dependencies()  import kipoi",
            "title": "List of main commands"
        },
        {
            "location": "/tutorials/python-api/#source",
            "text": "Available sources are specified in the config file located at:  ~/.kipoi/config.yaml . Here is an example config file:  model_sources:\n    kipoi: # default\n        type: git-lfs # git repository with large file storage (git-lfs)\n        remote_url: git@github.com:kipoi/models.git # git remote\n        local_path: ~/.kipoi/models/ # local storage path\n    gl:\n        type: git-lfs  # custom model\n        remote_url: https://i12g-gagneurweb.informatik.tu-muenchen.de/gitlab/gagneurlab/model-zoo.git\n        local_path: /s/project/model-zoo  There are three different model sources possible:    git-lfs  - git repository with source files tracked normally by git and all the binary files like model weights (located in  files*  directories) are tracked by  git-lfs .   Requires  git-lfs  to be installed.  git  - all the files including weights (not recommended)  local  - local directory containing models defined in subdirectories   For  git-lfs  source type, larger files tracked by  git-lfs  will be downloaded into the specified directory  local_path  only after the model has been requested (when invoking  kipoi.get_model() ).",
            "title": "Source"
        },
        {
            "location": "/tutorials/python-api/#note",
            "text": "A particular model/dataloader is defined by its source (say  kipoi  or  my_git_models ) and the relative path of the desired model directory from the model source root (say  rbp/ ).  A directory is considered a model if it contains a  model.yaml  file.  import kipoi  import warnings\nwarnings.filterwarnings('ignore')\n\nimport logging\nlogging.disable(1000)  kipoi.list_sources()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       source \n       type \n       location \n       local_size \n       n_models \n       n_dataloaders \n     \n   \n   \n     \n       0 \n       kipoi \n       git-lfs \n       /home/avsec/.kipoi/mo... \n       1,2G \n       780 \n       780 \n     \n      s = kipoi.get_source(\"kipoi\")  s  GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/')  kipoi.list_models().head()   \n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }  \n   \n     \n       \n       source \n       model \n       version \n       authors \n       contributors \n       doc \n       type \n       inputs \n       targets \n       postproc_score_variants \n       license \n       cite_as \n       trained_on \n       training_procedure \n       tags \n     \n   \n   \n     \n       0 \n       kipoi \n       DeepSEAKeras \n       0.1 \n       [Author(name='Jian Zh... \n       [Author(name='Lara Ur... \n       This CNN is based on ... \n       keras \n       seq \n       TFBS_DHS_probs \n       True \n       MIT \n       https://doi.org/10.10... \n       ENCODE and Roadmap Ep... \n       https://www.nature.co... \n       [Histone modification... \n     \n     \n       1 \n       kipoi \n       extended_coda \n       0.1 \n       [Author(name='Pang We... \n       [Author(name='Johnny ... \n       Single bp resolution ... \n       keras \n       [H3K27AC_subsampled] \n       [H3K27ac] \n       False \n       MIT \n       https://doi.org/10.10... \n       Described in https://... \n       Described in https://... \n       [Histone modification] \n     \n     \n       2 \n       kipoi \n       DeepCpG_DNA/Hou2016_m... \n       1.0.4 \n       [Author(name='Christo... \n       [Author(name='Roman K... \n       This is the extractio... \n       keras \n       [dna] \n       [cpg/mESC1, cpg/mESC2... \n       True \n       MIT \n       https://doi.org/10.11... \n       scBS-seq and scRRBS-s... \n       Described in https://... \n       [DNA methylation] \n     \n     \n       3 \n       kipoi \n       DeepCpG_DNA/Smallwood... \n       1.0.4 \n       [Author(name='Christo... \n       [Author(name='Roman K... \n       This is the extractio... \n       keras \n       [dna] \n       [cpg/BS24_1_2I, cpg/B... \n       True \n       MIT \n       https://doi.org/10.11... \n       scBS-seq and scRRBS-s... \n       Described in https://... \n       [DNA methylation] \n     \n     \n       4 \n       kipoi \n       DeepCpG_DNA/Hou2016_H... \n       1.0.4 \n       [Author(name='Christo... \n       [Author(name='Roman K... \n       This is the extractio... \n       keras \n       [dna] \n       [cpg/HepG21, cpg/HepG... \n       True \n       MIT \n       https://doi.org/10.11... \n       scBS-seq and scRRBS-s... \n       Described in https://... \n       [DNA methylation]",
            "title": "Note"
        },
        {
            "location": "/tutorials/python-api/#model",
            "text": "Let's choose to use the  rbp_eclip/UPF1  model from kipoi  MODEL = \"rbp_eclip/UPF1\"  NOTE:  If you are using python2, use a different model like  MaxEntScan/3prime  to following this example.  # Note. Install all the dependencies for that model:\n# add --gpu flag to install gpu-compatible dependencies (e.g. installs tensorflow-gpu instead of tensorflow)\n!kipoi env install {MODEL}  model = kipoi.get_model(MODEL)",
            "title": "Model"
        },
        {
            "location": "/tutorials/python-api/#available-fields",
            "text": "",
            "title": "Available fields:"
        },
        {
            "location": "/tutorials/python-api/#model_1",
            "text": "type  args  info  authors  name  version  tags  doc  schema  inputs  targets   default_dataloader - loaded dataloader class    predict_on_batch()   source  source_dir  pipeline  predict()  predict_example()  predict_generator()",
            "title": "Model"
        },
        {
            "location": "/tutorials/python-api/#dataloader",
            "text": "type  defined_as  args  info (same as for the model)  output_schema  inputs  targets   metadata    source   source_dir  example_kwargs  init_example()  batch_iter()  batch_train_iter()  batch_predict_iter()  load_all()   model  <kipoi.model.KerasModel at 0x7f95b27af2b0>  model.type  'keras'",
            "title": "Dataloader"
        },
        {
            "location": "/tutorials/python-api/#info",
            "text": "model.info  ModelInfo(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='\\'RBP binding model from Avsec et al: \"Modeling positional effects of regulatory sequences with spline transformations increases prediction accuracy of deep neural networks\". \\'  ', name=None, version='0.1', license='MIT', tags=['RNA binding'], contributors=[Author(name='Ziga Avsec', github='avsecz', email=None)], cite_as='https://doi.org/10.1093/bioinformatics/btx727', trained_on='RBP occupancy peaks measured by eCLIP-seq (Van Nostrand et al., 2016 - https://doi.org/10.1038/nmeth.3810), https://github.com/gagneurlab/Manuscript_Avsec_Bioinformatics_2017\n', training_procedure='Single task training with ADAM')  model.info.version  '0.1'",
            "title": "Info"
        },
        {
            "location": "/tutorials/python-api/#schema",
            "text": "dict(model.schema.inputs)  {'dist_exon_intron': ArraySchema(shape=(1, 10), doc='Distance the nearest exon_intron (splice donor) site transformed with B-splines', name='dist_exon_intron', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_gene_end': ArraySchema(shape=(1, 10), doc='Distance the nearest gene end transformed with B-splines', name='dist_gene_end', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_gene_start': ArraySchema(shape=(1, 10), doc='Distance the nearest gene start transformed with B-splines', name='dist_gene_start', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_intron_exon': ArraySchema(shape=(1, 10), doc='Distance the nearest intron_exon (splice acceptor) site transformed with B-splines', name='dist_intron_exon', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_polya': ArraySchema(shape=(1, 10), doc='Distance the nearest Poly-A site transformed with B-splines', name='dist_polya', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_start_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest start codon transformed with B-splines', name='dist_start_codon', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_stop_codon': ArraySchema(shape=(1, 10), doc='Distance the nearest stop codon transformed with B-splines', name='dist_stop_codon', special_type=None, associated_metadata=[], column_labels=None),\n 'dist_tss': ArraySchema(shape=(1, 10), doc='Distance the nearest TSS site transformed with B-splines', name='dist_tss', special_type=None, associated_metadata=[], column_labels=None),\n 'seq': ArraySchema(shape=(101, 4), doc='One-hot encoded RNA sequence', name='seq', special_type=<ArraySpecialType.DNASeq: 'DNASeq'>, associated_metadata=[], column_labels=None)}  model.schema.targets  ArraySchema(shape=(1,), doc='Predicted binding strength', name=None, special_type=None, associated_metadata=[], column_labels=None)",
            "title": "Schema"
        },
        {
            "location": "/tutorials/python-api/#default-dataloader",
            "text": "Model already has the default dataloder present. To use it, specify  model.source_dir  '/home/avsec/.kipoi/models/rbp_eclip/UPF1'  model.default_dataloader  dataloader.SeqDistDataset  model.default_dataloader.info  Info(authors=[Author(name='Ziga Avsec', github='avsecz', email=None)], doc='RBP binding model taking as input 101nt long sequence as well as 8 distances to nearest genomic landmarks -  tss, poly-A, exon-intron boundary, intron-exon boundary, start codon, stop codon, gene start, gene end  ', name=None, version='0.1', license='MIT', tags=[])",
            "title": "Default dataloader"
        },
        {
            "location": "/tutorials/python-api/#predict_on_batch",
            "text": "model.predict_on_batch  <bound method KerasModel.predict_on_batch of <kipoi.model.KerasModel object at 0x7f95b27af2b0>>",
            "title": "Predict_on_batch"
        },
        {
            "location": "/tutorials/python-api/#others",
            "text": "# Model source\nmodel.source  GitLFSSource(remote_url='git@github.com:kipoi/models.git', local_path='/home/avsec/.kipoi/models/')  # model location directory\nmodel.source_dir  '/home/avsec/.kipoi/models/rbp_eclip/UPF1'",
            "title": "Others"
        },
        {
            "location": "/tutorials/python-api/#dataloader_1",
            "text": "DataLoader = kipoi.get_dataloader_factory(MODEL)\n# same as DataLoader = model.default_dataloader  A dataloader will most likely require input arguments in which the input files are defined, for example input fasta files or bed files, based on which the model input is generated. There are several options where the dataloader input keyword arguments are displayed:  # Display information about the dataloader\nprint(DataLoader.__doc__)      Args:\n        intervals_file: file path; tsv file\n            Assumes bed-like `chrom start end id score strand` format.\n        fasta_file: file path; Genome sequence\n        gtf_file: file path; Genome annotation GTF file.\n        filter_protein_coding: Considering genomic landmarks only for protein coding genes\n        preproc_transformer: file path; tranformer used for pre-processing.\n        target_file: file path; path to the targets\n        batch_size: int  # Alternatively the dataloader keyword arguments can be displayed using the function:\nkipoi.print_dl_kwargs(DataLoader)  Keyword argument: `intervals_file`\n    doc: bed6 file with `chrom start end id score strand` columns\n    type: str\n    optional: False\n    example: example_files/intervals.bed\nKeyword argument: `fasta_file`\n    doc: Reference genome sequence\n    type: str\n    optional: False\n    example: example_files/hg38_chr22.fa\nKeyword argument: `gtf_file`\n    doc: file path; Genome annotation GTF file\n    type: str\n    optional: False\n    example: example_files/gencode.v24.annotation_chr22.gtf\nKeyword argument: `filter_protein_coding`\n    doc: Considering genomic landmarks only for protein coding genes when computing the distances to the nearest genomic landmark.\n    type: str\n    optional: True\n    example: True\nKeyword argument: `target_file`\n    doc: path to the targets (txt) file\n    type: str\n    optional: True\n    example: example_files/targets.tsv\nKeyword argument: `use_linecache`\n    doc: if True, use linecache https://docs.python.org/3/library/linecache.html to access bed file rows\n    type: str\n    optional: True\n--------------------------------------------------------------------------------\nExample keyword arguments are: {'intervals_file': 'example_files/intervals.bed', 'fasta_file': 'example_files/hg38_chr22.fa', 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf', 'filter_protein_coding': True, 'target_file': 'example_files/targets.tsv'}",
            "title": "DataLoader"
        },
        {
            "location": "/tutorials/python-api/#run-dataloader-on-some-examples",
            "text": "# each dataloader already provides example files which can be used to illustrate its use:\nDataLoader.example_kwargs  {'fasta_file': 'example_files/hg38_chr22.fa',\n 'filter_protein_coding': True,\n 'gtf_file': 'example_files/gencode.v24.annotation_chr22.gtf',\n 'intervals_file': 'example_files/intervals.bed',\n 'target_file': 'example_files/targets.tsv'}  import os  # cd into the source directory \nos.chdir(DataLoader.source_dir)  !tree  .\n\u251c\u2500\u2500 custom_keras_objects.py -> ../template/custom_keras_objects.py\n\u251c\u2500\u2500 dataloader_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 position_transformer.pkl\n\u251c\u2500\u2500 dataloader.py -> ../template/dataloader.py\n\u251c\u2500\u2500 dataloader.yaml -> ../template/dataloader.yaml\n\u251c\u2500\u2500 example_files -> ../template/example_files\n\u251c\u2500\u2500 model_files\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 model.h5\n\u251c\u2500\u2500 model.yaml -> ../template/model.yaml\n\u2514\u2500\u2500 __pycache__\n    \u251c\u2500\u2500 custom_keras_objects.cpython-36.pyc\n    \u2514\u2500\u2500 dataloader.cpython-36.pyc\n\n4 directories, 8 files  dl = DataLoader(**DataLoader.example_kwargs)\n# could be also done with DataLoader.init_example()  # This particular dataloader is of type Dataset\n# i.e. it implements the __getitem__ method:\ndl[0].keys()  dict_keys(['inputs', 'targets', 'metadata'])  dl[0][\"inputs\"][\"seq\"][:5]  array([[0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25]], dtype=float32)  dl[0][\"inputs\"][\"seq\"][:5]  array([[0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25]], dtype=float32)  len(dl)  14",
            "title": "Run dataloader on some examples"
        },
        {
            "location": "/tutorials/python-api/#get-the-whole-dataset",
            "text": "whole_data = dl.load_all()  100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.24it/s]  whole_data.keys()  dict_keys(['inputs', 'targets', 'metadata'])  whole_data[\"inputs\"][\"seq\"].shape  (14, 101, 4)",
            "title": "Get the whole dataset"
        },
        {
            "location": "/tutorials/python-api/#get-the-iterator-to-run-predictions",
            "text": "it = dl.batch_iter(batch_size=1, shuffle=False, num_workers=0, drop_last=False)  next(it)[\"inputs\"][\"seq\"].shape  (1, 101, 4)  model.predict_on_batch(next(it)[\"inputs\"])  array([[0.1351]], dtype=float32)",
            "title": "Get the iterator to run predictions"
        },
        {
            "location": "/tutorials/python-api/#pipeline",
            "text": "Pipeline object will take the dataloader arguments and run the whole pipeline directly:  dataloader arguments --Dataloader-->  numpy arrays --Model--> prediction  example_kwargs = model.default_dataloader.example_kwargs  preds = model.pipeline.predict_example()\npreds  100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.78it/s]\n\n\n\n\n\narray([[0.4208],\n       [0.0005],\n       [0.0005],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.0005],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208]], dtype=float32)  model.pipeline.predict(example_kwargs)  1it [00:01,  1.56s/it]\n\n\n\n\n\narray([0.4208, 0.0005, 0.0005, 0.4208, 0.4208, 0.4208, 0.0005, 0.4208, 0.4208, 0.4208, 0.4208,\n       0.4208, 0.4208, 0.4208], dtype=float32)  next(model.pipeline.predict_generator(example_kwargs, batch_size=2))  array([[0.4208],\n       [0.0005]], dtype=float32)  from kipoi.data_utils import numpy_collate\nnumpy_collate_concat(list(model.pipeline.predict_generator(example_kwargs)))  array([[0.4208],\n       [0.0005],\n       [0.0005],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.0005],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208],\n       [0.4208]], dtype=float32)",
            "title": "Pipeline"
        },
        {
            "location": "/tutorials/python-api/#re-train-the-keras-model",
            "text": "Keras model is stored under the  .model  attribute.  model.model.compile(\"adam\", \"binary_crossentropy\")  train_it = dl.batch_train_iter(batch_size=2)  # model.model.summary()  model.model.fit_generator(train_it, steps_per_epoch=3, epochs=1)  Epoch 1/1\n3/3 [==============================] - 1s 291ms/step - loss: 1.3592\n\n\n\n\n\n<keras.callbacks.History at 0x7f95b0095fd0>",
            "title": "Re-train the Keras model"
        },
        {
            "location": "/tutorials/R-api/",
            "text": "Generated from \nnbs/R-api.ipynb\n\n\nUsing Kipoi from R\n\n\nThanks to the \nreticulate\n R package from RStudio, it is possible to easily call python functions from R. Hence one can use kipoi python API from R. This tutorial will show how to do that.\n\n\nMake sure you have git-lfs and Kipoi correctly installed: \n\n\n\n\nInstall git-lfs\n\n\nconda install -c conda-forge git-lfs && git lfs install\n (alternatively see \nhttps://git-lfs.github.com/\n)\n\n\n\n\n\n\nInstall kipoi\n\n\npip install kipoi\n\n\n\n\n\n\n\n\nPlease read \ndocs/using/getting started\n before going through this notebook.\n\n\nInstall and load \nreticulate\n\n\nMake sure you have the reticulate R package installed\n\n\n# install.packages(\"reticulate\")\n\n\n\n\nlibrary(reticulate)\n\n\n\n\nReticulate quick intro\n\n\nIn general, using Kipoi from R is almost the same as using it from Python: instead of using \nobject.method()\n or \nobject.attribute\n as in python, use \n$\n: \nobject$method()\n, \nobject$attribute\n. \n\n\n# short reticulate example \nos <- import(\"os\")\nos$chdir(\"/tmp\")\nos$getcwd()\n\n\n\n\n'/tmp'\n\n\nType mapping R <-> python\n\n\nReticulate translates objects between R and python in the following way:\n\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nSingle-element vector\n\n\nScalar\n\n\n1\n, \n1L\n, \nTRUE\n, \n\"foo\"\n\n\n\n\n\n\nMulti-element vector\n\n\nList\n\n\nc(1.0, 2.0, 3.0)\n, \nc(1L, 2L, 3L)\n\n\n\n\n\n\nList of multiple types\n\n\nTuple\n\n\nlist(1L, TRUE, \"foo\")\n\n\n\n\n\n\nNamed list\n\n\nDict\n\n\nlist(a = 1L, b = 2.0)\n, \ndict(x = x_data)\n\n\n\n\n\n\nMatrix/Array\n\n\nNumPy ndarray\n\n\nmatrix(c(1,2,3,4), nrow = 2, ncol = 2)\n\n\n\n\n\n\nFunction\n\n\nPython function\n\n\nfunction(x) x + 1\n\n\n\n\n\n\nNULL, TRUE, FALSE\n\n\nNone, True, False\n\n\nNULL\n, \nTRUE\n, \nFALSE\n\n\n\n\n\n\n\n\nFor more info on reticulate, please visit https://github.com/rstudio/reticulate/.\n\n\nSetup the python environment\n\n\nWith \nreticulate::py_config()\n you can check if the python configuration used by reticulate is correct. You can can also choose to use a different conda environment with \nuse_condaenv(...)\n. This comes handy when using different models depending on different conda environments.\n\n\nreticulate::py_config()\n\n\n\n\npython:         /home/avsec/bin/anaconda3/bin/python\nlibpython:      /home/avsec/bin/anaconda3/lib/libpython3.6m.so\npythonhome:     /home/avsec/bin/anaconda3:/home/avsec/bin/anaconda3\nversion:        3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49)  [GCC 7.2.0]\nnumpy:          /home/avsec/bin/anaconda3/lib/python3.6/site-packages/numpy\nnumpy_version:  1.14.0\nos:             /home/avsec/bin/anaconda3/lib/python3.6/os.py\n\npython versions found: \n /home/avsec/bin/anaconda3/bin/python\n /usr/bin/python\n /usr/bin/python3\n\n\n\nList all conda environments:\n\n\nreticulate::conda_list()\n\n\n\n\nCreate a new conda environment for the model:\n\n\n$ kipoi env create HAL\n\n\n\n\nUse that environment in R:\n\n\nreticulate::use_condaenv(\"kipoi-HAL')\n\n\n\n\nLoad kipoi\n\n\nkipoi <- import(\"kipoi\")\n\n\n\n\nList models\n\n\nkipoi$list_models()$head()\n\n\n\n\n  source                             model version  \\\n0  kipoi                      DeepSEAKeras     0.1   \n1  kipoi                     extended_coda     0.1   \n2  kipoi      DeepCpG_DNA/Hou2016_mESC_dna   1.0.4   \n3  kipoi  DeepCpG_DNA/Smallwood2014_2i_dna   1.0.4   \n4  kipoi     DeepCpG_DNA/Hou2016_HepG2_dna   1.0.4\n\n                                             authors  \\\n0  [Author(name='Jian Zhou', github=None, email=N...   \n1  [Author(name='Pang Wei Koh', github='kohpangwe...   \n2  [Author(name='Christof Angermueller', github='...   \n3  [Author(name='Christof Angermueller', github='...   \n4  [Author(name='Christof Angermueller', github='...\n\n                                        contributors  \\\n0  [Author(name='Lara Urban', github='LaraUrban',...   \n1  [Author(name='Johnny Israeli', github='jisrael...   \n2  [Author(name='Roman Kreuzhuber', github='krrom...   \n3  [Author(name='Roman Kreuzhuber', github='krrom...   \n4  [Author(name='Roman Kreuzhuber', github='krrom...\n\n                                                 doc   type  \\\n0  This CNN is based on the DeepSEA model from Zh...  keras   \n1  Single bp resolution ChIP-seq denoising - http...  keras   \n2  This is the extraction of the DNA-part of the ...  keras   \n3  This is the extraction of the DNA-part of the ...  keras   \n4  This is the extraction of the DNA-part of the ...  keras\n\n                 inputs                                            targets  \\\n0                   seq                                     TFBS_DHS_probs   \n1  [H3K27AC_subsampled]                                          [H3K27ac]   \n2                 [dna]  [cpg/mESC1, cpg/mESC2, cpg/mESC3, cpg/mESC4, c...   \n3                 [dna]  [cpg/BS24_1_2I, cpg/BS24_2_2I, cpg/BS24_4_2I, ...   \n4                 [dna]  [cpg/HepG21, cpg/HepG22, cpg/HepG23, cpg/HepG2...\n\n   postproc_score_variants license  \\\n0                     True     MIT   \n1                    False     MIT   \n2                     True     MIT   \n3                     True     MIT   \n4                     True     MIT\n\n                                             cite_as  \\\n0                 https://doi.org/10.1038/nmeth.3547   \n1      https://doi.org/10.1093/bioinformatics/btx243   \n2  https://doi.org/10.1186/s13059-017-1189-z, htt...   \n3  https://doi.org/10.1186/s13059-017-1189-z, htt...   \n4  https://doi.org/10.1186/s13059-017-1189-z, htt...\n\n                                          trained_on  \\\n0  ENCODE and Roadmap Epigenomics chromatin profi...   \n1  Described in https://academic.oup.com/bioinfor...   \n2  scBS-seq and scRRBS-seq datasets, https://geno...   \n3  scBS-seq and scRRBS-seq datasets, https://geno...   \n4  scBS-seq and scRRBS-seq datasets, https://geno...\n\n                                  training_procedure  \\\n0  https://www.nature.com/articles/nmeth.3547#met...   \n1  Described in https://academic.oup.com/bioinfor...   \n2  Described in https://genomebiology.biomedcentr...   \n3  Described in https://genomebiology.biomedcentr...   \n4  Described in https://genomebiology.biomedcentr...\n\n                                                tags  \n0  [Histone modification, DNA binding, DNA access...  \n1                             [Histone modification]  \n2                                  [DNA methylation]  \n3                                  [DNA methylation]  \n4                                  [DNA methylation]\n\n\n\nreticulate\n currently doesn't support direct convertion from \npandas.DataFrame\n to R's \ndata.frame\n. Let's make a convenience function to create an R dataframe via matrix conversion.\n\n\n#' List models as an R data.frame\nkipoi_list_models <- function() {\n    df_models <- kipoi$list_models()\n    df <- data.frame(df_models$as_matrix())\n    colnames(df) = df_models$columns$tolist()\n    return(df)\n\n}\n\n\n\n\ndf <- kipoi_list_models()\n\n\n\n\nhead(df, 2)\n\n\n\n\n\n\nsource\nmodel\nversion\nauthors\ncontributors\ndoc\ntype\ninputs\ntargets\npostproc_score_variants\nlicense\ncite_as\ntrained_on\ntraining_procedure\ntags\n\n\n\n    \nkipoi                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \nDeepSEAKeras                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n0.1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n<environment: 0x556afc757e38>                                                                                                                                                                                                                                                                                                                                                                                                                                                \n<environment: 0x556afbb0d538>                                                                                                                                                                                                                                                                                                                                                                                                                                                \nThis CNN is based on the DeepSEA model from Zhou and Troyanskaya (2015). It categorically predicts 918 cell type-specific epigenetic features from DNA sequence. The model is trained on publicly available ENCODE and Roadmap Epigenomics data and on DNA sequences of size 1000bp. The input of the tensor has to be (N, 1000, 4) for N samples, 1000bp window size and 4 nucleotides. Per sample, 918 probabilities of showing a specific epigentic feature will be predicted.\n\nkeras                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \nseq                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \nTFBS_DHS_probs                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \nTRUE                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \nMIT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \nhttps://doi.org/10.1038/nmeth.3547                                                                                                                                                                                                                                                                                                                                                                                                                                                 \nENCODE and Roadmap Epigenomics chromatin profiles https://www.nature.com/articles/nmeth.3547#methods                                                                                                                                                                                                                                                                                                                                                                               \nhttps://www.nature.com/articles/nmeth.3547#methods                                                                                                                                                                                                                                                                                                                                                                                                                                 \n<environment: 0x556afcddfd50>                                                                                                                                                                                                                                                                                                                                                                                                                                                \n\n    \nkipoi                                                                                    \nextended_coda                                                                            \n0.1                                                                                      \n<environment: 0x556afc764260>                                                      \n<environment: 0x556afbaff708>                                                      \nSingle bp resolution ChIP-seq denoising - https://github.com/kundajelab/coda             \nkeras                                                                                    \nH3K27AC_subsampled                                                                       \nH3K27ac                                                                                  \nFALSE                                                                                    \nMIT                                                                                      \nhttps://doi.org/10.1093/bioinformatics/btx243                                            \nDescribed in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343\nDescribed in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343\n<environment: 0x556afcde7f60>                                                      \n\n\n\n\n\n\n\nGet the kipoi model and make a prediction for the example files\n\n\nTo run the following example, make sure you have all the dependencies installed. Run:\n\n\nkipoi$install_model_requirements(\"MaxEntScan/3prime\")\n\n\n\n\nfrom R or\n\n\nkipoi env install MaxEntScan/3prime\n\n\n\n\nfrom the command-line. This will install all the required dependencies for both, the model and the dataloader.\n\n\nkipoi$install_model_requirements(\"MaxEntScan/3prime\")\n\n\n\n\nmodel <- kipoi$get_model(\"MaxEntScan/3prime\")\n\n\n\n\npredictions <- model$pipeline$predict_example()\n\n\n\n\nhead(predictions)\n\n\n\n\n\n    \n6.72899227874919\n\n    \n6.15729433240656\n\n    \n7.14095214875511\n\n    \n2.13760519765451\n\n    \n-9.52033554891735\n\n    \n9.54342300799607\n\n\n\n\n\nUse the model and dataloader independently\n\n\n# Get the dataloader\nsetwd('~/.kipoi/models/MaxEntScan/3prime')\n\ndl <- model$default_dataloader(gtf_file='example_files/hg19.chr22.gtf', fasta_file='example_files/hg19.chr22.fa')\n\n\n\n\n# get a batch iterator\nit <- dl$batch_iter(batch_size=4)\n\n\n\n\nit\n\n\n\n\nDataLoaderIter\n\n\n\n# Retrieve a batch of data\nbatch <- iter_next(it)\n\n\n\n\nstr(batch)\n\n\n\n\nList of 2\n $ inputs  : chr [1:4(1d)] \"TCTTCTCTCCCCAATCTCAGCCT\" \"ATTCTCAGTTGTCTTTACAGTTT\" \"CCTTAGTTTTATTTTTTCAGAGT\" \"ATTTTTGTTTTTAGACATAGGAT\"\n $ metadata:List of 5\n  ..$ geneID      : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\"\n  ..$ transcriptID: chr [1:4(1d)] \"ENST00000424770\" \"ENST00000420638\" \"ENST00000420638\" \"ENST00000420638\"\n  ..$ biotype     : chr [1:4(1d)] \"lincRNA\" \"pseudogene\" \"pseudogene\" \"pseudogene\"\n  ..$ order       : num [1:4(1d)] 0 0 1 2\n  ..$ ranges      :List of 5\n  .. ..$ chr   : chr [1:4(1d)] \"22\" \"22\" \"22\" \"22\"\n  .. ..$ start : num [1:4(1d)] 16062790 16118910 16101471 16100645\n  .. ..$ end   : num [1:4(1d)] 16062813 16118933 16101494 16100668\n  .. ..$ id    : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\"\n  .. ..$ strand: chr [1:4(1d)] \"+\" \"-\" \"-\" \"-\"\n\n\n\n# make the prediction with a model\nmodel$predict_on_batch(batch$inputs)\n\n\n\n\n\n    \n6.72899227874919\n\n    \n6.15729433240656\n\n    \n7.14095214875511\n\n    \n2.13760519765451\n\n\n\n\n\nTroubleshooting\n\n\nSince Kipoi is not natively implemented in R, the error messages are cryptic and hence debugging can be a bit of a pain. \n\n\nRun the same code in python or CLI\n\n\nWhen you encounter an error, try to run the analogous code snippet from the command line or python. A good starting point is to first run\n\n\n$ kipoi test MaxEntScan/3prime --source=kipoi\n\n\n\n\nfrom the command-line first. \n\n\nDependency issues\n\n\nIt's very likely that the error will be due to missing dependencies. Also note that some models will work only with python 3 or python 2. To install all the required dependencies for the model, run:\n\n\n$ kipoi env install MaxEntScan/3prime\n\n\n\n\nThis will install the dependencies into your current conda environment. If you wish to create a new environment with all the dependencies installed, run\n\n\n$ kipoi env create MaxEntScan/3prime\n\n\n\n\nTo use that environment in R, run:\n\n\nuse_condaenv(\"kipoi-MaxEntScan__3prime\")\n\n\n\n\nMake sure you run that code snippet right after importing the \nreticulate\n library (i.e. make sure you run it before \nkipoi <- import('kipoi')\n)\n\n\nFloat/Double type issues\n\n\nWhen using a pytorch model: \nDeepSEA/predict\n\n\nkipoi$install_model_requirements(\"DeepSEA/predict\")\n\n\n\n\n# Get the dataloader\nsetwd('~/.kipoi/models/DeepSEA/predict')\nmodel <- kipoi$get_model(\"DeepSEA/predict\")\ndl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa')\n# get a batch iterator\nit <- dl$batch_iter(batch_size=4)\n# predict for a batch\nbatch <- iter_next(it)\n\n\n\n\n# model$predict_on_batch(batch$inputs)\n\n\n\n\nWe get an error:\n\n\nError in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Input type (CUDADoubleTensor) and weight type (CUDAFloatTensor) should be the same\n\n\n\n\nThis means that the feeded array is Double instead of Float.\n\n\nR arrays are by default converted to float64 numpy dtype:\n\n\nnp <- import(\"numpy\", convert=FALSE)\nnp$array(0.1)$dtype\n\n\n\n\nfloat64\n\n\n\nnp$array(batch$inputs)$dtype\n\n\n\n\nfloat64\n\n\n\nTo fix this, we need to explicitly convert them to \nfloat32\n before passing the batch to the model:\n\n\nmodel$predict_on_batch(np$array(batch$inputs, dtype='float32'))\n\n\n\n\n\n\n\n    \n0.003497796 \n0.003443634 \n0.00475722  \n0.006346597 \n0.01217456  \n0.008442441 \n0.005778539 \n0.007471715 \n0.005652952 \n0.009384833 \n\u22ef           \n0.0003717453\n0.001310135 \n0.01009644  \n0.008201431 \n0.0004381537\n0.007473897 \n0.009021533 \n0.003500142 \n0.003842842 \n0.0003947651\n\n    \n0.003497796 \n0.003443634 \n0.00475722  \n0.006346597 \n0.01217456  \n0.008442441 \n0.005778539 \n0.007471715 \n0.005652952 \n0.009384833 \n\u22ef           \n0.0003717453\n0.001310135 \n0.01009644  \n0.008201431 \n0.0004381537\n0.007473897 \n0.009021533 \n0.003500142 \n0.003842842 \n0.0003947651\n\n    \n0.003497796 \n0.003443634 \n0.00475722  \n0.006346597 \n0.01217456  \n0.008442441 \n0.005778539 \n0.007471715 \n0.005652952 \n0.009384833 \n\u22ef           \n0.0003717453\n0.001310135 \n0.01009644  \n0.008201431 \n0.0004381537\n0.007473897 \n0.009021533 \n0.003500142 \n0.003842842 \n0.0003947651\n\n    \n0.003497796 \n0.003443634 \n0.00475722  \n0.006346597 \n0.01217456  \n0.008442441 \n0.005778539 \n0.007471715 \n0.005652952 \n0.009384833 \n\u22ef           \n0.0003717453\n0.001310135 \n0.01009644  \n0.008201431 \n0.0004381537\n0.007473897 \n0.009021533 \n0.003500142 \n0.003842842 \n0.0003947651",
            "title": "R API"
        },
        {
            "location": "/tutorials/R-api/#using-kipoi-from-r",
            "text": "Thanks to the  reticulate  R package from RStudio, it is possible to easily call python functions from R. Hence one can use kipoi python API from R. This tutorial will show how to do that.  Make sure you have git-lfs and Kipoi correctly installed:    Install git-lfs  conda install -c conda-forge git-lfs && git lfs install  (alternatively see  https://git-lfs.github.com/ )    Install kipoi  pip install kipoi     Please read  docs/using/getting started  before going through this notebook.",
            "title": "Using Kipoi from R"
        },
        {
            "location": "/tutorials/R-api/#install-and-load-reticulate",
            "text": "Make sure you have the reticulate R package installed  # install.packages(\"reticulate\")  library(reticulate)",
            "title": "Install and load reticulate"
        },
        {
            "location": "/tutorials/R-api/#reticulate-quick-intro",
            "text": "In general, using Kipoi from R is almost the same as using it from Python: instead of using  object.method()  or  object.attribute  as in python, use  $ :  object$method() ,  object$attribute .   # short reticulate example \nos <- import(\"os\")\nos$chdir(\"/tmp\")\nos$getcwd()  '/tmp'",
            "title": "Reticulate quick intro"
        },
        {
            "location": "/tutorials/R-api/#type-mapping-r-python",
            "text": "Reticulate translates objects between R and python in the following way:     R  Python  Examples      Single-element vector  Scalar  1 ,  1L ,  TRUE ,  \"foo\"    Multi-element vector  List  c(1.0, 2.0, 3.0) ,  c(1L, 2L, 3L)    List of multiple types  Tuple  list(1L, TRUE, \"foo\")    Named list  Dict  list(a = 1L, b = 2.0) ,  dict(x = x_data)    Matrix/Array  NumPy ndarray  matrix(c(1,2,3,4), nrow = 2, ncol = 2)    Function  Python function  function(x) x + 1    NULL, TRUE, FALSE  None, True, False  NULL ,  TRUE ,  FALSE     For more info on reticulate, please visit https://github.com/rstudio/reticulate/.",
            "title": "Type mapping R &lt;-&gt; python"
        },
        {
            "location": "/tutorials/R-api/#setup-the-python-environment",
            "text": "With  reticulate::py_config()  you can check if the python configuration used by reticulate is correct. You can can also choose to use a different conda environment with  use_condaenv(...) . This comes handy when using different models depending on different conda environments.  reticulate::py_config()  python:         /home/avsec/bin/anaconda3/bin/python\nlibpython:      /home/avsec/bin/anaconda3/lib/libpython3.6m.so\npythonhome:     /home/avsec/bin/anaconda3:/home/avsec/bin/anaconda3\nversion:        3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49)  [GCC 7.2.0]\nnumpy:          /home/avsec/bin/anaconda3/lib/python3.6/site-packages/numpy\nnumpy_version:  1.14.0\nos:             /home/avsec/bin/anaconda3/lib/python3.6/os.py\n\npython versions found: \n /home/avsec/bin/anaconda3/bin/python\n /usr/bin/python\n /usr/bin/python3  List all conda environments:  reticulate::conda_list()  Create a new conda environment for the model:  $ kipoi env create HAL  Use that environment in R:  reticulate::use_condaenv(\"kipoi-HAL')",
            "title": "Setup the python environment"
        },
        {
            "location": "/tutorials/R-api/#load-kipoi",
            "text": "kipoi <- import(\"kipoi\")",
            "title": "Load kipoi"
        },
        {
            "location": "/tutorials/R-api/#list-models",
            "text": "kipoi$list_models()$head()    source                             model version  \\\n0  kipoi                      DeepSEAKeras     0.1   \n1  kipoi                     extended_coda     0.1   \n2  kipoi      DeepCpG_DNA/Hou2016_mESC_dna   1.0.4   \n3  kipoi  DeepCpG_DNA/Smallwood2014_2i_dna   1.0.4   \n4  kipoi     DeepCpG_DNA/Hou2016_HepG2_dna   1.0.4\n\n                                             authors  \\\n0  [Author(name='Jian Zhou', github=None, email=N...   \n1  [Author(name='Pang Wei Koh', github='kohpangwe...   \n2  [Author(name='Christof Angermueller', github='...   \n3  [Author(name='Christof Angermueller', github='...   \n4  [Author(name='Christof Angermueller', github='...\n\n                                        contributors  \\\n0  [Author(name='Lara Urban', github='LaraUrban',...   \n1  [Author(name='Johnny Israeli', github='jisrael...   \n2  [Author(name='Roman Kreuzhuber', github='krrom...   \n3  [Author(name='Roman Kreuzhuber', github='krrom...   \n4  [Author(name='Roman Kreuzhuber', github='krrom...\n\n                                                 doc   type  \\\n0  This CNN is based on the DeepSEA model from Zh...  keras   \n1  Single bp resolution ChIP-seq denoising - http...  keras   \n2  This is the extraction of the DNA-part of the ...  keras   \n3  This is the extraction of the DNA-part of the ...  keras   \n4  This is the extraction of the DNA-part of the ...  keras\n\n                 inputs                                            targets  \\\n0                   seq                                     TFBS_DHS_probs   \n1  [H3K27AC_subsampled]                                          [H3K27ac]   \n2                 [dna]  [cpg/mESC1, cpg/mESC2, cpg/mESC3, cpg/mESC4, c...   \n3                 [dna]  [cpg/BS24_1_2I, cpg/BS24_2_2I, cpg/BS24_4_2I, ...   \n4                 [dna]  [cpg/HepG21, cpg/HepG22, cpg/HepG23, cpg/HepG2...\n\n   postproc_score_variants license  \\\n0                     True     MIT   \n1                    False     MIT   \n2                     True     MIT   \n3                     True     MIT   \n4                     True     MIT\n\n                                             cite_as  \\\n0                 https://doi.org/10.1038/nmeth.3547   \n1      https://doi.org/10.1093/bioinformatics/btx243   \n2  https://doi.org/10.1186/s13059-017-1189-z, htt...   \n3  https://doi.org/10.1186/s13059-017-1189-z, htt...   \n4  https://doi.org/10.1186/s13059-017-1189-z, htt...\n\n                                          trained_on  \\\n0  ENCODE and Roadmap Epigenomics chromatin profi...   \n1  Described in https://academic.oup.com/bioinfor...   \n2  scBS-seq and scRRBS-seq datasets, https://geno...   \n3  scBS-seq and scRRBS-seq datasets, https://geno...   \n4  scBS-seq and scRRBS-seq datasets, https://geno...\n\n                                  training_procedure  \\\n0  https://www.nature.com/articles/nmeth.3547#met...   \n1  Described in https://academic.oup.com/bioinfor...   \n2  Described in https://genomebiology.biomedcentr...   \n3  Described in https://genomebiology.biomedcentr...   \n4  Described in https://genomebiology.biomedcentr...\n\n                                                tags  \n0  [Histone modification, DNA binding, DNA access...  \n1                             [Histone modification]  \n2                                  [DNA methylation]  \n3                                  [DNA methylation]  \n4                                  [DNA methylation]  reticulate  currently doesn't support direct convertion from  pandas.DataFrame  to R's  data.frame . Let's make a convenience function to create an R dataframe via matrix conversion.  #' List models as an R data.frame\nkipoi_list_models <- function() {\n    df_models <- kipoi$list_models()\n    df <- data.frame(df_models$as_matrix())\n    colnames(df) = df_models$columns$tolist()\n    return(df)\n\n}  df <- kipoi_list_models()  head(df, 2)   source model version authors contributors doc type inputs targets postproc_score_variants license cite_as trained_on training_procedure tags  \n     kipoi                                                                                                                                                                                                                                                                                                                                                                                                                                                                               DeepSEAKeras                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 <environment: 0x556afc757e38>                                                                                                                                                                                                                                                                                                                                                                                                                                                 <environment: 0x556afbb0d538>                                                                                                                                                                                                                                                                                                                                                                                                                                                 This CNN is based on the DeepSEA model from Zhou and Troyanskaya (2015). It categorically predicts 918 cell type-specific epigenetic features from DNA sequence. The model is trained on publicly available ENCODE and Roadmap Epigenomics data and on DNA sequences of size 1000bp. The input of the tensor has to be (N, 1000, 4) for N samples, 1000bp window size and 4 nucleotides. Per sample, 918 probabilities of showing a specific epigentic feature will be predicted. keras                                                                                                                                                                                                                                                                                                                                                                                                                                                                               seq                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 TFBS_DHS_probs                                                                                                                                                                                                                                                                                                                                                                                                                                                                      TRUE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                MIT                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 https://doi.org/10.1038/nmeth.3547                                                                                                                                                                                                                                                                                                                                                                                                                                                  ENCODE and Roadmap Epigenomics chromatin profiles https://www.nature.com/articles/nmeth.3547#methods                                                                                                                                                                                                                                                                                                                                                                                https://www.nature.com/articles/nmeth.3547#methods                                                                                                                                                                                                                                                                                                                                                                                                                                  <environment: 0x556afcddfd50>                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n     kipoi                                                                                     extended_coda                                                                             0.1                                                                                       <environment: 0x556afc764260>                                                       <environment: 0x556afbaff708>                                                       Single bp resolution ChIP-seq denoising - https://github.com/kundajelab/coda              keras                                                                                     H3K27AC_subsampled                                                                        H3K27ac                                                                                   FALSE                                                                                     MIT                                                                                       https://doi.org/10.1093/bioinformatics/btx243                                             Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 Described in https://academic.oup.com/bioinformatics/article/33/14/i225/3953958#100805343 <environment: 0x556afcde7f60>",
            "title": "List models"
        },
        {
            "location": "/tutorials/R-api/#get-the-kipoi-model-and-make-a-prediction-for-the-example-files",
            "text": "To run the following example, make sure you have all the dependencies installed. Run:  kipoi$install_model_requirements(\"MaxEntScan/3prime\")  from R or  kipoi env install MaxEntScan/3prime  from the command-line. This will install all the required dependencies for both, the model and the dataloader.  kipoi$install_model_requirements(\"MaxEntScan/3prime\")  model <- kipoi$get_model(\"MaxEntScan/3prime\")  predictions <- model$pipeline$predict_example()  head(predictions)  \n     6.72899227874919 \n     6.15729433240656 \n     7.14095214875511 \n     2.13760519765451 \n     -9.52033554891735 \n     9.54342300799607",
            "title": "Get the kipoi model and make a prediction for the example files"
        },
        {
            "location": "/tutorials/R-api/#use-the-model-and-dataloader-independently",
            "text": "# Get the dataloader\nsetwd('~/.kipoi/models/MaxEntScan/3prime')\n\ndl <- model$default_dataloader(gtf_file='example_files/hg19.chr22.gtf', fasta_file='example_files/hg19.chr22.fa')  # get a batch iterator\nit <- dl$batch_iter(batch_size=4)  it  DataLoaderIter  # Retrieve a batch of data\nbatch <- iter_next(it)  str(batch)  List of 2\n $ inputs  : chr [1:4(1d)] \"TCTTCTCTCCCCAATCTCAGCCT\" \"ATTCTCAGTTGTCTTTACAGTTT\" \"CCTTAGTTTTATTTTTTCAGAGT\" \"ATTTTTGTTTTTAGACATAGGAT\"\n $ metadata:List of 5\n  ..$ geneID      : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\"\n  ..$ transcriptID: chr [1:4(1d)] \"ENST00000424770\" \"ENST00000420638\" \"ENST00000420638\" \"ENST00000420638\"\n  ..$ biotype     : chr [1:4(1d)] \"lincRNA\" \"pseudogene\" \"pseudogene\" \"pseudogene\"\n  ..$ order       : num [1:4(1d)] 0 0 1 2\n  ..$ ranges      :List of 5\n  .. ..$ chr   : chr [1:4(1d)] \"22\" \"22\" \"22\" \"22\"\n  .. ..$ start : num [1:4(1d)] 16062790 16118910 16101471 16100645\n  .. ..$ end   : num [1:4(1d)] 16062813 16118933 16101494 16100668\n  .. ..$ id    : chr [1:4(1d)] \"ENSG00000233866\" \"ENSG00000223875\" \"ENSG00000223875\" \"ENSG00000223875\"\n  .. ..$ strand: chr [1:4(1d)] \"+\" \"-\" \"-\" \"-\"  # make the prediction with a model\nmodel$predict_on_batch(batch$inputs)  \n     6.72899227874919 \n     6.15729433240656 \n     7.14095214875511 \n     2.13760519765451",
            "title": "Use the model and dataloader independently"
        },
        {
            "location": "/tutorials/R-api/#troubleshooting",
            "text": "Since Kipoi is not natively implemented in R, the error messages are cryptic and hence debugging can be a bit of a pain.",
            "title": "Troubleshooting"
        },
        {
            "location": "/tutorials/R-api/#run-the-same-code-in-python-or-cli",
            "text": "When you encounter an error, try to run the analogous code snippet from the command line or python. A good starting point is to first run  $ kipoi test MaxEntScan/3prime --source=kipoi  from the command-line first.",
            "title": "Run the same code in python or CLI"
        },
        {
            "location": "/tutorials/R-api/#dependency-issues",
            "text": "It's very likely that the error will be due to missing dependencies. Also note that some models will work only with python 3 or python 2. To install all the required dependencies for the model, run:  $ kipoi env install MaxEntScan/3prime  This will install the dependencies into your current conda environment. If you wish to create a new environment with all the dependencies installed, run  $ kipoi env create MaxEntScan/3prime  To use that environment in R, run:  use_condaenv(\"kipoi-MaxEntScan__3prime\")  Make sure you run that code snippet right after importing the  reticulate  library (i.e. make sure you run it before  kipoi <- import('kipoi') )",
            "title": "Dependency issues"
        },
        {
            "location": "/tutorials/R-api/#floatdouble-type-issues",
            "text": "When using a pytorch model:  DeepSEA/predict  kipoi$install_model_requirements(\"DeepSEA/predict\")  # Get the dataloader\nsetwd('~/.kipoi/models/DeepSEA/predict')\nmodel <- kipoi$get_model(\"DeepSEA/predict\")\ndl <- model$default_dataloader(intervals_file='example_files/intervals.bed', fasta_file='example_files/hg38_chr22.fa')\n# get a batch iterator\nit <- dl$batch_iter(batch_size=4)\n# predict for a batch\nbatch <- iter_next(it)  # model$predict_on_batch(batch$inputs)  We get an error:  Error in py_call_impl(callable, dots$args, dots$keywords): RuntimeError: Input type (CUDADoubleTensor) and weight type (CUDAFloatTensor) should be the same  This means that the feeded array is Double instead of Float.  R arrays are by default converted to float64 numpy dtype:  np <- import(\"numpy\", convert=FALSE)\nnp$array(0.1)$dtype  float64  np$array(batch$inputs)$dtype  float64  To fix this, we need to explicitly convert them to  float32  before passing the batch to the model:  model$predict_on_batch(np$array(batch$inputs, dtype='float32'))   \n     0.003497796  0.003443634  0.00475722   0.006346597  0.01217456   0.008442441  0.005778539  0.007471715  0.005652952  0.009384833  \u22ef            0.0003717453 0.001310135  0.01009644   0.008201431  0.0004381537 0.007473897  0.009021533  0.003500142  0.003842842  0.0003947651 \n     0.003497796  0.003443634  0.00475722   0.006346597  0.01217456   0.008442441  0.005778539  0.007471715  0.005652952  0.009384833  \u22ef            0.0003717453 0.001310135  0.01009644   0.008201431  0.0004381537 0.007473897  0.009021533  0.003500142  0.003842842  0.0003947651 \n     0.003497796  0.003443634  0.00475722   0.006346597  0.01217456   0.008442441  0.005778539  0.007471715  0.005652952  0.009384833  \u22ef            0.0003717453 0.001310135  0.01009644   0.008201431  0.0004381537 0.007473897  0.009021533  0.003500142  0.003842842  0.0003947651 \n     0.003497796  0.003443634  0.00475722   0.006346597  0.01217456   0.008442441  0.005778539  0.007471715  0.005652952  0.009384833  \u22ef            0.0003717453 0.001310135  0.01009644   0.008201431  0.0004381537 0.007473897  0.009021533  0.003500142  0.003842842  0.0003947651",
            "title": "Float/Double type issues"
        },
        {
            "location": "/tutorials/tf_binding_models/",
            "text": "Generated from \nnbs/tf_binding_models.ipynb\n\n\nModel benchmarking with Kipoi\n\n\nThis tutorial will show to to easily benchmark tf-binding models in Kipoi. By providing a unified access to models, it takes the same effort to run a simple PWM scanning model then to run a more complicated model (DeepBind in this example).\n\n\nLoad software tools\n\n\nLet's start by loading software for this tutorial: the kipoi model zoo, \n\n\nimport kipoi\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\n\n\n\nPrepare data files\n\n\nNext, we introduce a labeled BED-format interval file and a genome fasta file\n\n\nintervals_file = 'example_data/chr22.101bp.2000_intervals.JUND.HepG2.tsv'\nfasta_file  = 'example_data/hg19_chr22.fa'\ndl_kwargs = {'intervals_file': intervals_file, 'fasta_file': fasta_file}\n\n\n\n\nLet's look at the first few lines in the intervals file\n\n\n!head $intervals_file\n\n\n\n\nchr22   20208963    20209064    0\nchr22   29673572    29673673    0\nchr22   28193720    28193821    0\nchr22   43864274    43864375    0\nchr22   18261550    18261651    0\nchr22   7869409 7869510 0\nchr22   49798024    49798125    0\nchr22   43088594    43088695    0\nchr22   35147671    35147772    0\nchr22   49486843    49486944    0\n\n\n\nThe four columns in this file contain chromosomes, interval start coordinate, interval end coordinate, and the label. This file contains 2000 examples, 1000 positives and 1000 negatives.\n\n\nLet's load the labels from the last column: \n\n\nlabels = np.loadtxt(intervals_file, usecols=(3,))\n\n\n\n\nNext, to evaluate the DeepBind model for JUND, we will 1) install software requirements to run the model, 2) load the model, and 3) get model predictions using our intervals and fasta file.\n\n\nInstall DeepBind model software requirements\n\n\ndeepbind_model_name = \"DeepBind/D00776.005\"\nkipoi.install_model_requirements(deepbind_model_name)\n# Use `$ kipoi env install DeepBind/D00776.005 --gpu` from the command-line to install the gpu version of the dependencies\n\n\n\n\nConda dependencies to be installed:\n['python=2.7', 'h5py']\nFetching package metadata ...........\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /users/jisraeli/local/anaconda/envs/kipoi:\n#\nh5py                      2.7.1            py27h2697762_0  \npip dependencies to be installed:\n['tensorflow==1.4', 'keras==2.1.4']\nRequirement already satisfied: tensorflow==1.4 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages\nCollecting keras==2.1.4\n  Using cached Keras-2.1.4-py2.py3-none-any.whl\nRequirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4)\nRequirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4)\nRequirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4)\nRequirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4)\nRequirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4)\nRequirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4)\nRequirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4)\nRequirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4)\nRequirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4)\nRequirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow==1.4)\nInstalling collected packages: keras\n  Found existing installation: Keras 2.0.4\n    Uninstalling Keras-2.0.4:\n      Successfully uninstalled Keras-2.0.4\nSuccessfully installed keras-2.1.4\n\n\n\nLoad DeepBind model\n\n\ndeepbind_model = kipoi.get_model(deepbind_model_name)\n\n\n\n\n/users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n\n\n\nGet DeepBind predictions\n\n\ndeepbind_predictions = deepbind_model.pipeline.predict(dl_kwargs, batch_size=1000)\n\n\n\n\nEvaluate DeepBind predictions\n\n\nLet's check the auROC of deepbind predictions:\n\n\nroc_auc_score(labels, deepbind_predictions)\n\n\n\n\n0.808138\n\n\n\nLoad, run, and evaluate a HOCOMOCO PWM model\n\n\npwm_model_name = \"pwm_HOCOMOCO/human/JUND\"\nkipoi.install_model_requirements(pwm_model_name)\n# Use `$ kipoi env install pwm_HOCOMOCO/human/JUND --gpu` from the command-line to install the gpu version of the dependencies\npwm_model = kipoi.get_model(pwm_model_name)\npwm_predictions = pwm_model.pipeline.predict(dl_kwargs, batch_size=1000)\nprint(\"PWM auROC:\")\nroc_auc_score(labels, pwm_predictions)\n\n\n\n\nConda dependencies to be installed:\n['python=3.5', 'h5py']\nFetching package metadata ...........\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /users/jisraeli/local/anaconda/envs/kipoi:\n#\nh5py                      2.7.1            py27h2697762_0  \npip dependencies to be installed:\n['tensorflow', 'keras==2.0.4']\nRequirement already satisfied: tensorflow in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages\nCollecting keras==2.0.4\nRequirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: theano in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/Theano-1.0.1-py2.7.egg (from keras==2.0.4)\nRequirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.0.4)\nRequirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)\nRequirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)\nRequirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\nRequirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\nRequirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\nRequirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\nRequirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\nRequirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow)\nRequirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from theano->keras==2.0.4)\nInstalling collected packages: keras\n  Found existing installation: Keras 2.1.4\n    Uninstalling Keras-2.1.4:\n      Successfully uninstalled Keras-2.1.4\nSuccessfully installed keras-2.0.4\n\n\n/users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n  custom_objects=custom_objects)\n\n\nPWM auROC:\n\n\n\n\n\n0.6431155\n\n\n\nIn this example, DeepBind's auROC of 80.8% outperforms the HOCOMOCO PWM auROC of 64.3%",
            "title": "Comparing models"
        },
        {
            "location": "/tutorials/tf_binding_models/#model-benchmarking-with-kipoi",
            "text": "This tutorial will show to to easily benchmark tf-binding models in Kipoi. By providing a unified access to models, it takes the same effort to run a simple PWM scanning model then to run a more complicated model (DeepBind in this example).",
            "title": "Model benchmarking with Kipoi"
        },
        {
            "location": "/tutorials/tf_binding_models/#load-software-tools",
            "text": "Let's start by loading software for this tutorial: the kipoi model zoo,   import kipoi\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score",
            "title": "Load software tools"
        },
        {
            "location": "/tutorials/tf_binding_models/#prepare-data-files",
            "text": "Next, we introduce a labeled BED-format interval file and a genome fasta file  intervals_file = 'example_data/chr22.101bp.2000_intervals.JUND.HepG2.tsv'\nfasta_file  = 'example_data/hg19_chr22.fa'\ndl_kwargs = {'intervals_file': intervals_file, 'fasta_file': fasta_file}  Let's look at the first few lines in the intervals file  !head $intervals_file  chr22   20208963    20209064    0\nchr22   29673572    29673673    0\nchr22   28193720    28193821    0\nchr22   43864274    43864375    0\nchr22   18261550    18261651    0\nchr22   7869409 7869510 0\nchr22   49798024    49798125    0\nchr22   43088594    43088695    0\nchr22   35147671    35147772    0\nchr22   49486843    49486944    0  The four columns in this file contain chromosomes, interval start coordinate, interval end coordinate, and the label. This file contains 2000 examples, 1000 positives and 1000 negatives.  Let's load the labels from the last column:   labels = np.loadtxt(intervals_file, usecols=(3,))  Next, to evaluate the DeepBind model for JUND, we will 1) install software requirements to run the model, 2) load the model, and 3) get model predictions using our intervals and fasta file.",
            "title": "Prepare data files"
        },
        {
            "location": "/tutorials/tf_binding_models/#install-deepbind-model-software-requirements",
            "text": "deepbind_model_name = \"DeepBind/D00776.005\"\nkipoi.install_model_requirements(deepbind_model_name)\n# Use `$ kipoi env install DeepBind/D00776.005 --gpu` from the command-line to install the gpu version of the dependencies  Conda dependencies to be installed:\n['python=2.7', 'h5py']\nFetching package metadata ...........\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /users/jisraeli/local/anaconda/envs/kipoi:\n#\nh5py                      2.7.1            py27h2697762_0  \npip dependencies to be installed:\n['tensorflow==1.4', 'keras==2.1.4']\nRequirement already satisfied: tensorflow==1.4 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages\nCollecting keras==2.1.4\n  Using cached Keras-2.1.4-py2.py3-none-any.whl\nRequirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow==1.4)\nRequirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4)\nRequirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.1.4)\nRequirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4)\nRequirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow==1.4)\nRequirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4)\nRequirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4)\nRequirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4)\nRequirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4)\nRequirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4)\nRequirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow==1.4)\nInstalling collected packages: keras\n  Found existing installation: Keras 2.0.4\n    Uninstalling Keras-2.0.4:\n      Successfully uninstalled Keras-2.0.4\nSuccessfully installed keras-2.1.4",
            "title": "Install DeepBind model software requirements"
        },
        {
            "location": "/tutorials/tf_binding_models/#load-deepbind-model",
            "text": "deepbind_model = kipoi.get_model(deepbind_model_name)  /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.",
            "title": "Load DeepBind model"
        },
        {
            "location": "/tutorials/tf_binding_models/#get-deepbind-predictions",
            "text": "deepbind_predictions = deepbind_model.pipeline.predict(dl_kwargs, batch_size=1000)",
            "title": "Get DeepBind predictions"
        },
        {
            "location": "/tutorials/tf_binding_models/#evaluate-deepbind-predictions",
            "text": "Let's check the auROC of deepbind predictions:  roc_auc_score(labels, deepbind_predictions)  0.808138",
            "title": "Evaluate DeepBind predictions"
        },
        {
            "location": "/tutorials/tf_binding_models/#load-run-and-evaluate-a-hocomoco-pwm-model",
            "text": "pwm_model_name = \"pwm_HOCOMOCO/human/JUND\"\nkipoi.install_model_requirements(pwm_model_name)\n# Use `$ kipoi env install pwm_HOCOMOCO/human/JUND --gpu` from the command-line to install the gpu version of the dependencies\npwm_model = kipoi.get_model(pwm_model_name)\npwm_predictions = pwm_model.pipeline.predict(dl_kwargs, batch_size=1000)\nprint(\"PWM auROC:\")\nroc_auc_score(labels, pwm_predictions)  Conda dependencies to be installed:\n['python=3.5', 'h5py']\nFetching package metadata ...........\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /users/jisraeli/local/anaconda/envs/kipoi:\n#\nh5py                      2.7.1            py27h2697762_0  \npip dependencies to be installed:\n['tensorflow', 'keras==2.0.4']\nRequirement already satisfied: tensorflow in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages\nCollecting keras==2.0.4\nRequirement already satisfied: enum34>=1.1.6 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: backports.weakref>=1.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: wheel in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: mock>=2.0.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: numpy>=1.12.1 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: protobuf>=3.3.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: six>=1.10.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow)\nRequirement already satisfied: theano in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/Theano-1.0.1-py2.7.egg (from keras==2.0.4)\nRequirement already satisfied: pyyaml in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from keras==2.0.4)\nRequirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)\nRequirement already satisfied: pbr>=0.11 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)\nRequirement already satisfied: bleach==1.5.0 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\nRequirement already satisfied: futures>=3.1.1; python_version < \"3.2\" in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\nRequirement already satisfied: markdown>=2.6.8 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\nRequirement already satisfied: werkzeug>=0.11.10 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\nRequirement already satisfied: html5lib==0.9999999 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow)\nRequirement already satisfied: setuptools in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from protobuf>=3.3.0->tensorflow)\nRequirement already satisfied: scipy>=0.14 in /users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages (from theano->keras==2.0.4)\nInstalling collected packages: keras\n  Found existing installation: Keras 2.1.4\n    Uninstalling Keras-2.1.4:\n      Successfully uninstalled Keras-2.1.4\nSuccessfully installed keras-2.0.4\n\n\n/users/jisraeli/local/anaconda/envs/kipoi/lib/python2.7/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n  custom_objects=custom_objects)\n\n\nPWM auROC:\n\n\n\n\n\n0.6431155  In this example, DeepBind's auROC of 80.8% outperforms the HOCOMOCO PWM auROC of 64.3%",
            "title": "Load, run, and evaluate a HOCOMOCO PWM model"
        },
        {
            "location": "/tutorials/composing_models/",
            "text": "Generated from \nnbs/composing_models.ipynb\n\n\nComposing models\n\n\nby Ziga Avsec\n\n\nComposing models means that we take the predictions of some model and use it as input for another model like this:\n\n\n\n\nThree different scenarios can occur when we want to compose models from Kipoi:\n\n\n\n\nall models are written in the same framework (say Keras)\n\n\nmodels are written in different frameworks but can all be executed in the same python environment\n\n\nmodels are written in different frameworks and can't be executed in the same python environment due to dependency incompatibilities\n\n\n\n\nAll models in the same framework\n\n\nIn case all models are written in the same framework, you can stitch things together in the framework. Here is an example of how to do this in Keras.\n\n\n\n\nLet's first dump 4 dummy models:\n\n\nimport keras.layers as kl\nfrom keras.models import Model\nfrom keras.models import load_model\n\n\n\n\n# create model 1\ninp1 = kl.Input((3,), name=\"input1\")\nout1 = kl.Dense(4)(inp1)\nm1 = Model(inp1, out1)\nm1.save(\"/tmp/m1.h5\")\n\n# create model 2\ninp2 = kl.Input((7,), name=\"input1_model1\")\nout2 = kl.Dense(3)(inp2)\nm2 = Model(inp2, out2)\nm2.save(\"/tmp/m2.h5\")\n\n# create model 3\ninp3 = kl.Input((6,), name=\"input2\")\nout3 = kl.Dense(4)(inp3)\nm3 = Model(inp3, out3)\nm3.save(\"/tmp/m3.h5\")\n\n# create model 4\ninp4 = kl.Input((7,), name=\"model2_model3\")\nout4 = kl.Dense(1)(inp4)\nm4 = Model(inp4, out4)\nm4.save(\"/tmp/m4.h5\")\n\n\n\n\nNext, we load the models back:\n\n\n## Load models\nm1 = load_model(\"/tmp/m1.h5\")\nm2 = load_model(\"/tmp/m2.h5\")\nm3 = load_model(\"/tmp/m3.h5\")\nm4 = load_model(\"/tmp/m4.h5\")\n\n\n\n\n/opt/modules/i12g/anaconda/3-5.0.1/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n  warnings.warn('No training configuration found in save file: '\n\n\n\nAnd compose them\n\n\nm2_in = kl.concatenate([m1.output, m1.input])\nm2_out = m2(m2_in)\n\nm3_in = kl.concatenate([m2_out, m3.output])\nout = m4(m3_in)\n\nm = Model(inputs=[m1.input, m3.input], outputs=out)\n\n\n\n\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nsvg_img = model_to_dot(m, ).create(prog='dot', format='svg')\nSVG(svg_img)\n\n\n\n\n\n\nNow we could go ahead, merge the dataloaders from model1 and model3 into a single one (providing input1 and input2) and train this global network for a new task. In case we would like to freeze supparts of the network, we should 'freeze' the underlying models by setting \nm1.trainable = False\n.\n\n\nContributing to Kipoi\n\n\nTo contribute such model to Kipoi, we would need to submit the merged dataloader (providing input1 and input2 from raw files) and dump the stitched Keras model. \n\n\nModels in different frameworks\n\n\nThere are two scenarios when composing models from different frameworks. Either their dependencies (dataloader, etc) are compatible (say a tensorflow and a keras model) or they are incompatible (one model uses \nkeras=0.3\n and and another one \nkeras=2.0\n).\n\n\nCompatible dependencies\n\n\nTo compose compatible models, we pack the majority of the models into the dataloader and then have the final ensembling model stored as the model.\n\n\n\n\ndef new_dataloader(dl1_kwargs, dl2_kwargs, target_file, batch_size=32, num_workers=1):\n    m1 = kipoi.get_model(\"model1\")\n    m2 = kipoi.get_model(\"model2\")\n    m3 = kipoi.get_model(\"model3\")\n\n    dl1 = m1.default_dataloader(**dl1_kwargs)\n    dl2 = m1.default_dataloader(**dl2_kwargs)\n\n    target_gen = get_target_gen(target_file)\n\n    batch_it1 = dl1.batch_iter(batch_size=batch_size, num_workers=num_workers)\n    batch_it2 = dl2.batch_iter(batch_size=batch_size, num_workers=num_workers)\n\n    while True:\n        batch1 = next(batch_it1)['inputs']\n        batch2 = next(batch_it2)['inputs']\n        targets, ids = next(target_gen)\n\n        m1_pred = m1.predict_on_batch(batch1)\n        m2_pred = m2.predict_on_batch(np.concatenate((batch1, m1_pred), axis=1))\n        m3_pred = m3.predict_on_batch(batch2)\n        yield {\"inputs\": {\"model2\": m2_pred, \"model3\": m3_pred}, \n               \"targets\": targets, \n               \"metadata\": {\"model1_id\": batch1[\"metadata\"][\"id\"],\n                            \"model3_id\": batch2[\"metadata\"][\"id\"],\n                            \"targets_id\": ids,\n                           }\n              }\n\n\n\n\n# create model 4\ninp2 = kl.Input((3,), name=\"model2\")\ninp3 = kl.Input((4,), name=\"model3\")\nx = kl.concatenate([inp2, inp3])\nout4 = kl.Dense(1)(x)\nm4 = Model([inp2, inp3], out4)\nm4.compile('rmsprop',\n           loss='categorical_crossentropy',\n           metrics=['accuracy'])\n\n\n\n\n# Train model4\ndef create_train_gen(**kwargs):\n    while True:\n        gen = new_dataloader(**kwargs)\n        while True:\n            batch = next(gen)\n            yield (batch['inputs'], batch['targets'])\n\ntrain_gen = create_train_gen(...)            \nm4.fit_generator(train_gen, ...)\n\n\n\n\n# Dump model4\nm4.save(\"model_files/model.h5\")\n\n\n\n\nIncompatible dependencies\n\n\nSometimes, making a prediction for all the models in the same python environment might be difficult or impossible due to the incompatible dependencies. \n\n\nIn that case, we should run the prediction of each model in a separate environment and save the predictions to the disk.\n\n\nLuckily, there exist many Make-like tools that can support this kind of a workflow. My favorite is Snakemake \nhttp://snakemake.readthedocs.io/\n. I'll show you how to do this in snakemake.\n\n\nLet's consider the following case:\n\n\n\n\n# Python part of the Snakefile\nimport os\nimport subprocess\npy_path = subprocess.check_output(['which', 'python']).decode().strip()\nenv_paths = os.path.join(os.path.dirname(py_path), \"../envs\")\n\ndef get_args(wildcards):\n    \"\"\"Function returning a dictionary of dataloader kwargs\n    for the corresponding model\n    \"\"\"\n    if wildcards.model == \"model3\":\n        return {\"arg1\": 1}\n    elif wildcards.model == \"model3\":\n        return {\"\"}\n    else:\n        return {\"arg2\": 1}\n\n\n\n\n# Yaml part of the Snakefile\nrule all:\n  inputs: expand(\"predictions/{model}.h5\", [\"model1\", \"model2\"])\n\nrule create_evironment:\n  \"\"\"Create a new conda environment for each model\"\"\"\n  output: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\")\n  shell: \"kipoi env create {wildcards.model} -e kipoi-{wildcards.model}\"\n\nrule run_predictions:\n  \"\"\"Create a new conda environment for each model\"\"\"\n  input: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\")\n  output: \"predictions/{model}.h5\"\n  params:\n    dl_args: get_args\n    batch_size: 15\n  threads: 8\n  shell: \n      \"\"\"\n      source activate kipoi-{wildcards.model}\n      kipoi predict {wildcards.model} \\\n        -n {threads} \\\n        --dataloader_args='{params.dl_args}' \\\n        --batch_size={params.batch_size} \\\n        -f hdf5 \\\n        -o {output} \n      \"\"\"\n\n\n\n\nThis snakefile will generate the following hdf5 files\n\n\n\n\npredictions/model1.h5\n\n\npredictions/model2.h5\n\n\n\n\nTo combine them, let's write new dataloader, taking as input the hdf5 files containing predictions\n\n\nimport deepdish\n\n\ndef new_dataloader(model1_h5, model2_h5, target_file):\n    d1 = deepdish.io.load(model1_h5)\n    d2 = deepdish.io.load(model2_h5)\n    targets = load_target_file(target_file)\n    return {\n        \"inputs\": {\n            \"model1\": d1[\"predictions\"],\n            \"model2\": d2[\"predictions\"],\n        },\n        \"targets\": targets,\n        \"metadata\": {\n            \"model1_id\": d1[\"metdata\"][\"id\"],\n            \"model2_id\": d2[\"metdata\"][\"id\"],\n        }\n    }\n\n\n\n\n# get the training data ...\ndata_train = new_dataloader(\"predictions/model1.h5\"\n                            \"predictions/model1.h5\",\n                            \"target_file.h5\")\n\n\n\n\n# train the model...\nm4.fit(data_train['inputs'], data_train['targets'])\n\n\n\n\n# Dump the model\nm4.save(\"model_files/model.h5\")\n\n\n\n\nUploading composite models to Kipoi\n\n\nSince every Kipoi model pipeline consists of a single dataloader and a single model, we have to pack multiple models either into a single model or a single dataloader. Here is the recommendation how to do so:\n\n\n\n\nAll models in the same framework\n\n\nDataloader:\n newly written, combines dataloaders\n\n\nModel:\n combines models by stitching them together in the framework\n\n\n\n\n\n\nDifferent frameworks, compatible dependencies\n\n\nDataloader:\n newly written, combines dataloaders and models\n\n\nModel:\n final ensembling model (model 4)\n\n\n\n\n\n\nDifferent frameworks, in-compatible dependencies\n\n\nDataloader:\n newly written, loads data from the hdf5 files containing model predictions\n\n\nModel:\n final ensembling model (model 4)",
            "title": "Composing models"
        },
        {
            "location": "/tutorials/composing_models/#composing-models",
            "text": "by Ziga Avsec  Composing models means that we take the predictions of some model and use it as input for another model like this:   Three different scenarios can occur when we want to compose models from Kipoi:   all models are written in the same framework (say Keras)  models are written in different frameworks but can all be executed in the same python environment  models are written in different frameworks and can't be executed in the same python environment due to dependency incompatibilities",
            "title": "Composing models"
        },
        {
            "location": "/tutorials/composing_models/#all-models-in-the-same-framework",
            "text": "In case all models are written in the same framework, you can stitch things together in the framework. Here is an example of how to do this in Keras.   Let's first dump 4 dummy models:  import keras.layers as kl\nfrom keras.models import Model\nfrom keras.models import load_model  # create model 1\ninp1 = kl.Input((3,), name=\"input1\")\nout1 = kl.Dense(4)(inp1)\nm1 = Model(inp1, out1)\nm1.save(\"/tmp/m1.h5\")\n\n# create model 2\ninp2 = kl.Input((7,), name=\"input1_model1\")\nout2 = kl.Dense(3)(inp2)\nm2 = Model(inp2, out2)\nm2.save(\"/tmp/m2.h5\")\n\n# create model 3\ninp3 = kl.Input((6,), name=\"input2\")\nout3 = kl.Dense(4)(inp3)\nm3 = Model(inp3, out3)\nm3.save(\"/tmp/m3.h5\")\n\n# create model 4\ninp4 = kl.Input((7,), name=\"model2_model3\")\nout4 = kl.Dense(1)(inp4)\nm4 = Model(inp4, out4)\nm4.save(\"/tmp/m4.h5\")  Next, we load the models back:  ## Load models\nm1 = load_model(\"/tmp/m1.h5\")\nm2 = load_model(\"/tmp/m2.h5\")\nm3 = load_model(\"/tmp/m3.h5\")\nm4 = load_model(\"/tmp/m4.h5\")  /opt/modules/i12g/anaconda/3-5.0.1/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n  warnings.warn('No training configuration found in save file: '  And compose them  m2_in = kl.concatenate([m1.output, m1.input])\nm2_out = m2(m2_in)\n\nm3_in = kl.concatenate([m2_out, m3.output])\nout = m4(m3_in)\n\nm = Model(inputs=[m1.input, m3.input], outputs=out)  from IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nsvg_img = model_to_dot(m, ).create(prog='dot', format='svg')\nSVG(svg_img)   Now we could go ahead, merge the dataloaders from model1 and model3 into a single one (providing input1 and input2) and train this global network for a new task. In case we would like to freeze supparts of the network, we should 'freeze' the underlying models by setting  m1.trainable = False .",
            "title": "All models in the same framework"
        },
        {
            "location": "/tutorials/composing_models/#contributing-to-kipoi",
            "text": "To contribute such model to Kipoi, we would need to submit the merged dataloader (providing input1 and input2 from raw files) and dump the stitched Keras model.",
            "title": "Contributing to Kipoi"
        },
        {
            "location": "/tutorials/composing_models/#models-in-different-frameworks",
            "text": "There are two scenarios when composing models from different frameworks. Either their dependencies (dataloader, etc) are compatible (say a tensorflow and a keras model) or they are incompatible (one model uses  keras=0.3  and and another one  keras=2.0 ).",
            "title": "Models in different frameworks"
        },
        {
            "location": "/tutorials/composing_models/#compatible-dependencies",
            "text": "To compose compatible models, we pack the majority of the models into the dataloader and then have the final ensembling model stored as the model.   def new_dataloader(dl1_kwargs, dl2_kwargs, target_file, batch_size=32, num_workers=1):\n    m1 = kipoi.get_model(\"model1\")\n    m2 = kipoi.get_model(\"model2\")\n    m3 = kipoi.get_model(\"model3\")\n\n    dl1 = m1.default_dataloader(**dl1_kwargs)\n    dl2 = m1.default_dataloader(**dl2_kwargs)\n\n    target_gen = get_target_gen(target_file)\n\n    batch_it1 = dl1.batch_iter(batch_size=batch_size, num_workers=num_workers)\n    batch_it2 = dl2.batch_iter(batch_size=batch_size, num_workers=num_workers)\n\n    while True:\n        batch1 = next(batch_it1)['inputs']\n        batch2 = next(batch_it2)['inputs']\n        targets, ids = next(target_gen)\n\n        m1_pred = m1.predict_on_batch(batch1)\n        m2_pred = m2.predict_on_batch(np.concatenate((batch1, m1_pred), axis=1))\n        m3_pred = m3.predict_on_batch(batch2)\n        yield {\"inputs\": {\"model2\": m2_pred, \"model3\": m3_pred}, \n               \"targets\": targets, \n               \"metadata\": {\"model1_id\": batch1[\"metadata\"][\"id\"],\n                            \"model3_id\": batch2[\"metadata\"][\"id\"],\n                            \"targets_id\": ids,\n                           }\n              }  # create model 4\ninp2 = kl.Input((3,), name=\"model2\")\ninp3 = kl.Input((4,), name=\"model3\")\nx = kl.concatenate([inp2, inp3])\nout4 = kl.Dense(1)(x)\nm4 = Model([inp2, inp3], out4)\nm4.compile('rmsprop',\n           loss='categorical_crossentropy',\n           metrics=['accuracy'])  # Train model4\ndef create_train_gen(**kwargs):\n    while True:\n        gen = new_dataloader(**kwargs)\n        while True:\n            batch = next(gen)\n            yield (batch['inputs'], batch['targets'])\n\ntrain_gen = create_train_gen(...)            \nm4.fit_generator(train_gen, ...)  # Dump model4\nm4.save(\"model_files/model.h5\")",
            "title": "Compatible dependencies"
        },
        {
            "location": "/tutorials/composing_models/#incompatible-dependencies",
            "text": "Sometimes, making a prediction for all the models in the same python environment might be difficult or impossible due to the incompatible dependencies.   In that case, we should run the prediction of each model in a separate environment and save the predictions to the disk.  Luckily, there exist many Make-like tools that can support this kind of a workflow. My favorite is Snakemake  http://snakemake.readthedocs.io/ . I'll show you how to do this in snakemake.  Let's consider the following case:   # Python part of the Snakefile\nimport os\nimport subprocess\npy_path = subprocess.check_output(['which', 'python']).decode().strip()\nenv_paths = os.path.join(os.path.dirname(py_path), \"../envs\")\n\ndef get_args(wildcards):\n    \"\"\"Function returning a dictionary of dataloader kwargs\n    for the corresponding model\n    \"\"\"\n    if wildcards.model == \"model3\":\n        return {\"arg1\": 1}\n    elif wildcards.model == \"model3\":\n        return {\"\"}\n    else:\n        return {\"arg2\": 1}  # Yaml part of the Snakefile\nrule all:\n  inputs: expand(\"predictions/{model}.h5\", [\"model1\", \"model2\"])\n\nrule create_evironment:\n  \"\"\"Create a new conda environment for each model\"\"\"\n  output: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\")\n  shell: \"kipoi env create {wildcards.model} -e kipoi-{wildcards.model}\"\n\nrule run_predictions:\n  \"\"\"Create a new conda environment for each model\"\"\"\n  input: os.path.join(env_paths, \"kipoi-{model}\", \"bin/kipoi\")\n  output: \"predictions/{model}.h5\"\n  params:\n    dl_args: get_args\n    batch_size: 15\n  threads: 8\n  shell: \n      \"\"\"\n      source activate kipoi-{wildcards.model}\n      kipoi predict {wildcards.model} \\\n        -n {threads} \\\n        --dataloader_args='{params.dl_args}' \\\n        --batch_size={params.batch_size} \\\n        -f hdf5 \\\n        -o {output} \n      \"\"\"  This snakefile will generate the following hdf5 files   predictions/model1.h5  predictions/model2.h5   To combine them, let's write new dataloader, taking as input the hdf5 files containing predictions  import deepdish\n\n\ndef new_dataloader(model1_h5, model2_h5, target_file):\n    d1 = deepdish.io.load(model1_h5)\n    d2 = deepdish.io.load(model2_h5)\n    targets = load_target_file(target_file)\n    return {\n        \"inputs\": {\n            \"model1\": d1[\"predictions\"],\n            \"model2\": d2[\"predictions\"],\n        },\n        \"targets\": targets,\n        \"metadata\": {\n            \"model1_id\": d1[\"metdata\"][\"id\"],\n            \"model2_id\": d2[\"metdata\"][\"id\"],\n        }\n    }  # get the training data ...\ndata_train = new_dataloader(\"predictions/model1.h5\"\n                            \"predictions/model1.h5\",\n                            \"target_file.h5\")  # train the model...\nm4.fit(data_train['inputs'], data_train['targets'])  # Dump the model\nm4.save(\"model_files/model.h5\")",
            "title": "Incompatible dependencies"
        },
        {
            "location": "/tutorials/composing_models/#uploading-composite-models-to-kipoi",
            "text": "Since every Kipoi model pipeline consists of a single dataloader and a single model, we have to pack multiple models either into a single model or a single dataloader. Here is the recommendation how to do so:   All models in the same framework  Dataloader:  newly written, combines dataloaders  Model:  combines models by stitching them together in the framework    Different frameworks, compatible dependencies  Dataloader:  newly written, combines dataloaders and models  Model:  final ensembling model (model 4)    Different frameworks, in-compatible dependencies  Dataloader:  newly written, loads data from the hdf5 files containing model predictions  Model:  final ensembling model (model 4)",
            "title": "Uploading composite models to Kipoi"
        },
        {
            "location": "/tutorials/variant_effect_prediction/",
            "text": "Generated from \nnbs/variant_effect_prediction.ipynb\n\n\nVariant effect prediction\n\n\nVariant effect prediction offers a simple way to predict effects of SNVs using any model that uses DNA sequence as an input. Many different scoring methods can be chosen, but the principle relies on in-silico mutagenesis. The default input is a VCF and the default output again is a VCF annotated with predictions of variant effects. \n\n\nFor details please take a look at the documentation in Postprocessing/Variant effect prediction. This iPython notebook goes through the basic programmatic steps that are needed to preform variant effect prediction. First a variant-centered approach will be taken and secondly overlap-based variant effect prediction will be presented. For details in how this is done programmatically, please refer to the documentation.\n\n\nVariant centered effect prediction\n\n\nModels that accept input \n.bed\n files can make use of variant-centered effect prediction. This procedure starts out from the query VCF and generates genomic regions of the length of the model input, centered on the individual variant in the VCF.The model dataloader is then used to produce the model input samples for those regions, which are then mutated according to the alleles in the VCF:\n\n\n\n\nFirst an instance of \nSnvCenteredRg\n generates a temporary bed file with regions matching the input sequence length defined in the model.yaml input schema. Then the model dataloader is used to preduce the model input in batches. These chunks of data are then modified by the effect prediction algorithm, the model batch prediction function is triggered for all mutated sequence sets and finally the scoring method is applied.\n\n\nThe selected scoring methods compare model predicitons for sequences carrying the reference or alternative allele. Those scoring methods can be \nDiff\n for simple subtraction of prediction, \nLogit\n for substraction of logit-transformed model predictions, or \nDeepSEA_effect\n which is a combination of \nDiff\n and \nLogit\n, which was published in the Troyanskaya et al. (2015) publication.\n\n\nLet's start out by loading the DeepSEA model and dataloader factory:\n\n\nimport kipoi\nmodel_name = \"DeepSEA/variantEffects\"\nkipoi.pipeline.install_model_requirements(model_name)\n# get the model\nmodel = kipoi.get_model(model_name)\n# get the dataloader factory\nDataloader = kipoi.get_dataloader_factory(model_name)\n\n\n\n\nConda dependencies to be installed:\n['h5py', 'pytorch::pytorch-cpu>=0.2.0']\nFetching package metadata .................\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi:\n#\nh5py                      2.7.1            py35ha1a889d_0  \npytorch-cpu               0.3.1                py35_cpu_2    pytorch\npip dependencies to be installed:\n[]\n\n\n\nNext we will have to define the variants we want to look at, let's look at a sample VCF in chromosome 22:\n\n\n!head -n 40 example_data/clinvar_donor_acceptor_chr22.vcf\n\n\n\n\n##fileformat=VCFv4.0\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##contig=<ID=chr1,length=249250621>\n##contig=<ID=chr2,length=243199373>\n##contig=<ID=chr3,length=198022430>\n##contig=<ID=chr4,length=191154276>\n##contig=<ID=chr5,length=180915260>\n##contig=<ID=chr6,length=171115067>\n##contig=<ID=chr7,length=159138663>\n##contig=<ID=chr8,length=146364022>\n##contig=<ID=chr9,length=141213431>\n##contig=<ID=chr10,length=135534747>\n##contig=<ID=chr11,length=135006516>\n##contig=<ID=chr12,length=133851895>\n##contig=<ID=chr13,length=115169878>\n##contig=<ID=chr14,length=107349540>\n##contig=<ID=chr15,length=102531392>\n##contig=<ID=chr16,length=90354753>\n##contig=<ID=chr17,length=81195210>\n##contig=<ID=chr18,length=78077248>\n##contig=<ID=chr19,length=59128983>\n##contig=<ID=chr20,length=63025520>\n##contig=<ID=chr21,length=48129895>\n##contig=<ID=chr22,length=51304566>\n##contig=<ID=chrX,length=155270560>\n##contig=<ID=chrY,length=59373566>\n##contig=<ID=chrMT,length=16569>\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO\nchr22   41320486    4   G   T   .   .   .\nchr22   31009031    9   T   G   .   .   .\nchr22   43024150    15  C   G   .   .   .\nchr22   43027392    16  A   G   .   .   .\nchr22   37469571    122 C   T   .   .   .\nchr22   37465112    123 C   G   .   .   .\nchr22   37494466    124 G   T   .   .   .\nchr22   18561373    177 G   T   .   .   .\nchr22   51065593    241 C   T   .   .   .\nchr22   51064006    242 C   T   .   .   .\nchr22   51065269    243 G   A   .   .   .\nchr22   30032866    260 G   T   .   .   .\n\n\n\nNow we will define path variable for vcf input and output paths and instantiate a VcfWriter, which will write out the annotated VCF:\n\n\nfrom kipoi.postprocessing.variant_effects import VcfWriter\n# The input vcf path\nvcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\"\n# The output vcf path, based on the input file name    \nout_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\")\n# The writer object that will output the annotated VCF\nwriter = VcfWriter(model, vcf_path, out_vcf_fpath)\n\n\n\n\nThen we need to instantiate an object that can generate variant-centered regions (\nSnvCenteredRg\n objects). This class needs information on the model input sequence length which is extracted automatically within \nModelInfoExtractor\n objects:\n\n\n# Information extraction from dataloader and model\nmodel_info = kipoi.postprocessing.variant_effects.ModelInfoExtractor(model, Dataloader)\n# vcf_to_region will generate a variant-centered regions when presented a VCF record.\nvcf_to_region = kipoi.postprocessing.variant_effects.SnvCenteredRg(model_info)\n\n\n\n\nNow we can define the required dataloader arguments, omitting the \nintervals_file\n as this will be replaced by the automatically generated bed file:\n\n\ndataloader_arguments = {\"fasta_file\": \"example_data/hg19_chr22.fa\"}\n\n\n\n\nThis is the moment to run the variant effect prediction:\n\n\nimport kipoi.postprocessing.variant_effects.snv_predict as sp\nfrom kipoi.postprocessing.variant_effects import Diff, DeepSEA_effect\nsp.predict_snvs(model,\n                Dataloader,\n                vcf_path,\n                batch_size = 32,\n                dataloader_args=dataloader_arguments,\n                vcf_to_region=vcf_to_region,\n                evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\"), 'deepsea_effect': DeepSEA_effect(\"mean\")}},\n                sync_pred_writer=writer)\n\n\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:38<00:00,  2.73s/it]\n\n\n\nIn the example above we have used the variant scoring method \nDiff\n and \nDeepSEA_effect\n from \nkipoi.postprocessing.variant_effects\n. As mentioned above variant scoring methods calculate the difference between predictions for reference and alternative, but there is another dimension to this: Models that have the \nuse_rc: true\n flag set in their model.yaml file (DeepSEA/variantEffects does) will not only be queried with the reference and alternative carrying input sequences, but also with the reverse complement of the the sequences. In order to know of to combine predictions for forward and reverse sequences there is a initialisation flag (here set to: \n\"mean\"\n) for the variant scoring methods. \n\"mean\"\n in this case means that after calculating the effect (e.g.: Difference) the average over the difference between the prediction for the forward and for the reverse sequence should be returned. Setting \n\"mean\"\n complies with what was used in the Troyanskaya et al. publication.\n\n\nNow let's look at the output:\n\n\n# Let's print out the first 40 lines of the annotated VCF (up to 80 characters per line maximum)\nwith open(\"example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf\") as ifh:\n    for i,l in enumerate(ifh):\n        long_line = \"\"\n        if len(l)>80:\n            long_line = \"...\"\n        print(l[:80].rstrip() +long_line)\n        if i >=40:\n            break\n\n\n\n\n##fileformat=VCFv4.0\n##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:DEEPSEA_EFFECT,Number=.,Type=String,D...\n##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:DIFF,Number=.,Type=String,Description...\n##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:rID,Number=.,Type=String,Description=...\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##contig=<ID=chr1,length=249250621>\n##contig=<ID=chr2,length=243199373>\n##contig=<ID=chr3,length=198022430>\n##contig=<ID=chr4,length=191154276>\n##contig=<ID=chr5,length=180915260>\n##contig=<ID=chr6,length=171115067>\n##contig=<ID=chr7,length=159138663>\n##contig=<ID=chr8,length=146364022>\n##contig=<ID=chr9,length=141213431>\n##contig=<ID=chr10,length=135534747>\n##contig=<ID=chr11,length=135006516>\n##contig=<ID=chr12,length=133851895>\n##contig=<ID=chr13,length=115169878>\n##contig=<ID=chr14,length=107349540>\n##contig=<ID=chr15,length=102531392>\n##contig=<ID=chr16,length=90354753>\n##contig=<ID=chr17,length=81195210>\n##contig=<ID=chr18,length=78077248>\n##contig=<ID=chr19,length=59128983>\n##contig=<ID=chr20,length=63025520>\n##contig=<ID=chr21,length=48129895>\n##contig=<ID=chr22,length=51304566>\n##contig=<ID=chrX,length=155270560>\n##contig=<ID=chrY,length=59373566>\n##contig=<ID=chrMT,length=16569>\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO\nchr22   41320486    chr22:41320486:G:['T']  G   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   31009031    chr22:31009031:T:['G']  T   G   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   43024150    chr22:43024150:C:['G']  C   G   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   43027392    chr22:43027392:A:['G']  A   G   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   37469571    chr22:37469571:C:['T']  C   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   37465112    chr22:37465112:C:['G']  C   G   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   37494466    chr22:37494466:G:['T']  G   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   18561373    chr22:18561373:G:['T']  G   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   51065593    chr22:51065593:C:['T']  C   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   51064006    chr22:51064006:C:['T']  C   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\n\n\n\nThis shows that variants have been annotated with variant effect scores. For every different scoring method a different INFO tag was created and the score of every model output is concantenated with the \n|\n separator symbol. A legend is given in the header section of the VCF. The name tag indicates with model was used, wich version of it and it displays the scoring function label (\nDIFF\n) which is derived from the scoring function label defined in the \nevaluation_function_kwargs\n dictionary (\n'diff'\n).\n\n\nThe most comprehensive representation of effect preditions is in the annotated VCF. Kipoi offers a VCF parser class that enables simple parsing of annotated VCFs:\n\n\nfrom kipoi.postprocessing.variant_effects import KipoiVCFParser\nvcf_reader = KipoiVCFParser(\"example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf\")\n\n#We can have a look at the different labels which were created in the VCF\nprint(list(vcf_reader.kipoi_parsed_colnames.values()))\n\n\n\n\n[('kipoi', 'DeepSEA/variantEffects', 'DEEPSEA_EFFECT'), ('kipoi', 'DeepSEA/variantEffects', 'rID'), ('kipoi', 'DeepSEA/variantEffects', 'DIFF')]\n\n\n\nWe can see that two scores have been saved - \n'DEEPSEA_EFFECT'\n and \n'DIFF'\n. Additionally there is \n'rID'\n which is the region ID - that is the ID given by the dataloader for a genomic region which was overlapped with the variant to get the prediction that is listed in the effect score columns mentioned before. Let's take a look at the VCF entries:\n\n\nimport pandas as pd\nentries = [el for el in vcf_reader]\nprint(pd.DataFrame(entries).head().iloc[:,:7])\n\n\n\n\n               variant_id variant_chr  variant_pos variant_ref variant_alt  \\\n0  chr22:41320486:G:['T']       chr22     41320486           G           T   \n1  chr22:31009031:T:['G']       chr22     31009031           T           G   \n2  chr22:43024150:C:['G']       chr22     43024150           C           G   \n3  chr22:43027392:A:['G']       chr22     43027392           A           G   \n4  chr22:37469571:C:['T']       chr22     37469571           C           T\n\n   KV_DeepSEA/variantEffects_DEEPSEA_EFFECT_8988T_DNase_None_0  \\\n0                                           0.000377             \n1                                           0.004129             \n2                                           0.001582             \n3                                           0.068382             \n4                                           0.001174\n\n   KV_DeepSEA/variantEffects_DEEPSEA_EFFECT_AoSMC_DNase_None_1  \n0                                       9.700000e-07            \n1                                       3.683220e-03            \n2                                       1.824500e-04            \n3                                       2.689577e-01            \n4                                       4.173300e-04\n\n\n\nAnother way to access effect predicitons programmatically is to keep all the results in memory and receive them as a dictionary of pandas dataframes:\n\n\neffects = sp.predict_snvs(model,\n            Dataloader,\n            vcf_path,\n            batch_size = 32,\n            dataloader_args=dataloader_arguments,\n            vcf_to_region=vcf_to_region,\n            evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\"), 'deepsea_effect': DeepSEA_effect(\"mean\")}},\n            return_predictions=True)\n\n\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:36<00:00,  2.64s/it]\n\n\n\nFor every key in the \nevaluation_function_kwargs\n dictionary there is a key in \neffects\n and (the equivalent of an additional INFO tag in the VCF). Now let's take a look at the results:\n\n\nfor k in effects:\n    print(k)\n    print(effects[k].head().iloc[:,:4])\n    print(\"-\"*80)\n\n\n\n\ndeepsea_effect\n                        8988T_DNase_None  AoSMC_DNase_None  \\\nchr22:41320486:G:['T']          0.000377      9.663903e-07   \nchr22:31009031:T:['G']          0.004129      3.683221e-03   \nchr22:43024150:C:['G']          0.001582      1.824510e-04   \nchr22:43027392:A:['G']          0.068382      2.689577e-01   \nchr22:37469571:C:['T']          0.001174      4.173280e-04\n\n                        Chorion_DNase_None  CLL_DNase_None  \nchr22:41320486:G:['T']            0.000162        0.000040  \nchr22:31009031:T:['G']            0.000201        0.002139  \nchr22:43024150:C:['G']            0.000322        0.000033  \nchr22:43027392:A:['G']            0.133855        0.000773  \nchr22:37469571:C:['T']            0.000008        0.000079  \n--------------------------------------------------------------------------------\ndiff\n                        8988T_DNase_None  AoSMC_DNase_None  \\\nchr22:41320486:G:['T']         -0.002850         -0.000094   \nchr22:31009031:T:['G']         -0.027333         -0.008740   \nchr22:43024150:C:['G']          0.010773          0.000702   \nchr22:43027392:A:['G']         -0.121747         -0.247321   \nchr22:37469571:C:['T']         -0.006546          0.000784\n\n                        Chorion_DNase_None  CLL_DNase_None  \nchr22:41320486:G:['T']           -0.001533       -0.000353  \nchr22:31009031:T:['G']           -0.003499       -0.008143  \nchr22:43024150:C:['G']            0.004689       -0.000609  \nchr22:43027392:A:['G']           -0.167689       -0.010695  \nchr22:37469571:C:['T']           -0.000383       -0.000924  \n--------------------------------------------------------------------------------\n\n\n\nWe see that for \ndiff\n and \ndeepsea_effect\n there is a dataframe with variant identifiers as rows and model output labels as columns. The DeepSEA model predicts 919 tasks simultaneously hence there are 919 columns in the dataframe.\n\n\nOverlap based prediction\n\n\nModels that cannot predict on every region of the genome might not accept a \n.bed\n file as dataloader input. An example of such a model is a splicing model. Those models only work in certain regions of the genome. Here variant effect prediction can be executed based on overlaps between the regions generated by the dataloader and the variants defined in the VCF:\n\n\n\n\nThe procedure is similar to the variant centered effect prediction explained above, but in this case no temporary bed file is generated and the effect prediction is based on all the regions generated by the dataloader which overlap any variant in the VCF. If a region is overlapped by two variants the effect of the two variants is predicted independently.\n\n\nHere the VCF has to be tabixed so that a regional lookup can be performed efficiently, this can be done by using the \nensure_tabixed\n function, the rest remains the same as before:\n\n\nimport kipoi\nfrom kipoi.postprocessing.variant_effects import VcfWriter\nfrom kipoi.postprocessing.variant_effects import ensure_tabixed_vcf\n# Use a splicing model\nmodel_name = \"HAL\"\n# install dependencies\nkipoi.pipeline.install_model_requirements(model_name)\n# get the model\nmodel = kipoi.get_model(model_name)\n# get the dataloader factory\nDataloader = kipoi.get_dataloader_factory(model_name)\n# The input vcf path\nvcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\"\n\n# Make sure that the vcf is bgzipped and tabixed, if not then generate the compressed vcf in the same place\nvcf_path_tbx = ensure_tabixed_vcf(vcf_path)\n\n# The output vcf path, based on the input file name    \nout_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\")\n# The writer object that will output the annotated VCF\nwriter = VcfWriter(model, vcf_path, out_vcf_fpath)\n\n\n\n\nConda dependencies to be installed:\n['numpy']\nFetching package metadata ...............\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi:\n#\nnumpy                     1.14.1           py35h3dfced4_1  \npip dependencies to be installed:\n[]\n\n\n\nThis time we don't need an object that generates regions, hence we can directly define the dataloader arguments and run the prediction:\n\n\nfrom kipoi.postprocessing.variant_effects import Diff, predict_snvs\ndataloader_arguments = {\"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75.filtered_chr22.gtf\",\n                               \"fasta_file\": \"example_data/hg19_chr22.fa\"}\n\neffects = predict_snvs(model,\n                        Dataloader,\n                        vcf_path_tbx,\n                        batch_size = 32,\n                        dataloader_args=dataloader_arguments,\n                        evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\")}},\n                        sync_pred_writer=writer,\n                        return_predictions=True)\n\n\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:04<00:00, 176.63it/s]\n\n\n\nLet's have a look at the VCF:\n\n\n# A slightly convoluted way of printing out the first 40 lines and up to 80 characters per line maximum\nwith open(\"example_data/clinvar_donor_acceptor_chr22HAL.vcf\") as ifh:\n    for i,l in enumerate(ifh):\n        long_line = \"\"\n        if len(l)>80:\n            long_line = \"...\"\n        print(l[:80].rstrip() +long_line)\n        if i >=40:\n            break\n\n\n\n\n##fileformat=VCFv4.0\n##INFO=<ID=KV:kipoi:HAL:DIFF,Number=.,Type=String,Description=\"DIFF SNV effect p...\n##INFO=<ID=KV:kipoi:HAL:rID,Number=.,Type=String,Description=\"Range or region id...\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##contig=<ID=chr1,length=249250621>\n##contig=<ID=chr2,length=243199373>\n##contig=<ID=chr3,length=198022430>\n##contig=<ID=chr4,length=191154276>\n##contig=<ID=chr5,length=180915260>\n##contig=<ID=chr6,length=171115067>\n##contig=<ID=chr7,length=159138663>\n##contig=<ID=chr8,length=146364022>\n##contig=<ID=chr9,length=141213431>\n##contig=<ID=chr10,length=135534747>\n##contig=<ID=chr11,length=135006516>\n##contig=<ID=chr12,length=133851895>\n##contig=<ID=chr13,length=115169878>\n##contig=<ID=chr14,length=107349540>\n##contig=<ID=chr15,length=102531392>\n##contig=<ID=chr16,length=90354753>\n##contig=<ID=chr17,length=81195210>\n##contig=<ID=chr18,length=78077248>\n##contig=<ID=chr19,length=59128983>\n##contig=<ID=chr20,length=63025520>\n##contig=<ID=chr21,length=48129895>\n##contig=<ID=chr22,length=51304566>\n##contig=<ID=chrX,length=155270560>\n##contig=<ID=chrY,length=59373566>\n##contig=<ID=chrMT,length=16569>\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO\nchr22   17684454    chr22:17684454:G:['A']  G   A   .   .   KV:kipoi:HAL:DIFF=0.10586491;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   17684454    chr22:17684454:G:['A']  G   A   .   .   KV:kipoi:HAL:DIFF=0.10586491;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   17684454    chr22:17684454:G:['A']  G   A   .   .   KV:kipoi:HAL:DIFF=0.10586491;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   17684454    chr22:17684454:G:['A']  G   A   .   .   KV:kipoi:HAL:DIFF=0.10586491;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   18561370    chr22:18561370:C:['T']  C   T   .   .   KV:kipoi:HAL:DIFF=-1.33794269;KV:k...\n\n\n\nAnd the prediction output this time is less helpful because it's the ids that the dataloader created which are displayed as index. In general it is advisable to use the output VCF for more detailed information on which variant was overlapped with which region fo produce a prediction.\n\n\nfor k in effects:\n    print(k)\n    print(effects[k].head())\n    print(\"-\"*80)\n\n\n\n\ndiff\n            0\n290  0.105865\n293  0.000000\n299  0.000000\n304  0.105865\n307  0.000000\n--------------------------------------------------------------------------------\n\n\n\nCommand-line based effect prediction\n\n\nThe above command can also conveniently be executed using the command line:\n\n\nimport json\nimport os\nmodel_name = \"DeepSEA/variantEffects\"\ndl_args = json.dumps({\"fasta_file\": \"example_data/hg19_chr22.fa\"})\nout_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\")\nscorings = \"diff deepsea_effect\"\ncommand = (\"kipoi postproc score_variants {model} \"\n           \"--dataloader_args='{dl_args}' \"\n           \"--vcf_path {input_vcf} \"\n           \"--out_vcf_fpath {output_vcf} \"\n           \"--scoring {scorings}\").format(model=model_name,\n                                          dl_args=dl_args,\n                                          input_vcf=vcf_path,\n                                          output_vcf=out_vcf_fpath,\n                                          scorings=scorings)\n# Print out the command:\nprint(command)\n\n\n\n\nkipoi postproc score_variants DeepSEA/variantEffects --dataloader_args='{\"fasta_file\": \"example_data/hg19_chr22.fa\"}' --vcf_path example_data/clinvar_donor_acceptor_chr22.vcf --out_vcf_fpath example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf --scoring diff deepsea_effect\n\n\n\n! $command\n\n\n\n\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m Update /homes/rkreuzhu/.kipoi/models/\u001b[0m\nAlready up-to-date.\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/variantEffects/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/model_files/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/example_files/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m model DeepSEA/variantEffects loaded\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/variantEffects/./**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/model_files/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/example_files/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m dataloader DeepSEA/variantEffects/. loaded\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from /homes/rkreuzhu/.kipoi/models/DeepSEA/variantEffects/dataloader.py::SeqDataset\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.postproc]\u001b[0m Using variant-centered sequence generation.\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.postproc]\u001b[0m Model SUPPORTS simple reverse complementation of input DNA sequences.\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.postproc]\u001b[0m Annotated VCF will be written to example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf.\u001b[0m\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:38<00:00,  2.76s/it]\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.postproc]\u001b[0m Successfully predicted samples\u001b[0m\n\n\n\nBatch prediction\n\n\nSince the syntax basically doesn't change for different kinds of models a simple for-loop can be written to do what we just did on many models:\n\n\nimport kipoi\n# Run effect predicton\nmodels_df = kipoi.list_models()\nmodels_substr = [\"HAL\", \"MaxEntScan\", \"labranchor\", \"rbp\"]\nmodels_df_subsets = {ms: models_df.loc[models_df[\"model\"].str.contains(ms)] for ms in models_substr}\n\n\n\n\nLet's make sure that all the dependencies are installed:\n\n\nfor ms in models_substr:\n    model_name = models_df_subsets[ms][\"model\"].iloc[0]\n    kipoi.pipeline.install_model_requirements(model_name)\n\n\n\n\nConda dependencies to be installed:\n['numpy']\nFetching package metadata ...............\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi:\n#\nnumpy                     1.14.1           py35h3dfced4_1  \npip dependencies to be installed:\n[]\nConda dependencies to be installed:\n['bioconda::maxentpy']\nFetching package metadata ...............\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi:\n#\nmaxentpy                  0.0.1                    py35_0    bioconda\npip dependencies to be installed:\n[]\nConda dependencies to be installed:\n[]\npip dependencies to be installed:\n['tensorflow>=1.0.0', 'keras>=2.0.4']\nRequirement already satisfied: tensorflow>=1.0.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages\nRequirement already satisfied: keras>=2.0.4 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages\nRequirement already satisfied: numpy>=1.12.1 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: protobuf>=3.3.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: wheel>=0.26 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: six>=1.10.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: enum34>=1.1.6 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: scipy>=0.14 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from keras>=2.0.4)\nRequirement already satisfied: pyyaml in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from keras>=2.0.4)\nRequirement already satisfied: setuptools in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from protobuf>=3.3.0->tensorflow>=1.0.0)\nRequirement already satisfied: bleach==1.5.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow>=1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow>=1.0.0)\nRequirement already satisfied: html5lib==0.9999999 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow>=1.0.0)\nRequirement already satisfied: werkzeug>=0.11.10 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow>=1.0.0)\nConda dependencies to be installed:\n[]\npip dependencies to be installed:\n['concise>=0.6.5', 'tensorflow==1.4.1', 'keras>=2.0.4']\nRequirement already satisfied: concise>=0.6.5 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages\nRequirement already satisfied: tensorflow==1.4.1 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages\nRequirement already satisfied: keras>=2.0.4 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages\nRequirement already satisfied: scikit-learn>=0.18 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: hyperopt in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: shapely in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: pandas in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: scipy in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: gtfparse>=1.0.7 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: descartes in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: matplotlib in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: numpy in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow==1.4.1)\nRequirement already satisfied: enum34>=1.1.6 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow==1.4.1)\nRequirement already satisfied: protobuf>=3.3.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow==1.4.1)\nRequirement already satisfied: wheel>=0.26 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow==1.4.1)\nRequirement already satisfied: six>=1.10.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow==1.4.1)\nRequirement already satisfied: pyyaml in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from keras>=2.0.4)\nRequirement already satisfied: networkx in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from hyperopt->concise>=0.6.5)\nRequirement already satisfied: pymongo in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from hyperopt->concise>=0.6.5)\nRequirement already satisfied: nose in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from hyperopt->concise>=0.6.5)\nRequirement already satisfied: future in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from hyperopt->concise>=0.6.5)\nRequirement already satisfied: python-dateutil>=2 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/python_dateutil-2.3-py3.5.egg (from pandas->concise>=0.6.5)\nRequirement already satisfied: pytz>=2011k in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from pandas->concise>=0.6.5)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from matplotlib->concise>=0.6.5)\nRequirement already satisfied: cycler>=0.10 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from matplotlib->concise>=0.6.5)\nRequirement already satisfied: werkzeug>=0.11.10 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.1)\nRequirement already satisfied: markdown>=2.6.8 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.1)\nRequirement already satisfied: html5lib==0.9999999 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.1)\nRequirement already satisfied: bleach==1.5.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.1)\nRequirement already satisfied: setuptools in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from protobuf>=3.3.0->tensorflow==1.4.1)\nRequirement already satisfied: decorator>=4.1.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from networkx->hyperopt->concise>=0.6.5)\n\n\n\nNow we are good to go:\n\n\n# Run variant effect prediction using a basic Diff\n\nimport kipoi\nfrom kipoi.postprocessing.variant_effects import ensure_tabixed_vcf\nimport kipoi.postprocessing.variant_effects.snv_predict as sp\nfrom kipoi.postprocessing.variant_effects import VcfWriter\nfrom kipoi.postprocessing.variant_effects import Diff\n\n\n\nsplicing_dl_args = {\"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75.filtered_chr22.gtf\",\n                               \"fasta_file\": \"example_data/hg19_chr22.fa\"}\ndataloader_args_dict = {\"HAL\": splicing_dl_args,\n                        \"labranchor\": splicing_dl_args,\n                        \"MaxEntScan\":splicing_dl_args,\n                        \"rbp\": {\"fasta_file\": \"example_data/hg19_chr22.fa\",\n                               \"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75_chr22.gtf\"}\n                       }\n\nfor ms in models_substr:\n    model_name = models_df_subsets[ms][\"model\"].iloc[0]\n    #kipoi.pipeline.install_model_requirements(model_name)\n    model = kipoi.get_model(model_name)\n    vcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\"\n    vcf_path_tbx = ensure_tabixed_vcf(vcf_path)\n\n    out_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\")\n\n    writer = VcfWriter(model, vcf_path, out_vcf_fpath)\n\n    print(model_name)\n\n    Dataloader = kipoi.get_dataloader_factory(model_name)\n    dataloader_arguments = dataloader_args_dict[ms]\n    model_info = kipoi.postprocessing.variant_effects.ModelInfoExtractor(model, Dataloader)\n    vcf_to_region = None\n    if ms == \"rbp\":\n        vcf_to_region = kipoi.postprocessing.variant_effects.SnvCenteredRg(model_info)\n    sp.predict_snvs(model,\n                    Dataloader,\n                    vcf_path_tbx,\n                    batch_size = 32,\n                    dataloader_args=dataloader_arguments,\n                    vcf_to_region=vcf_to_region,\n                    evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\")}},\n                    sync_pred_writer=writer)\n    writer.close()\n\n\n\n\nHAL\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:04<00:00, 174.46it/s]\n\n\nMaxEntScan/3prime\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:01<00:00, 455.98it/s]\n/nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n/nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n  warnings.warn('Error in loading the saved optimizer '\n\n\nlabranchor\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:05<00:00, 136.26it/s]\n2018-03-06 12:28:12,624 [INFO] successfully loaded the dataloader from /homes/rkreuzhu/.kipoi/models/rbp_eclip/AARS/dataloader.py::SeqDistDataset\n/nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n  warnings.warn('Error in loading the saved optimizer '\n2018-03-06 12:28:14,495 [INFO] successfully loaded the model from model_files/model.h5\n2018-03-06 12:28:14,498 [INFO] dataloader.output_schema is compatible with model.schema\n2018-03-06 12:28:14,545 [INFO] git-lfs pull -I rbp_eclip/AARS/**\n2018-03-06 12:28:14,656 [INFO] git-lfs pull -I rbp_eclip/template/**\n\n\nrbp_eclip/AARS\n\n\n2018-03-06 12:28:14,788 [INFO] dataloader rbp_eclip/AARS loaded\n2018-03-06 12:28:14,817 [INFO] successfully loaded the dataloader from /homes/rkreuzhu/.kipoi/models/rbp_eclip/AARS/dataloader.py::SeqDistDataset\n2018-03-06 12:28:16,018 [INFO] Extracted GTF attributes: ['gene_id', 'gene_name', 'gene_source', 'gene_biotype', 'transcript_id', 'transcript_name', 'transcript_source', 'exon_number', 'exon_id', 'tag', 'protein_id', 'ccds_id']\nINFO:2018-03-06 12:28:16,413:genomelake] Running landmark extractors..\n2018-03-06 12:28:16,413 [INFO] Running landmark extractors..\nINFO:2018-03-06 12:28:16,585:genomelake] Done!\n2018-03-06 12:28:16,585 [INFO] Done!\n/nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n/nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator Imputer from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:04<00:00,  3.10it/s]\n\n\n\nlet's validate that things have worked:\n\n\n! wc -l example_data/clinvar_donor_acceptor_chr22*.vcf\n\n\n\n\n    450 example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf\n   2035 example_data/clinvar_donor_acceptor_chr22HAL.vcf\n    794 example_data/clinvar_donor_acceptor_chr22labranchor.vcf\n   1176 example_data/clinvar_donor_acceptor_chr22MaxEntScan_3prime.vcf\n    449 example_data/clinvar_donor_acceptor_chr22rbp_eclip_AARS.vcf\n    447 example_data/clinvar_donor_acceptor_chr22.vcf\n   5351 total",
            "title": "Variant effect prediction"
        },
        {
            "location": "/tutorials/variant_effect_prediction/#variant-effect-prediction",
            "text": "Variant effect prediction offers a simple way to predict effects of SNVs using any model that uses DNA sequence as an input. Many different scoring methods can be chosen, but the principle relies on in-silico mutagenesis. The default input is a VCF and the default output again is a VCF annotated with predictions of variant effects.   For details please take a look at the documentation in Postprocessing/Variant effect prediction. This iPython notebook goes through the basic programmatic steps that are needed to preform variant effect prediction. First a variant-centered approach will be taken and secondly overlap-based variant effect prediction will be presented. For details in how this is done programmatically, please refer to the documentation.",
            "title": "Variant effect prediction"
        },
        {
            "location": "/tutorials/variant_effect_prediction/#variant-centered-effect-prediction",
            "text": "Models that accept input  .bed  files can make use of variant-centered effect prediction. This procedure starts out from the query VCF and generates genomic regions of the length of the model input, centered on the individual variant in the VCF.The model dataloader is then used to produce the model input samples for those regions, which are then mutated according to the alleles in the VCF:   First an instance of  SnvCenteredRg  generates a temporary bed file with regions matching the input sequence length defined in the model.yaml input schema. Then the model dataloader is used to preduce the model input in batches. These chunks of data are then modified by the effect prediction algorithm, the model batch prediction function is triggered for all mutated sequence sets and finally the scoring method is applied.  The selected scoring methods compare model predicitons for sequences carrying the reference or alternative allele. Those scoring methods can be  Diff  for simple subtraction of prediction,  Logit  for substraction of logit-transformed model predictions, or  DeepSEA_effect  which is a combination of  Diff  and  Logit , which was published in the Troyanskaya et al. (2015) publication.  Let's start out by loading the DeepSEA model and dataloader factory:  import kipoi\nmodel_name = \"DeepSEA/variantEffects\"\nkipoi.pipeline.install_model_requirements(model_name)\n# get the model\nmodel = kipoi.get_model(model_name)\n# get the dataloader factory\nDataloader = kipoi.get_dataloader_factory(model_name)  Conda dependencies to be installed:\n['h5py', 'pytorch::pytorch-cpu>=0.2.0']\nFetching package metadata .................\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi:\n#\nh5py                      2.7.1            py35ha1a889d_0  \npytorch-cpu               0.3.1                py35_cpu_2    pytorch\npip dependencies to be installed:\n[]  Next we will have to define the variants we want to look at, let's look at a sample VCF in chromosome 22:  !head -n 40 example_data/clinvar_donor_acceptor_chr22.vcf  ##fileformat=VCFv4.0\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##contig=<ID=chr1,length=249250621>\n##contig=<ID=chr2,length=243199373>\n##contig=<ID=chr3,length=198022430>\n##contig=<ID=chr4,length=191154276>\n##contig=<ID=chr5,length=180915260>\n##contig=<ID=chr6,length=171115067>\n##contig=<ID=chr7,length=159138663>\n##contig=<ID=chr8,length=146364022>\n##contig=<ID=chr9,length=141213431>\n##contig=<ID=chr10,length=135534747>\n##contig=<ID=chr11,length=135006516>\n##contig=<ID=chr12,length=133851895>\n##contig=<ID=chr13,length=115169878>\n##contig=<ID=chr14,length=107349540>\n##contig=<ID=chr15,length=102531392>\n##contig=<ID=chr16,length=90354753>\n##contig=<ID=chr17,length=81195210>\n##contig=<ID=chr18,length=78077248>\n##contig=<ID=chr19,length=59128983>\n##contig=<ID=chr20,length=63025520>\n##contig=<ID=chr21,length=48129895>\n##contig=<ID=chr22,length=51304566>\n##contig=<ID=chrX,length=155270560>\n##contig=<ID=chrY,length=59373566>\n##contig=<ID=chrMT,length=16569>\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO\nchr22   41320486    4   G   T   .   .   .\nchr22   31009031    9   T   G   .   .   .\nchr22   43024150    15  C   G   .   .   .\nchr22   43027392    16  A   G   .   .   .\nchr22   37469571    122 C   T   .   .   .\nchr22   37465112    123 C   G   .   .   .\nchr22   37494466    124 G   T   .   .   .\nchr22   18561373    177 G   T   .   .   .\nchr22   51065593    241 C   T   .   .   .\nchr22   51064006    242 C   T   .   .   .\nchr22   51065269    243 G   A   .   .   .\nchr22   30032866    260 G   T   .   .   .  Now we will define path variable for vcf input and output paths and instantiate a VcfWriter, which will write out the annotated VCF:  from kipoi.postprocessing.variant_effects import VcfWriter\n# The input vcf path\nvcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\"\n# The output vcf path, based on the input file name    \nout_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\")\n# The writer object that will output the annotated VCF\nwriter = VcfWriter(model, vcf_path, out_vcf_fpath)  Then we need to instantiate an object that can generate variant-centered regions ( SnvCenteredRg  objects). This class needs information on the model input sequence length which is extracted automatically within  ModelInfoExtractor  objects:  # Information extraction from dataloader and model\nmodel_info = kipoi.postprocessing.variant_effects.ModelInfoExtractor(model, Dataloader)\n# vcf_to_region will generate a variant-centered regions when presented a VCF record.\nvcf_to_region = kipoi.postprocessing.variant_effects.SnvCenteredRg(model_info)  Now we can define the required dataloader arguments, omitting the  intervals_file  as this will be replaced by the automatically generated bed file:  dataloader_arguments = {\"fasta_file\": \"example_data/hg19_chr22.fa\"}  This is the moment to run the variant effect prediction:  import kipoi.postprocessing.variant_effects.snv_predict as sp\nfrom kipoi.postprocessing.variant_effects import Diff, DeepSEA_effect\nsp.predict_snvs(model,\n                Dataloader,\n                vcf_path,\n                batch_size = 32,\n                dataloader_args=dataloader_arguments,\n                vcf_to_region=vcf_to_region,\n                evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\"), 'deepsea_effect': DeepSEA_effect(\"mean\")}},\n                sync_pred_writer=writer)  100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:38<00:00,  2.73s/it]  In the example above we have used the variant scoring method  Diff  and  DeepSEA_effect  from  kipoi.postprocessing.variant_effects . As mentioned above variant scoring methods calculate the difference between predictions for reference and alternative, but there is another dimension to this: Models that have the  use_rc: true  flag set in their model.yaml file (DeepSEA/variantEffects does) will not only be queried with the reference and alternative carrying input sequences, but also with the reverse complement of the the sequences. In order to know of to combine predictions for forward and reverse sequences there is a initialisation flag (here set to:  \"mean\" ) for the variant scoring methods.  \"mean\"  in this case means that after calculating the effect (e.g.: Difference) the average over the difference between the prediction for the forward and for the reverse sequence should be returned. Setting  \"mean\"  complies with what was used in the Troyanskaya et al. publication.  Now let's look at the output:  # Let's print out the first 40 lines of the annotated VCF (up to 80 characters per line maximum)\nwith open(\"example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf\") as ifh:\n    for i,l in enumerate(ifh):\n        long_line = \"\"\n        if len(l)>80:\n            long_line = \"...\"\n        print(l[:80].rstrip() +long_line)\n        if i >=40:\n            break  ##fileformat=VCFv4.0\n##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:DEEPSEA_EFFECT,Number=.,Type=String,D...\n##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:DIFF,Number=.,Type=String,Description...\n##INFO=<ID=KV:kipoi:DeepSEA/variantEffects:rID,Number=.,Type=String,Description=...\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##contig=<ID=chr1,length=249250621>\n##contig=<ID=chr2,length=243199373>\n##contig=<ID=chr3,length=198022430>\n##contig=<ID=chr4,length=191154276>\n##contig=<ID=chr5,length=180915260>\n##contig=<ID=chr6,length=171115067>\n##contig=<ID=chr7,length=159138663>\n##contig=<ID=chr8,length=146364022>\n##contig=<ID=chr9,length=141213431>\n##contig=<ID=chr10,length=135534747>\n##contig=<ID=chr11,length=135006516>\n##contig=<ID=chr12,length=133851895>\n##contig=<ID=chr13,length=115169878>\n##contig=<ID=chr14,length=107349540>\n##contig=<ID=chr15,length=102531392>\n##contig=<ID=chr16,length=90354753>\n##contig=<ID=chr17,length=81195210>\n##contig=<ID=chr18,length=78077248>\n##contig=<ID=chr19,length=59128983>\n##contig=<ID=chr20,length=63025520>\n##contig=<ID=chr21,length=48129895>\n##contig=<ID=chr22,length=51304566>\n##contig=<ID=chrX,length=155270560>\n##contig=<ID=chrY,length=59373566>\n##contig=<ID=chrMT,length=16569>\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO\nchr22   41320486    chr22:41320486:G:['T']  G   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   31009031    chr22:31009031:T:['G']  T   G   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   43024150    chr22:43024150:C:['G']  C   G   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   43027392    chr22:43027392:A:['G']  A   G   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   37469571    chr22:37469571:C:['T']  C   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   37465112    chr22:37465112:C:['G']  C   G   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   37494466    chr22:37494466:G:['T']  G   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   18561373    chr22:18561373:G:['T']  G   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   51065593    chr22:51065593:C:['T']  C   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...\nchr22   51064006    chr22:51064006:C:['T']  C   T   .   .   KV:kipoi:DeepSEA/variantEffects:DE...  This shows that variants have been annotated with variant effect scores. For every different scoring method a different INFO tag was created and the score of every model output is concantenated with the  |  separator symbol. A legend is given in the header section of the VCF. The name tag indicates with model was used, wich version of it and it displays the scoring function label ( DIFF ) which is derived from the scoring function label defined in the  evaluation_function_kwargs  dictionary ( 'diff' ).  The most comprehensive representation of effect preditions is in the annotated VCF. Kipoi offers a VCF parser class that enables simple parsing of annotated VCFs:  from kipoi.postprocessing.variant_effects import KipoiVCFParser\nvcf_reader = KipoiVCFParser(\"example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf\")\n\n#We can have a look at the different labels which were created in the VCF\nprint(list(vcf_reader.kipoi_parsed_colnames.values()))  [('kipoi', 'DeepSEA/variantEffects', 'DEEPSEA_EFFECT'), ('kipoi', 'DeepSEA/variantEffects', 'rID'), ('kipoi', 'DeepSEA/variantEffects', 'DIFF')]  We can see that two scores have been saved -  'DEEPSEA_EFFECT'  and  'DIFF' . Additionally there is  'rID'  which is the region ID - that is the ID given by the dataloader for a genomic region which was overlapped with the variant to get the prediction that is listed in the effect score columns mentioned before. Let's take a look at the VCF entries:  import pandas as pd\nentries = [el for el in vcf_reader]\nprint(pd.DataFrame(entries).head().iloc[:,:7])                 variant_id variant_chr  variant_pos variant_ref variant_alt  \\\n0  chr22:41320486:G:['T']       chr22     41320486           G           T   \n1  chr22:31009031:T:['G']       chr22     31009031           T           G   \n2  chr22:43024150:C:['G']       chr22     43024150           C           G   \n3  chr22:43027392:A:['G']       chr22     43027392           A           G   \n4  chr22:37469571:C:['T']       chr22     37469571           C           T\n\n   KV_DeepSEA/variantEffects_DEEPSEA_EFFECT_8988T_DNase_None_0  \\\n0                                           0.000377             \n1                                           0.004129             \n2                                           0.001582             \n3                                           0.068382             \n4                                           0.001174\n\n   KV_DeepSEA/variantEffects_DEEPSEA_EFFECT_AoSMC_DNase_None_1  \n0                                       9.700000e-07            \n1                                       3.683220e-03            \n2                                       1.824500e-04            \n3                                       2.689577e-01            \n4                                       4.173300e-04  Another way to access effect predicitons programmatically is to keep all the results in memory and receive them as a dictionary of pandas dataframes:  effects = sp.predict_snvs(model,\n            Dataloader,\n            vcf_path,\n            batch_size = 32,\n            dataloader_args=dataloader_arguments,\n            vcf_to_region=vcf_to_region,\n            evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\"), 'deepsea_effect': DeepSEA_effect(\"mean\")}},\n            return_predictions=True)  100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:36<00:00,  2.64s/it]  For every key in the  evaluation_function_kwargs  dictionary there is a key in  effects  and (the equivalent of an additional INFO tag in the VCF). Now let's take a look at the results:  for k in effects:\n    print(k)\n    print(effects[k].head().iloc[:,:4])\n    print(\"-\"*80)  deepsea_effect\n                        8988T_DNase_None  AoSMC_DNase_None  \\\nchr22:41320486:G:['T']          0.000377      9.663903e-07   \nchr22:31009031:T:['G']          0.004129      3.683221e-03   \nchr22:43024150:C:['G']          0.001582      1.824510e-04   \nchr22:43027392:A:['G']          0.068382      2.689577e-01   \nchr22:37469571:C:['T']          0.001174      4.173280e-04\n\n                        Chorion_DNase_None  CLL_DNase_None  \nchr22:41320486:G:['T']            0.000162        0.000040  \nchr22:31009031:T:['G']            0.000201        0.002139  \nchr22:43024150:C:['G']            0.000322        0.000033  \nchr22:43027392:A:['G']            0.133855        0.000773  \nchr22:37469571:C:['T']            0.000008        0.000079  \n--------------------------------------------------------------------------------\ndiff\n                        8988T_DNase_None  AoSMC_DNase_None  \\\nchr22:41320486:G:['T']         -0.002850         -0.000094   \nchr22:31009031:T:['G']         -0.027333         -0.008740   \nchr22:43024150:C:['G']          0.010773          0.000702   \nchr22:43027392:A:['G']         -0.121747         -0.247321   \nchr22:37469571:C:['T']         -0.006546          0.000784\n\n                        Chorion_DNase_None  CLL_DNase_None  \nchr22:41320486:G:['T']           -0.001533       -0.000353  \nchr22:31009031:T:['G']           -0.003499       -0.008143  \nchr22:43024150:C:['G']            0.004689       -0.000609  \nchr22:43027392:A:['G']           -0.167689       -0.010695  \nchr22:37469571:C:['T']           -0.000383       -0.000924  \n--------------------------------------------------------------------------------  We see that for  diff  and  deepsea_effect  there is a dataframe with variant identifiers as rows and model output labels as columns. The DeepSEA model predicts 919 tasks simultaneously hence there are 919 columns in the dataframe.",
            "title": "Variant centered effect prediction"
        },
        {
            "location": "/tutorials/variant_effect_prediction/#overlap-based-prediction",
            "text": "Models that cannot predict on every region of the genome might not accept a  .bed  file as dataloader input. An example of such a model is a splicing model. Those models only work in certain regions of the genome. Here variant effect prediction can be executed based on overlaps between the regions generated by the dataloader and the variants defined in the VCF:   The procedure is similar to the variant centered effect prediction explained above, but in this case no temporary bed file is generated and the effect prediction is based on all the regions generated by the dataloader which overlap any variant in the VCF. If a region is overlapped by two variants the effect of the two variants is predicted independently.  Here the VCF has to be tabixed so that a regional lookup can be performed efficiently, this can be done by using the  ensure_tabixed  function, the rest remains the same as before:  import kipoi\nfrom kipoi.postprocessing.variant_effects import VcfWriter\nfrom kipoi.postprocessing.variant_effects import ensure_tabixed_vcf\n# Use a splicing model\nmodel_name = \"HAL\"\n# install dependencies\nkipoi.pipeline.install_model_requirements(model_name)\n# get the model\nmodel = kipoi.get_model(model_name)\n# get the dataloader factory\nDataloader = kipoi.get_dataloader_factory(model_name)\n# The input vcf path\nvcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\"\n\n# Make sure that the vcf is bgzipped and tabixed, if not then generate the compressed vcf in the same place\nvcf_path_tbx = ensure_tabixed_vcf(vcf_path)\n\n# The output vcf path, based on the input file name    \nout_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\")\n# The writer object that will output the annotated VCF\nwriter = VcfWriter(model, vcf_path, out_vcf_fpath)  Conda dependencies to be installed:\n['numpy']\nFetching package metadata ...............\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi:\n#\nnumpy                     1.14.1           py35h3dfced4_1  \npip dependencies to be installed:\n[]  This time we don't need an object that generates regions, hence we can directly define the dataloader arguments and run the prediction:  from kipoi.postprocessing.variant_effects import Diff, predict_snvs\ndataloader_arguments = {\"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75.filtered_chr22.gtf\",\n                               \"fasta_file\": \"example_data/hg19_chr22.fa\"}\n\neffects = predict_snvs(model,\n                        Dataloader,\n                        vcf_path_tbx,\n                        batch_size = 32,\n                        dataloader_args=dataloader_arguments,\n                        evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\")}},\n                        sync_pred_writer=writer,\n                        return_predictions=True)  100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:04<00:00, 176.63it/s]  Let's have a look at the VCF:  # A slightly convoluted way of printing out the first 40 lines and up to 80 characters per line maximum\nwith open(\"example_data/clinvar_donor_acceptor_chr22HAL.vcf\") as ifh:\n    for i,l in enumerate(ifh):\n        long_line = \"\"\n        if len(l)>80:\n            long_line = \"...\"\n        print(l[:80].rstrip() +long_line)\n        if i >=40:\n            break  ##fileformat=VCFv4.0\n##INFO=<ID=KV:kipoi:HAL:DIFF,Number=.,Type=String,Description=\"DIFF SNV effect p...\n##INFO=<ID=KV:kipoi:HAL:rID,Number=.,Type=String,Description=\"Range or region id...\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##contig=<ID=chr1,length=249250621>\n##contig=<ID=chr2,length=243199373>\n##contig=<ID=chr3,length=198022430>\n##contig=<ID=chr4,length=191154276>\n##contig=<ID=chr5,length=180915260>\n##contig=<ID=chr6,length=171115067>\n##contig=<ID=chr7,length=159138663>\n##contig=<ID=chr8,length=146364022>\n##contig=<ID=chr9,length=141213431>\n##contig=<ID=chr10,length=135534747>\n##contig=<ID=chr11,length=135006516>\n##contig=<ID=chr12,length=133851895>\n##contig=<ID=chr13,length=115169878>\n##contig=<ID=chr14,length=107349540>\n##contig=<ID=chr15,length=102531392>\n##contig=<ID=chr16,length=90354753>\n##contig=<ID=chr17,length=81195210>\n##contig=<ID=chr18,length=78077248>\n##contig=<ID=chr19,length=59128983>\n##contig=<ID=chr20,length=63025520>\n##contig=<ID=chr21,length=48129895>\n##contig=<ID=chr22,length=51304566>\n##contig=<ID=chrX,length=155270560>\n##contig=<ID=chrY,length=59373566>\n##contig=<ID=chrMT,length=16569>\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO\nchr22   17684454    chr22:17684454:G:['A']  G   A   .   .   KV:kipoi:HAL:DIFF=0.10586491;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   17684454    chr22:17684454:G:['A']  G   A   .   .   KV:kipoi:HAL:DIFF=0.10586491;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   17684454    chr22:17684454:G:['A']  G   A   .   .   KV:kipoi:HAL:DIFF=0.10586491;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   17684454    chr22:17684454:G:['A']  G   A   .   .   KV:kipoi:HAL:DIFF=0.10586491;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   17669232    chr22:17669232:T:['C']  T   C   .   .   KV:kipoi:HAL:DIFF=0.00000000;KV:ki...\nchr22   18561370    chr22:18561370:C:['T']  C   T   .   .   KV:kipoi:HAL:DIFF=-1.33794269;KV:k...  And the prediction output this time is less helpful because it's the ids that the dataloader created which are displayed as index. In general it is advisable to use the output VCF for more detailed information on which variant was overlapped with which region fo produce a prediction.  for k in effects:\n    print(k)\n    print(effects[k].head())\n    print(\"-\"*80)  diff\n            0\n290  0.105865\n293  0.000000\n299  0.000000\n304  0.105865\n307  0.000000\n--------------------------------------------------------------------------------",
            "title": "Overlap based prediction"
        },
        {
            "location": "/tutorials/variant_effect_prediction/#command-line-based-effect-prediction",
            "text": "The above command can also conveniently be executed using the command line:  import json\nimport os\nmodel_name = \"DeepSEA/variantEffects\"\ndl_args = json.dumps({\"fasta_file\": \"example_data/hg19_chr22.fa\"})\nout_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\")\nscorings = \"diff deepsea_effect\"\ncommand = (\"kipoi postproc score_variants {model} \"\n           \"--dataloader_args='{dl_args}' \"\n           \"--vcf_path {input_vcf} \"\n           \"--out_vcf_fpath {output_vcf} \"\n           \"--scoring {scorings}\").format(model=model_name,\n                                          dl_args=dl_args,\n                                          input_vcf=vcf_path,\n                                          output_vcf=out_vcf_fpath,\n                                          scorings=scorings)\n# Print out the command:\nprint(command)  kipoi postproc score_variants DeepSEA/variantEffects --dataloader_args='{\"fasta_file\": \"example_data/hg19_chr22.fa\"}' --vcf_path example_data/clinvar_donor_acceptor_chr22.vcf --out_vcf_fpath example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf --scoring diff deepsea_effect  ! $command  \u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m Update /homes/rkreuzhu/.kipoi/models/\u001b[0m\nAlready up-to-date.\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/variantEffects/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/model_files/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/example_files/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m model DeepSEA/variantEffects loaded\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/variantEffects/./**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/model_files/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/example_files/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m git-lfs pull -I DeepSEA/template/**\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.remote]\u001b[0m dataloader DeepSEA/variantEffects/. loaded\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.data]\u001b[0m successfully loaded the dataloader from /homes/rkreuzhu/.kipoi/models/DeepSEA/variantEffects/dataloader.py::SeqDataset\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.pipeline]\u001b[0m dataloader.output_schema is compatible with model.schema\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.postproc]\u001b[0m Using variant-centered sequence generation.\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.postproc]\u001b[0m Model SUPPORTS simple reverse complementation of input DNA sequences.\u001b[0m\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.postproc]\u001b[0m Annotated VCF will be written to example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf.\u001b[0m\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:38<00:00,  2.76s/it]\n\u001b[32mINFO\u001b[0m \u001b[44m[kipoi.cli.postproc]\u001b[0m Successfully predicted samples\u001b[0m",
            "title": "Command-line based effect prediction"
        },
        {
            "location": "/tutorials/variant_effect_prediction/#batch-prediction",
            "text": "Since the syntax basically doesn't change for different kinds of models a simple for-loop can be written to do what we just did on many models:  import kipoi\n# Run effect predicton\nmodels_df = kipoi.list_models()\nmodels_substr = [\"HAL\", \"MaxEntScan\", \"labranchor\", \"rbp\"]\nmodels_df_subsets = {ms: models_df.loc[models_df[\"model\"].str.contains(ms)] for ms in models_substr}  Let's make sure that all the dependencies are installed:  for ms in models_substr:\n    model_name = models_df_subsets[ms][\"model\"].iloc[0]\n    kipoi.pipeline.install_model_requirements(model_name)  Conda dependencies to be installed:\n['numpy']\nFetching package metadata ...............\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi:\n#\nnumpy                     1.14.1           py35h3dfced4_1  \npip dependencies to be installed:\n[]\nConda dependencies to be installed:\n['bioconda::maxentpy']\nFetching package metadata ...............\nSolving package specifications: .\n\n# All requested packages already installed.\n# packages in environment at /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi:\n#\nmaxentpy                  0.0.1                    py35_0    bioconda\npip dependencies to be installed:\n[]\nConda dependencies to be installed:\n[]\npip dependencies to be installed:\n['tensorflow>=1.0.0', 'keras>=2.0.4']\nRequirement already satisfied: tensorflow>=1.0.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages\nRequirement already satisfied: keras>=2.0.4 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages\nRequirement already satisfied: numpy>=1.12.1 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: protobuf>=3.3.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: wheel>=0.26 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: six>=1.10.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: enum34>=1.1.6 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow>=1.0.0)\nRequirement already satisfied: scipy>=0.14 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from keras>=2.0.4)\nRequirement already satisfied: pyyaml in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from keras>=2.0.4)\nRequirement already satisfied: setuptools in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from protobuf>=3.3.0->tensorflow>=1.0.0)\nRequirement already satisfied: bleach==1.5.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow>=1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow>=1.0.0)\nRequirement already satisfied: html5lib==0.9999999 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow>=1.0.0)\nRequirement already satisfied: werkzeug>=0.11.10 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow>=1.0.0)\nConda dependencies to be installed:\n[]\npip dependencies to be installed:\n['concise>=0.6.5', 'tensorflow==1.4.1', 'keras>=2.0.4']\nRequirement already satisfied: concise>=0.6.5 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages\nRequirement already satisfied: tensorflow==1.4.1 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages\nRequirement already satisfied: keras>=2.0.4 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages\nRequirement already satisfied: scikit-learn>=0.18 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: hyperopt in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: shapely in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: pandas in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: scipy in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: gtfparse>=1.0.7 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: descartes in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: matplotlib in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: numpy in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from concise>=0.6.5)\nRequirement already satisfied: tensorflow-tensorboard<0.5.0,>=0.4.0rc1 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow==1.4.1)\nRequirement already satisfied: enum34>=1.1.6 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow==1.4.1)\nRequirement already satisfied: protobuf>=3.3.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow==1.4.1)\nRequirement already satisfied: wheel>=0.26 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow==1.4.1)\nRequirement already satisfied: six>=1.10.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow==1.4.1)\nRequirement already satisfied: pyyaml in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from keras>=2.0.4)\nRequirement already satisfied: networkx in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from hyperopt->concise>=0.6.5)\nRequirement already satisfied: pymongo in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from hyperopt->concise>=0.6.5)\nRequirement already satisfied: nose in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from hyperopt->concise>=0.6.5)\nRequirement already satisfied: future in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from hyperopt->concise>=0.6.5)\nRequirement already satisfied: python-dateutil>=2 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/python_dateutil-2.3-py3.5.egg (from pandas->concise>=0.6.5)\nRequirement already satisfied: pytz>=2011k in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from pandas->concise>=0.6.5)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from matplotlib->concise>=0.6.5)\nRequirement already satisfied: cycler>=0.10 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from matplotlib->concise>=0.6.5)\nRequirement already satisfied: werkzeug>=0.11.10 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.1)\nRequirement already satisfied: markdown>=2.6.8 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.1)\nRequirement already satisfied: html5lib==0.9999999 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.1)\nRequirement already satisfied: bleach==1.5.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.1)\nRequirement already satisfied: setuptools in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from protobuf>=3.3.0->tensorflow==1.4.1)\nRequirement already satisfied: decorator>=4.1.0 in /nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages (from networkx->hyperopt->concise>=0.6.5)  Now we are good to go:  # Run variant effect prediction using a basic Diff\n\nimport kipoi\nfrom kipoi.postprocessing.variant_effects import ensure_tabixed_vcf\nimport kipoi.postprocessing.variant_effects.snv_predict as sp\nfrom kipoi.postprocessing.variant_effects import VcfWriter\nfrom kipoi.postprocessing.variant_effects import Diff\n\n\n\nsplicing_dl_args = {\"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75.filtered_chr22.gtf\",\n                               \"fasta_file\": \"example_data/hg19_chr22.fa\"}\ndataloader_args_dict = {\"HAL\": splicing_dl_args,\n                        \"labranchor\": splicing_dl_args,\n                        \"MaxEntScan\":splicing_dl_args,\n                        \"rbp\": {\"fasta_file\": \"example_data/hg19_chr22.fa\",\n                               \"gtf_file\":\"example_data/Homo_sapiens.GRCh37.75_chr22.gtf\"}\n                       }\n\nfor ms in models_substr:\n    model_name = models_df_subsets[ms][\"model\"].iloc[0]\n    #kipoi.pipeline.install_model_requirements(model_name)\n    model = kipoi.get_model(model_name)\n    vcf_path = \"example_data/clinvar_donor_acceptor_chr22.vcf\"\n    vcf_path_tbx = ensure_tabixed_vcf(vcf_path)\n\n    out_vcf_fpath = vcf_path[:-4] + \"%s.vcf\"%model_name.replace(\"/\", \"_\")\n\n    writer = VcfWriter(model, vcf_path, out_vcf_fpath)\n\n    print(model_name)\n\n    Dataloader = kipoi.get_dataloader_factory(model_name)\n    dataloader_arguments = dataloader_args_dict[ms]\n    model_info = kipoi.postprocessing.variant_effects.ModelInfoExtractor(model, Dataloader)\n    vcf_to_region = None\n    if ms == \"rbp\":\n        vcf_to_region = kipoi.postprocessing.variant_effects.SnvCenteredRg(model_info)\n    sp.predict_snvs(model,\n                    Dataloader,\n                    vcf_path_tbx,\n                    batch_size = 32,\n                    dataloader_args=dataloader_arguments,\n                    vcf_to_region=vcf_to_region,\n                    evaluation_function_kwargs={'diff_types': {'diff': Diff(\"mean\")}},\n                    sync_pred_writer=writer)\n    writer.close()  HAL\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:04<00:00, 174.46it/s]\n\n\nMaxEntScan/3prime\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:01<00:00, 455.98it/s]\n/nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n/nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n  warnings.warn('Error in loading the saved optimizer '\n\n\nlabranchor\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 709/709 [00:05<00:00, 136.26it/s]\n2018-03-06 12:28:12,624 [INFO] successfully loaded the dataloader from /homes/rkreuzhu/.kipoi/models/rbp_eclip/AARS/dataloader.py::SeqDistDataset\n/nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n  warnings.warn('Error in loading the saved optimizer '\n2018-03-06 12:28:14,495 [INFO] successfully loaded the model from model_files/model.h5\n2018-03-06 12:28:14,498 [INFO] dataloader.output_schema is compatible with model.schema\n2018-03-06 12:28:14,545 [INFO] git-lfs pull -I rbp_eclip/AARS/**\n2018-03-06 12:28:14,656 [INFO] git-lfs pull -I rbp_eclip/template/**\n\n\nrbp_eclip/AARS\n\n\n2018-03-06 12:28:14,788 [INFO] dataloader rbp_eclip/AARS loaded\n2018-03-06 12:28:14,817 [INFO] successfully loaded the dataloader from /homes/rkreuzhu/.kipoi/models/rbp_eclip/AARS/dataloader.py::SeqDistDataset\n2018-03-06 12:28:16,018 [INFO] Extracted GTF attributes: ['gene_id', 'gene_name', 'gene_source', 'gene_biotype', 'transcript_id', 'transcript_name', 'transcript_source', 'exon_number', 'exon_id', 'tag', 'protein_id', 'ccds_id']\nINFO:2018-03-06 12:28:16,413:genomelake] Running landmark extractors..\n2018-03-06 12:28:16,413 [INFO] Running landmark extractors..\nINFO:2018-03-06 12:28:16,585:genomelake] Done!\n2018-03-06 12:28:16,585 [INFO] Done!\n/nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n/nfs/research1/stegle/users/rkreuzhu/conda-envs/kipoi/lib/python3.5/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator Imputer from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [00:04<00:00,  3.10it/s]  let's validate that things have worked:  ! wc -l example_data/clinvar_donor_acceptor_chr22*.vcf      450 example_data/clinvar_donor_acceptor_chr22DeepSEA_variantEffects.vcf\n   2035 example_data/clinvar_donor_acceptor_chr22HAL.vcf\n    794 example_data/clinvar_donor_acceptor_chr22labranchor.vcf\n   1176 example_data/clinvar_donor_acceptor_chr22MaxEntScan_3prime.vcf\n    449 example_data/clinvar_donor_acceptor_chr22rbp_eclip_AARS.vcf\n    447 example_data/clinvar_donor_acceptor_chr22.vcf\n   5351 total",
            "title": "Batch prediction"
        },
        {
            "location": "/api/model/",
            "text": "Model\n\n\n\n\n[ ] describe the attributes of a model\n\n\n[ ] How do the model arguments relate to the yaml\n\n\n\n\nAvailable models\n\n\n[source]\n\n\nKerasModel\n\n\nkipoi.model.KerasModel(weights, arch=None, custom_objects=None, backend=None)\n\n\n\n\nLoads the serialized Keras model\n\n\nArguments\n\n\n\n\nweights\n: File path to the hdf5 weights or the hdf5 Keras model\n\n\narhc\n: Architecture json model. If None, \nweights\n is\nassumed to speficy the whole model\n\n\ncustom_objects\n: Python file defining the custom Keras objects\nin a \nOBJECTS\n dictionary\n\n\nbackend\n: Keras backend to use ('tensorflow', 'theano', ...)\n\n\n\n\nmodel.yml\n entry\n\n\n- __Model__:\n  - __type__: Keras\n  - __args__:\n    - __weights__: model.h5\n    - __arch__: model.json\n    - __custom_objects__: custom_keras_objects.py\n\n\n\n\n\n\n[source]\n\n\nPyTorchModel\n\n\nkipoi.model.PyTorchModel(file=None, build_fn=None, weights=None, auto_use_cuda=True)\n\n\n\n\nLoads a pytorch model. \n\n\n\n\n[source]\n\n\nSklearnModel\n\n\nkipoi.model.SklearnModel(pkl_file)\n\n\n\n\nLoads the serialized scikit learn model\n\n\nArguments\n\n\n\n\npkl_file\n: File path to the dumped sklearn file in the pickle format.\n\n\n\n\nmodel.yml entry\n\n\n- __Model__:\n  - __type__: sklearn\n  - __args__:\n    - __pkl_file__: asd.pkl\n\n\n\n\n\n\n[source]\n\n\nTensorFlowModel\n\n\nkipoi.model.TensorFlowModel(input_nodes, target_nodes, checkpoint_path, const_feed_dict_pkl=None)\n\n\n\n\n\n\nget_model\n\n\nget_model(model, source='kipoi', with_dataloader=True)\n\n\n\n\nLoad the \nmodel\n from \nsource\n, as well as the\ndefault dataloder to model.default_dataloder.\n\n\n\n\nArgs\n:\n  model, str:  model name\n  source, str:  source name\n  with_dataloader, bool: if True, the default dataloader is\nloaded to model.default_dataloadera and the pipeline at model.pipeline enabled.",
            "title": "Model"
        },
        {
            "location": "/api/model/#model",
            "text": "[ ] describe the attributes of a model  [ ] How do the model arguments relate to the yaml",
            "title": "Model"
        },
        {
            "location": "/api/model/#available-models",
            "text": "[source]",
            "title": "Available models"
        },
        {
            "location": "/api/model/#kerasmodel",
            "text": "kipoi.model.KerasModel(weights, arch=None, custom_objects=None, backend=None)  Loads the serialized Keras model  Arguments   weights : File path to the hdf5 weights or the hdf5 Keras model  arhc : Architecture json model. If None,  weights  is\nassumed to speficy the whole model  custom_objects : Python file defining the custom Keras objects\nin a  OBJECTS  dictionary  backend : Keras backend to use ('tensorflow', 'theano', ...)   model.yml  entry  - __Model__:\n  - __type__: Keras\n  - __args__:\n    - __weights__: model.h5\n    - __arch__: model.json\n    - __custom_objects__: custom_keras_objects.py   [source]",
            "title": "KerasModel"
        },
        {
            "location": "/api/model/#pytorchmodel",
            "text": "kipoi.model.PyTorchModel(file=None, build_fn=None, weights=None, auto_use_cuda=True)  Loads a pytorch model.    [source]",
            "title": "PyTorchModel"
        },
        {
            "location": "/api/model/#sklearnmodel",
            "text": "kipoi.model.SklearnModel(pkl_file)  Loads the serialized scikit learn model  Arguments   pkl_file : File path to the dumped sklearn file in the pickle format.   model.yml entry  - __Model__:\n  - __type__: sklearn\n  - __args__:\n    - __pkl_file__: asd.pkl   [source]",
            "title": "SklearnModel"
        },
        {
            "location": "/api/model/#tensorflowmodel",
            "text": "kipoi.model.TensorFlowModel(input_nodes, target_nodes, checkpoint_path, const_feed_dict_pkl=None)",
            "title": "TensorFlowModel"
        },
        {
            "location": "/api/model/#get_model",
            "text": "get_model(model, source='kipoi', with_dataloader=True)  Load the  model  from  source , as well as the\ndefault dataloder to model.default_dataloder.   Args :\n  model, str:  model name\n  source, str:  source name\n  with_dataloader, bool: if True, the default dataloader is\nloaded to model.default_dataloadera and the pipeline at model.pipeline enabled.",
            "title": "get_model"
        },
        {
            "location": "/api/dataloader/",
            "text": "Dataloader\n\n\n\n\n[ ] What args does dataloader have\n\n\n[ ] How do the fields relate to the yaml file\n\n\n\n\nAvailable dataloaders\n\n\n[source]\n\n\nDataset\n\n\nkipoi.data.Dataset()\n\n\n\n\nAn abstract class representing a Dataset.\n\n\nAll other datasets should subclass it. All subclasses should override\n\n__len__\n, that provides the size of the dataset, and \n__getitem__\n,\nsupporting integer indexing in range from 0 to len(self) exclusive.\n\n\n\n\n[source]\n\n\nBatchDataset\n\n\nkipoi.data.BatchDataset()\n\n\n\n\nAn abstract class representing a BatchDataset.\n\n\n\n\n[source]\n\n\nSampleIterator\n\n\nkipoi.data.SampleIterator()\n\n\n\n\n\n\n[source]\n\n\nBatchIterator\n\n\nkipoi.data.BatchIterator()\n\n\n\n\n\n\nget_dataloader_factory\n\n\nget_dataloader_factory(dataloader, source='kipoi')\n\n\n\n\n\n\nPreloadedDataset\n\n\nPreloadedDataset(self)\n\n\n\n\nGenerated by supplying a function returning the full dataset.\n\n\nThe full dataset is a nested (list/dict) python structure of numpy arrays\nwith the same first axis dimension.\n\n\n\n\nSampleGenerator\n\n\nSampleGenerator(self)\n\n\n\n\nTransform a generator of samples into SampleIterator\n\n\n\n\nBatchGenerator\n\n\nBatchGenerator(self)\n\n\n\n\nTransform a generator of batches into BatchIterator",
            "title": "Dataloader"
        },
        {
            "location": "/api/dataloader/#dataloader",
            "text": "[ ] What args does dataloader have  [ ] How do the fields relate to the yaml file",
            "title": "Dataloader"
        },
        {
            "location": "/api/dataloader/#available-dataloaders",
            "text": "[source]",
            "title": "Available dataloaders"
        },
        {
            "location": "/api/dataloader/#dataset",
            "text": "kipoi.data.Dataset()  An abstract class representing a Dataset.  All other datasets should subclass it. All subclasses should override __len__ , that provides the size of the dataset, and  __getitem__ ,\nsupporting integer indexing in range from 0 to len(self) exclusive.   [source]",
            "title": "Dataset"
        },
        {
            "location": "/api/dataloader/#batchdataset",
            "text": "kipoi.data.BatchDataset()  An abstract class representing a BatchDataset.   [source]",
            "title": "BatchDataset"
        },
        {
            "location": "/api/dataloader/#sampleiterator",
            "text": "kipoi.data.SampleIterator()   [source]",
            "title": "SampleIterator"
        },
        {
            "location": "/api/dataloader/#batchiterator",
            "text": "kipoi.data.BatchIterator()",
            "title": "BatchIterator"
        },
        {
            "location": "/api/dataloader/#get_dataloader_factory",
            "text": "get_dataloader_factory(dataloader, source='kipoi')",
            "title": "get_dataloader_factory"
        },
        {
            "location": "/api/dataloader/#preloadeddataset",
            "text": "PreloadedDataset(self)  Generated by supplying a function returning the full dataset.  The full dataset is a nested (list/dict) python structure of numpy arrays\nwith the same first axis dimension.",
            "title": "PreloadedDataset"
        },
        {
            "location": "/api/dataloader/#samplegenerator",
            "text": "SampleGenerator(self)  Transform a generator of samples into SampleIterator",
            "title": "SampleGenerator"
        },
        {
            "location": "/api/dataloader/#batchgenerator",
            "text": "BatchGenerator(self)  Transform a generator of batches into BatchIterator",
            "title": "BatchGenerator"
        },
        {
            "location": "/api/pipeline/",
            "text": "Pipeline\n\n\n[source]\n\n\nPipeline\n\n\nkipoi.pipeline.Pipeline(model, dataloader_cls)\n\n\n\n\nProvides the predict_example, predict and predict_generator to the kipoi.Model",
            "title": "Pipeline"
        },
        {
            "location": "/api/pipeline/#pipeline",
            "text": "[source]",
            "title": "Pipeline"
        },
        {
            "location": "/api/pipeline/#pipeline_1",
            "text": "kipoi.pipeline.Pipeline(model, dataloader_cls)  Provides the predict_example, predict and predict_generator to the kipoi.Model",
            "title": "Pipeline"
        },
        {
            "location": "/api/metadata/",
            "text": "Metadata\n\n\nHere are some of the custom classes that people might find useful when writing the metadata fields of the dataloaders.\n\n\nAvailable classes\n\n\n[source]\n\n\nGenomicRanges\n\n\nkipoi.metadata.GenomicRanges(chr, start, end, id, strand='*')",
            "title": "Metadata"
        },
        {
            "location": "/api/metadata/#metadata",
            "text": "Here are some of the custom classes that people might find useful when writing the metadata fields of the dataloaders.",
            "title": "Metadata"
        },
        {
            "location": "/api/metadata/#available-classes",
            "text": "[source]",
            "title": "Available classes"
        },
        {
            "location": "/api/metadata/#genomicranges",
            "text": "kipoi.metadata.GenomicRanges(chr, start, end, id, strand='*')",
            "title": "GenomicRanges"
        },
        {
            "location": "/api/remote/",
            "text": "Remote sources\n\n\n\n\n[ ] describe the main logic\n\n\nremote <-> local folder\n\n\n\n\nAvailable sources\n\n\n[source]\n\n\nGitLFSSource\n\n\nkipoi.remote.GitLFSSource(remote_url, local_path)\n\n\n\n\n\n\n[source]\n\n\nGitSource\n\n\nkipoi.remote.GitSource(remote_url, local_path)\n\n\n\n\n\n\n[source]\n\n\nLocalSource\n\n\nkipoi.remote.LocalSource(local_path)",
            "title": "Remotes"
        },
        {
            "location": "/api/remote/#remote-sources",
            "text": "[ ] describe the main logic  remote <-> local folder",
            "title": "Remote sources"
        },
        {
            "location": "/api/remote/#available-sources",
            "text": "[source]",
            "title": "Available sources"
        },
        {
            "location": "/api/remote/#gitlfssource",
            "text": "kipoi.remote.GitLFSSource(remote_url, local_path)   [source]",
            "title": "GitLFSSource"
        },
        {
            "location": "/api/remote/#gitsource",
            "text": "kipoi.remote.GitSource(remote_url, local_path)   [source]",
            "title": "GitSource"
        },
        {
            "location": "/api/remote/#localsource",
            "text": "kipoi.remote.LocalSource(local_path)",
            "title": "LocalSource"
        },
        {
            "location": "/api/postprocessing/variant_effects/",
            "text": "Variant effects\n\n\nMain function:\n\n\npredict_snvs\n\n\npredict_snvs(model, dataloader, vcf_fpath, batch_size, num_workers=0, dataloader_args=None, vcf_to_region=None, vcf_id_generator_fn=<function default_vcf_id_gen at 0x7f9941ff71e0>, evaluation_function=<function analyse_model_preds at 0x7f994224ff28>, evaluation_function_kwargs={'diff_types': {'logit': <kipoi.postprocessing.variant_effects.utils.scoring_fns.Logit object at 0x7f9941edd0f0>}}, sync_pred_writer=None, use_dataloader_example_data=False, return_predictions=False, generated_seq_writer=None)\n\n\n\n\nPredict the effect of SNVs\n\n\nPrediction of effects of SNV based on a VCF. If desired the VCF can be stored with the predicted values as\nannotation. For a detailed description of the requirements in the yaml files please take a look at\nkipoi/nbs/variant_effect_prediction.ipynb.\n\n\nArguments\n\n\n- __model__: A kipoi model handle generated by e.g.: kipoi.get_model()\n- __dataloader__: Dataloader factory generated by e.g.: kipoi.get_dataloader_factory()\n- __vcf_fpath__: Path of the VCF defining the positions that shall be assessed. Only SNVs will be tested.\n- __batch_size__: Prediction batch size used for calling the data loader. Each batch will be generated in 4\nmutated states yielding a system RAM consumption of >= 4x batch size.\n- __num_workers__: Number of parallel workers for loading the dataset.\n- __dataloader_args__: arguments passed on to the dataloader for sequence generation, arguments\nmentioned in dataloader.yaml > postprocessing > variant_effects > bed_input will be overwritten\nby the methods here.\n- __vcf_to_region__: Callable that generates a region compatible with dataloader/model from a cyvcf2 record\n- __vcf_id_generator_fn__: Callable that generates a unique ID from a cyvcf2 record\n- __evaluation_function__: effect evaluation function. Default is `analyse_model_preds`, which will get\narguments defined in `evaluation_function_kwargs`\n- __evaluation_function_kwargs__: kwargs passed on to `evaluation_function`.\n- __sync_pred_writer__: Single writer or list of writer objects like instances of `VcfWriter`. This object\nwill be called after effect prediction of a batch is done.\n- __use_dataloader_example_data__: Fill out the missing dataloader arguments with the example values given in the\ndataloader.yaml.\n- __return_predictions__: Return all variant effect predictions as a dictionary. Setting this to False will\nhelp maintain a low memory profile and is faster as it avoids concatenating batches after prediction.\n- __generated_seq_writer__: Single writer or list of writer objects like instances of `SyncHdf5SeqWriter`.\nThis object will be called after the DNA sequence sets have been generated. If this parameter is\nnot None, no prediction will be performed and only DNA sequence will be written!! This is relevant\nif you want to use the `predict_snvs` to generate appropriate input DNA sequences for your model.\n\n\n\nReturns\n\n\nIf return_predictions: Dictionary which contains a pandas DataFrame containing the calculated values\nfor each model output (target) column VCF SNV line. If return_predictions == False, returns None.",
            "title": "Variant effect prediction"
        },
        {
            "location": "/api/postprocessing/variant_effects/#variant-effects",
            "text": "Main function:",
            "title": "Variant effects"
        },
        {
            "location": "/api/postprocessing/variant_effects/#predict_snvs",
            "text": "predict_snvs(model, dataloader, vcf_fpath, batch_size, num_workers=0, dataloader_args=None, vcf_to_region=None, vcf_id_generator_fn=<function default_vcf_id_gen at 0x7f9941ff71e0>, evaluation_function=<function analyse_model_preds at 0x7f994224ff28>, evaluation_function_kwargs={'diff_types': {'logit': <kipoi.postprocessing.variant_effects.utils.scoring_fns.Logit object at 0x7f9941edd0f0>}}, sync_pred_writer=None, use_dataloader_example_data=False, return_predictions=False, generated_seq_writer=None)  Predict the effect of SNVs  Prediction of effects of SNV based on a VCF. If desired the VCF can be stored with the predicted values as\nannotation. For a detailed description of the requirements in the yaml files please take a look at\nkipoi/nbs/variant_effect_prediction.ipynb.",
            "title": "predict_snvs"
        },
        {
            "location": "/api/postprocessing/variant_effects/#arguments",
            "text": "- __model__: A kipoi model handle generated by e.g.: kipoi.get_model()\n- __dataloader__: Dataloader factory generated by e.g.: kipoi.get_dataloader_factory()\n- __vcf_fpath__: Path of the VCF defining the positions that shall be assessed. Only SNVs will be tested.\n- __batch_size__: Prediction batch size used for calling the data loader. Each batch will be generated in 4\nmutated states yielding a system RAM consumption of >= 4x batch size.\n- __num_workers__: Number of parallel workers for loading the dataset.\n- __dataloader_args__: arguments passed on to the dataloader for sequence generation, arguments\nmentioned in dataloader.yaml > postprocessing > variant_effects > bed_input will be overwritten\nby the methods here.\n- __vcf_to_region__: Callable that generates a region compatible with dataloader/model from a cyvcf2 record\n- __vcf_id_generator_fn__: Callable that generates a unique ID from a cyvcf2 record\n- __evaluation_function__: effect evaluation function. Default is `analyse_model_preds`, which will get\narguments defined in `evaluation_function_kwargs`\n- __evaluation_function_kwargs__: kwargs passed on to `evaluation_function`.\n- __sync_pred_writer__: Single writer or list of writer objects like instances of `VcfWriter`. This object\nwill be called after effect prediction of a batch is done.\n- __use_dataloader_example_data__: Fill out the missing dataloader arguments with the example values given in the\ndataloader.yaml.\n- __return_predictions__: Return all variant effect predictions as a dictionary. Setting this to False will\nhelp maintain a low memory profile and is faster as it avoids concatenating batches after prediction.\n- __generated_seq_writer__: Single writer or list of writer objects like instances of `SyncHdf5SeqWriter`.\nThis object will be called after the DNA sequence sets have been generated. If this parameter is\nnot None, no prediction will be performed and only DNA sequence will be written!! This is relevant\nif you want to use the `predict_snvs` to generate appropriate input DNA sequences for your model.",
            "title": "Arguments"
        },
        {
            "location": "/api/postprocessing/variant_effects/#returns",
            "text": "If return_predictions: Dictionary which contains a pandas DataFrame containing the calculated values\nfor each model output (target) column VCF SNV line. If return_predictions == False, returns None.",
            "title": "Returns"
        },
        {
            "location": "/api/postprocessing/variant_effect_scores/",
            "text": "Variant effect scores\n\n\n\n\n[ ] describe how these relate to the yaml file\n\n\n\n\nAvailable scores\n\n\n[source]\n\n\nLogit\n\n\nkipoi.postprocessing.variant_effects.utils.scoring_fns.Logit(rc_merging='mean')\n\n\n\n\n\n\n[source]\n\n\nLogitAlt\n\n\nkipoi.postprocessing.variant_effects.utils.scoring_fns.LogitAlt(rc_merging='mean')\n\n\n\n\n\n\n[source]\n\n\nLogitRef\n\n\nkipoi.postprocessing.variant_effects.utils.scoring_fns.LogitRef(rc_merging='mean')\n\n\n\n\n\n\n[source]\n\n\nLogitRef\n\n\nkipoi.postprocessing.variant_effects.utils.scoring_fns.LogitRef(rc_merging='mean')\n\n\n\n\n\n\n[source]\n\n\nDiff\n\n\nkipoi.postprocessing.variant_effects.utils.scoring_fns.Diff(rc_merging='mean')\n\n\n\n\n\n\n[source]\n\n\nDeepSEA_effect\n\n\nkipoi.postprocessing.variant_effects.utils.scoring_fns.DeepSEA_effect(rc_merging='mean')",
            "title": "Variant effect scores"
        },
        {
            "location": "/api/postprocessing/variant_effect_scores/#variant-effect-scores",
            "text": "[ ] describe how these relate to the yaml file",
            "title": "Variant effect scores"
        },
        {
            "location": "/api/postprocessing/variant_effect_scores/#available-scores",
            "text": "[source]",
            "title": "Available scores"
        },
        {
            "location": "/api/postprocessing/variant_effect_scores/#logit",
            "text": "kipoi.postprocessing.variant_effects.utils.scoring_fns.Logit(rc_merging='mean')   [source]",
            "title": "Logit"
        },
        {
            "location": "/api/postprocessing/variant_effect_scores/#logitalt",
            "text": "kipoi.postprocessing.variant_effects.utils.scoring_fns.LogitAlt(rc_merging='mean')   [source]",
            "title": "LogitAlt"
        },
        {
            "location": "/api/postprocessing/variant_effect_scores/#logitref",
            "text": "kipoi.postprocessing.variant_effects.utils.scoring_fns.LogitRef(rc_merging='mean')   [source]",
            "title": "LogitRef"
        },
        {
            "location": "/api/postprocessing/variant_effect_scores/#logitref_1",
            "text": "kipoi.postprocessing.variant_effects.utils.scoring_fns.LogitRef(rc_merging='mean')   [source]",
            "title": "LogitRef"
        },
        {
            "location": "/api/postprocessing/variant_effect_scores/#diff",
            "text": "kipoi.postprocessing.variant_effects.utils.scoring_fns.Diff(rc_merging='mean')   [source]",
            "title": "Diff"
        },
        {
            "location": "/api/postprocessing/variant_effect_scores/#deepsea_effect",
            "text": "kipoi.postprocessing.variant_effects.utils.scoring_fns.DeepSEA_effect(rc_merging='mean')",
            "title": "DeepSEA_effect"
        }
    ]
}